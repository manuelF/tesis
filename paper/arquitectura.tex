\subsection{Arquitectura y features GPGPU}


La arquitectura de las GPGPU esta enfocada a procesamiento de grandes cantidades de datos
de puntos flotante. El procesador de GPGPU cuenta con cientos de ALU sincronizadas
por bloques, permitiendo un paralelismo adaptativo a distintos problemas.

El procesamiento GPGPU es similar al procesamiento vectorial 
realizado por las supercomputadoras Cray y IBM que surgio en los 1960's.
El procesamiento consiste en en un hibrido entre compilador y procesador. Se determina
un conjunto de elementos a procesar y se elije una manera de dividirlos entre distintos
procesadores mediante distintas keywords del lenguaje. 

El paralelismo principalmente se divide en varios niveles:
-Thread Level
-Block Level
-Board level

El paralelismo a nivel de thread permite comunicaciones internas las distintas ALU, pero
manteniendose dentro del mismo bloque de ejecucion.

El paralelismo a nivel de bloque permite agrupar una cantidad de threads de manera
que se procesen todos simultaneamente en el mismo procesador fisicamente, y que
cuando algunos de esos threads se bloquea por alguna razon, el bloque cesa de ejecutar
y es reemplazador por otro listo para ejecutar.

El paralelismo a nivel de placa consiste en poder distribuir la carga entre distintas
GPGPU dispuestas en un mismo sistema compartiendo una memoria RAM comun. 

La GPGPU cuenta a su vez con multiples clases de memorias que se adaptan de maneras
diferentes a los distintos procesos. Estas incluyen:

-Registros
-Memoria Global
-Memoria local
-Memoria Compartida

Los registros son la unidad basica de almacenamiento de los kernels de ejecucion.
Cada thread de ejecucion cuenta con una cantidad limitada de registros de punto flotante de
acceso instantaneo. 

La memoria global es la memoria principal de la GPGPU. Esta es ilimitada en tama\~no 
y es compartida por todos los procesadores de la GPGPU y los CPU que integran el 
sistema. Es decir, tanto los GPGPU y los CPU pueden invocar las funciones de la libreria
para poder transferir datos de y hacia la memoria RAM.

La memoria compartida es una memoria que es visible para todos los threads dentro
de un bloque, pero es independiente entre bloques. Cada thread puede escribir en cualquier
parte de la memoria compartida de su bloque y puede ser leido por cualquier otro thread
de su bloque. Es una memoria muy rapida, que tarda una peque\~na cantidad de ciclos 
en poder ser leidos. Esta memoria es limitada y cuenta con una cantidad maxima del orden de los
Kb que debe ser compartido por todos los bloques.

La memoria local es un renombre de la memoria global, que es manejada por el compilador
de manera automatica y sirve para poder tener mas memoria interna al kernel de ejecucion
pero que sobrepase la cantidad de registros que cuenta ese thread para ejecutar.

La GPGPU cuenta con multiples niveles de memorias cache para poder aminorar el hecho
de que el principal cuello de botella del computo es la latencia en los accesos a memoria 
global.

-Cache L1
-Cache L2
-Cache de textura

CacheL1??

La cache L2 es automatica. La seccion de la memoria que esta dedicada a la cache L2
tambien esta dedicada a la memoria compartida, por lo que es posible en tiempo de ejecucion
darle directivas a la GPGPU que prioritice una memoria sobre la otra, permitiendo 
mayor concurrencia o mayor hit rate de cache.

La cache de textura es una cache que presenta no solo localidad espacial, como la mayoria
de las caches de procesadores normales (i.e. dato-1, dato, dato+1, etc.), sino que se le
puede agregar el concepto de dimensiones, para poder modelar datos en mas de una dimension.
Esto la convierte en una herramienta clave a la hora de minimizar los accesos a matrices
no solo por filas sino por columnas. Esta memoria se debe definir en momento de compilacion,
en el codigo y no es automatica, porque esta cache tiene limites espaciales (necesarios
para poder definir areas de memoria sobre la cual operar) y a su vez se debe acceder
a los dayos subyacentes a traves de funciones especificas.
 
\subsection{Funcionamiento de un GPGPU}
El procesador se encarga de levantar codigo de la RAM, ponerlo en la I$ de la GPGPU
y de poner a correr todos los bloques en cada procesador


\subsection{Diferencia entre CPU y GPU - Procesadores especulativos}
Viendo la historia de CPU, podemos hacer un recorrido de los problemas que los disen~nadores fueron
enfrentandose a lo largo del tiempo mirando los componentes que fueron agregando en los procesadores.

(insertar foto de die de nehalem)

caches - Pipelines - predictores - mas cache - memory controllers - many issues

Lo mas importante aca es observar el patron: "no desechemos algo que podramos necesitar pronto", 
"intentemos predecir el futuro de los condicionales", "intentemos correr dos instrucciones a la vez 
porque nos podemos llegar a bloquear"

Todos estos problemas convierten al CPU en una maquina que gira alrededor de la especulacion, 
de los valores futuros que van a tener las ejecuciones, del probable reuso de datos.
Podemos observar entonces que las unidades que verdaderamente hacen la ejecucion de las operaciones,
las ALU, son muy pocas en comparacion con la cantidad de dispositivos de soporte que estan
sobre el die del CPU. 

\ref{http://www.nvidia.com/content/PDF/fermi_white_papers/P.Glaskowsky_NVIDIAFermi-TheFirstCompleteGPUComputingArchitecture.pdf}

En contraste, las arquitecturas GPGPU son verdaderas maquinas de computo masivo. Estan dise\~nadas para
resolver una y otra vez operaciones muy bien definidas (transformaciones de punto flotante en su mayoria).
Comparativamente con un CPU, las ALU de las GPU son bastante pobres y lentas. No funcionan a las mismas 
velocidades de clock y deben estar sincronizadas entre si. Pero la gran ventaja esta en la cantidad. 
Un CPU cuenta con una ALU por core (alrededor de 16 cores por die de x86 es el state of the art), un GPU 
cuenta con miles de ALUs en total (mas de 3000 CUDA Cores en Tesla K20). En contraste, un GPU dispone 
de pocas unidades de soporte del procesamiento. No tiene pipelines especulativos, el tama\~no de las caches
estan a ordendes de magnitud de las de CPU, la latencia a las memorias principales de la GPU estan a 
decenas de clocks de distancia, etc. Basicamente buscaron resolver el problema del computo masivo pensando
en hacer mas cuentas a la vez y recalcular datos viejos de ser necesario, a diferencia del CPU que esta
pensado en hacer el menor trabajo posible y intentar mantener todos los datos que pueda en las memorias caches
gigantescas.
 

Como los CPU tienen que poder soportar cualquier aplicacion, no pueden avocarse de lleno a una sola
problematica. El hecho de no tener que dise\~nar un procesador de proposito general permitio un cambio radical
a la hora de pensar como concebir una arquitectura de gran throughput auxiliar al procesador, no reemplazandolo
pero mas bien adicionando poder de computo.

Las arquitecturas Fermi (y agregar AMD) han concebido el dise\~no de un procesador de alto desempe\~no. 
Su meta principal es poder soportar grandes cantidades de paralelismo, mediante el uso de procesadores
simetricos, pero tomando la fuerte restriccion de "no siempre tiene que andar bien". Es decir, ellos
mismos asumen que el codigo que van a correr esta bien adaptado a la arquitectura y no disponen
casi de mecanismos para dar "free lunch". Esta fuerte restriccion les permitio romper con el modelo
de computo de CPU y definir nuevas estrategias de paralelismo, que no siempre se adaptan bien a todos
los problemas, pero para un subconjunto de los desafios que se presentan en el area de HPC y de videojuegos
han probado ser un cambio paradigmatico.

\subsection{Idoneidad para el problema}
