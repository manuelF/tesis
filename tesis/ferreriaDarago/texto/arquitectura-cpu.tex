\section {CPU}

\subsection{Introducci\'on}

Los microprocesadores ganan en prevalencia desde principios de los 70,
cuando Intel introduce el chip 4004 y el 8008. Actualmente las arquitecturas
de procesadores de 64 bits basadas en la l\'inea x86-64 de Intel 
dominan no solo el mercado de computadoras personales sino que tambi\'en el de
servidores y \textit{clusters} de c\'omputo. Dado que utilizaremos el CPU 
estandar como punto de comparaci\'on para las dem\'as arquitecturas, y que estas
est\'an basadas en parte en su dise\~no, daremos una breve rese\~na de los aspectos
m\'as importantes y desarrollos modernos dentro de los procesadores modernos.

\subsection{Tipos de paralelismo}

Existen tres categor\'ias de paralelismo que una arquitectura puede aprovechar 
para mejorar la \textit{performance} de una aplicaci\'on, y que han ganado prevalencia
en el \'ultimo tiempo en el dise\~no de procesadores.

\begin{itemize}
    \item \textit{Instruction Level Parallelism}: Este tipo de optimizaciones buscan
    ejecutar la mayor cantidad de instrucciones en un mismo hilo de ejecuci\'on simultaneamente.
    Optimizaciones de este estilo incluyen
    \textit{pipelines} de procesador para ejecutar multiples instrucciones de manera solapada,
    ejecuci\'on superescalar fuera de \'orden para ejecutar multiples instrucciones que 
    utilizan unidades del procesador distintas o que no dependen una de la otra, ejecuci\'on
    especulativa (basada en predicci\'on de saltos del procesador), etc. 

    \item \textit{Data Level Parallelism}: Consideran las optimizaciones cuyo prop\'osito es
    lograr aplicar una misma operaci\'on a cada elemento de un conjunto datos simultaneamente 
    en un mismo hilo de ejecuci\'on.  Por eso este modelo se denomina SIMD
    (\textit{Single Instruction, Multiple Data}).

    \item \textit{Thread Level Parallelism}: Concierne al uso de multiples hilos de ejecuci\'on
    simultaneos, lo cual requiere el uso de procesadores usualmente
    comparten la memoria principal (arquitectura SMP, \textit{Symmetric Multiprocessing}), lo
    cual conlleva a esfuerzo adicional para mantener consistencia y coherencia de la misma, y
    que no se convierta en un cuello de botella.  
\end{itemize}

A continuaci\'on detallamos algunos aspectos de cada una de estas t\'ecnicas.

\subsection{Pipeline y Ejecuci\'on fuera de \'orden}

Los primeros usos de paralelismo fueron paralelismo a nivel de instrucciones, a trav\'es del uso
de \textit{pipelines} de multiples etapas. Estas etapas llegaron a ser tantas como 20, en la arquitectura
Intel Pentium 4. Cada etapa corresponde a una actividad distinta en el proceso de ejecutar una instrucci\'on.
Al tiempo que una instrucci\'on es decodificada, por ejemplo, otra instrucci\'on puede estar siendo retirada
de memoria, ya que idealmente las etapas previas no dependen de las posteriores. Por ejemplo, este sistema funciona
si una instrucci\'on no depende de los resultados de otra que viene anterior a ella. Sin embargo, esto puede
ocurrir, produciendose entonces un \textit{pipe stall} que requiere ejecutar las instrucciones de manera no solapada 
(con el costo de \textit{throughput} de instrucciones que ello implica).

Esta t\'ecnica llevada a su conclusi\'on l\'ogica se conoce como ejecuci\'on fuera de \'orden (\textit{Out of Order
Execution}). Mediante el uso de algoritmos y circuitos complejos, un procesador puede no depender del \'orden dado
de las instrucciones, sino solo de las dependencias entre las mismas, ejecutando etapas independientes para
instrucciones que no tienen requieren datos entre si. De esta manera las unidades del procesador pueden estar
ocupadas lo mayor posible.

La duplicaci\'on de unidades de soporte (ALUs, etc.) soportan esta clase de optimizaciones,
ya que aumentan la cantidad de eventos que pueden realizarse al mismo tiempo. Por eso es usual que un procesador
tenga m\'as de una unidad de operaciones aritm\'eticas o l\'ogicas.

Mejoras en este nivel eran usualmente invisibles al programador, dependiendo de los compiladores optimizantes,
pero sus ventajas fueron disminuyendo hacia principios del siglo XXI.

\subsection{Extensiones vectoriales}

Si bien las t\'ecnicas SIMD fueron desarrolladas por las supercomputadoras de los 70 y 80, su aparici\'on en los
microprocesadores x86 modernos ocurre en 1996 con el nombre MMX (\textit{MultiMedia eXtensions}), luego en las
extensiones SSE y AVX. AVX representa la \'ultima versi\'on de instrucciones de vectorizaci\'on y esta presente
en la l\'inea Intel Xeon de procesadores de alta gama.

Este paralelismo puede ser explotado por el compilador, que analiza los ciclos de programa y
puede convertirlos a utilizar multiples operaciones simult\'aneas con estas extensiones.

El uso de operaciones sobre multiples valores ha tomado mayor prevalencia como uno de los m\'etodos de incrementar
la performance de ejecuci\'on. La longitud de registros SIMD de las extensiones (128 bits para SSE, 256 para AVX) 
se duplica cada 4 a\~nos, con lo cual es importante para una aplicaci\'on que sus operaciones sean lo m\'as
vectorizables posible.~\cite{HennessyPatterson} Para esto es ideal que las operaciones sean lo m\'as regulares
posible, de modo de hacer mejor uso de estas facilidades.

\subsection{Caches}

A diferencia de los procesadores, la velocidad de acceso de las memorias principales no aumento de una manera
tan significativa, como puede verse en la figura~\ref{fig:cpu_vs_mem}. La consecuencia de esto es que la memoria
empez\'o a convertirse en un serio cuello de botella a la velocidad de ejecuci\'on de los programas.

Empleando el concepto de \textit{localidad}, producto de la observaci\'on de que los datos con los que opera una
secci\'on de un programa suelen estar cerca de memoria, los dise\~nos de procesadores empezaron a incluir distintos
tipos de caches, memorias r\'apidas y cercanas al CPU pero de menor tama\~no, para contener el subconjunto de los datos
recientemente en uso. Su eficacia impuls\'o el uso de una jerarqu\'ia de las mismas, organizadas en \'orden creciente
de tama\~no y decreciente en velocidad, empezando por las caches L1 y siguiendo por las L2 y L3.

El tama\~no de una cache L1 moderna est\'a en el \'orden de los 64 Kb, una cache L2 en el \'orden de los 512 Kb y
una L3 en el \'orden de 2 Mb. 

Si bien idealmente la aparici\'on de caches es invisible al programador, y usualmente lo es, accesos irregulares a la
memoria pueden producir que la cache se cargue con datos que no volveran a ser utilizados, y que datos posteriormente
a utilizar se hayan perdido y deban ser buscados en la relativamente lenta memoria principal (evento que se conoce como
\textit{cache miss}). Por esto es que la regularidad de los accesos a memoria para hacer buen uso de caches es
fundamental cuando la performance es de importancia.

\subsection{Multiprocesadores}

Los procesadores MIMD (\textit{Multiple Instruction Multiple Data}) implicaron una revoluci\'on en la computaci\'on, pero
cada procesador continua las l\'ineas anteriores.  Los dise\~nos m\'as utilizados se basan en SMP (\textit{Simetric
Multiprocessing}), donde todos los procesadores son iguales y comparten una misma memoria principal. Cada procesador tiene
sus propios registros y se comunica con los dem\'as mediante memoria compartida o interrupcciones.

Por ejemplo, el procesador Intel Xeon E7-8800 posee 12 procesadores (cores) que pueden ejecutar dos hilos simultaneamente
mediante el uso de \textit{Hyper-threading}. 

A diferencia de los otros m\'etodos, las mejoras posibles mediante el procesamiento paralelo en tareas son sustanciales,
pero dependen grandemente del programador. Un programa serial no se beneficiar\'a ampliamente de multiples \textit{cores},
incluso siendo recompilado, a menos que este paralelismo se aproveche explicitamente. Otro aspecto importante es la
\textit{escalabilidad}, que consiste en que la divisi\'on de tareas mantenga a todos los procesadores disponibles ocupados,
aunque la cantidad de los mismos crezca.

Un resultado importante a tener en cuenta es la denominada \textit{Ley de Amdahl}, que establece una relaci\'on entre
el \textit{speedup} m\'aximo alcanzable mediante un incremento en la cantidad de procesadores disponibles, el porcentaje
de la aplicaci\'on que es paralelizable y el porcentaje que no lo \'es. Matem\'aticamente,

\begin{equation}
    \label{eq:amdahl}
    S(n) = \frac{1}{1 + \frac{1}{n} (1 - B)}
\end{equation}

Donde $S$ es el porcentaje m\'aximo de mejora alcanzable, $B$ es la fracci\'on del algoritmo a ejecutar que debe ejecutarse
serialmente, y $n$ la cantidad de hilos de ejecuci\'on paralelos que se dispone.

Un ejemplo de esta ley en acci\'on es que si $95 \%$ del problema es paralelizable entonces el l\'imite te\'orico de
mejora es de 20 veces (el programa corriendo con infinitos cores correr\'ia en un veinteavo del tiempo que originalmente
requer\'ia). La curva puede verse graficada en~\ref{fig:amdahl_plot}.

Esto incluso resulta una simplificaci\'on, ya que asume que todos los cores tienen trabajo perfectamente distribuido, y que
no hay comunicaci\'on entre ellos por motivos de sincronizaci\'on, lo cual puede ocurrir en la presencia de caches.

Por otro lado, la presencia de un componente com\'un (la memoria) puede introducir cuellos de botella en el acceso a los
datos, ya que si el \textit{bus} de memoria es saturado con pedidos los procesadores deben detener su ejecuci\'on hasta que
los datos est\'en listos, elimin\'andose entonces el procesamiendo paralelo.

Otro elemento de conflicto son los caches. Como los procesadores deben tener una visi\'on unificada y consistente de la
memoria, a veces es necesario que estos coordinen los valores de sus caches, especialmente ante una escritura de memoria.
Esto se conoce como \textit{coherencia de caches} e involucra una sincronizaci\'on serial de alto \textit{overhead}, ya
que implica coordinaci\'on serial entre dos o m\'as procesadores, a trav\'es de un bus de memoria. 

Es en este punto que el impacto en el comportamiento del programa puede ser tan fuerte como sutil. Un fen\'omeno que
ilustra esto es el de \textit{false sharing}. Este fen\'omeno ocurre cuando una variable no compartida entre threads
reside en la misma l\'inea de cache con una que si, requiriendo entonces que sea pasada de lado a lado entre cores aunque
nunca fuese esto necesario, da\~nando seriamente la escalabilidad del algoritmo a muchos procesadores.
