\subsection{Estructura inicial del programa}

El programa originalmente estaba concebido para ser corrido en GPU nVidia GTX8800.
Luego, las cosas que se usaron para este desarrollo consistieron en la libreria CUDA version
2, para arquitecturas a lo sumo SM20.

La estructura del programa era la siguiente:
\begin{enumerate}
\item Se determinan los mallados del sistema
\item Se clasifican en cubos y esferas
\item Para cada elemento, se resuelve el sistema
\item Esto se repite hasta que converga a lo sumo en una cantidad limitada de pasos, o diverga
\end{enumerate}

La resoluci\'on del sistema en si esta compuesta de varios pasos:
\begin{enumerate}
\item Se obtiene el valor de la funci\'on de onda en cada punto de la malla
\item Se generan las matrices con los gradientes y los hessianos de cada funci\'on
\item Se calculan las densidades y el producto matricial por bloques por funci\'on
\item Se calcula la matriz de coeficientes de las fuerzas entre las particulas
\end{enumerate}


\subsection{Cuellos de botellas originales, limitantes estructurales}

El cuello de botella principal que presentaba este c\'odigo era en la
resoluci\'on del sistema, especificamente en el c\'odigo que calculaba la matriz de Kohn-Sham.
En el GPU, esta funci\'on insumia el 95\% del tiempo total, que en sistemas de gran cantidad
de puntos, estaba en el orden de minutos. Los problemas principales que mostraba esta funci\'on fueron:
\begin{itemize}
\item Cantidad de accesos a memoria global
\item Falta de accesos coalescentes
\item Sobreuso de la memoria compartida
\item Subsaturacion de los SM
\end{itemize}

\subsubsection{Accesos a memoria global}

\subsubsection{Falta de accesos coalescentes}
Asi como a los CPU les interesa realizar accesos alineados a memoria, a los GPU les
interesa aun mas. El termino coalescencia de memoria se aplica a GPU mediante
la organizacion de los accesos a memoria de manera ordenada y predecible. La l\'ogica
detras de esto es que cuando el GPU accede a memoria de manera alineada, puede traer
entre 16 y 64 bytes en una sola lectura, mientras que si debe acceder de manera no
alineada a memoria, o con threads que no acceden de manera predecible a esta, entonces
se deberan serializar los accesos y separar en m\'ultiples accesos a memoria global para leer
esta informaci\'on. Este problema solamente se agrava si se debe hacer muy frecuentemente
por cientos de threads, como es el caso de los bloques con gran nivel de paralelismo explicito.

\subsubsection{Sobreuso de la memoria compartida}
Los SM cuentan con una memoria compartida entre todos los threads, la memoria shared. Esta
tiene un bus de baja latencia con conexionado especifico dentro de cada SM.


\subsubsection{Subsaturacion de los SM}
La subsaturacion de los SM se da en los casos donde haya SM que esten listos para correr
c\'odigo pero que no puedan hacerlo porque tienen contencion en algunos de sus recursos.
La m\'etrica usada para determinar esta saturaci\'on es la ocupancia de los SP.
Esta es la proporci\'on de threads activos sobre el total de threads disponibles de un bloque.

Existen en esta arquitectura principalmentre tres recursos que, en un principio, parecen
ilimitados pero en realidad son finitos y compartidos por los procesadores de la GPGPU.
Estos son:
\begin{itemize}
\item Cantidad total de threads por bloque.
\item Cantidad total de registros usados por thread.
\item Cantidad de memoria compartida por bloque.
\end{itemize}

El mecanismo de scheduling de los SM funciona asignando un bloque a cada SM, que
va a correr sin preemption hasta que terminen todos sus threads asignados. Idealmente, cada
bloque cuenta con una cantidad de threads suficiente para poder esconder la latencia
de las ejecuciones mediante un cambio de contexto. La arquitectura GPGPU esta dise\~nada
para este fin, por lo cual se cuenta con un mecanismo de cambio de contexto de costo cero~\cite{NvidiaFermi} para
poder empezar a correr los threads de un warp diferente, del mismo bloque.
Si el bloque no cuenta con suficiente cantidad de threads para poner a correr de manera
concurrente, el SM va a forzosamente esperar que finalicen las operaciones de alta latencia
de estos warps sin nada que hacer mientras tanto. Si, por el contrario, se contasen con
miles de threads por bloque, entonces es posible que las operaciones que sirvan
para sincronizar los threads de todo un bloque en un punto espec\'ifico antes de proseguir
(un barrier) sean excesivamente costosas.

La arquitectura GPGPU de NVIDIA organiza los registros de todos los threads en un unico
register file, com\'un a todos los bloques. Como cada thread usa decenas de registros para guardar
los computos intermedios, NVIDIA decidio unificarlos, ya que es muy variable la cantidad que va a usar
cada kernel de ejecuci\'on. Una de las grandes diferencias entre Fermi y Kepler es la cantidad m\'axima de
registros por thread. Mientras que Fermi permitia hasta 63 registros, Kepler permite hasta 255. Esto
es positivo para poder correr bloques de pocos threads pero gran cantidad de registros. Por otro lado,
aumenta la presion sobre el register file. Cuando se lanzan muchos threads
que puedan estar corriendo paralelamente entre todos los SM de la GPU, es posible que se supere
la cantidad m\'axima de registros presentes en el register file. Esto fuerza a que el scheduler
no pueda poner a ejecutar m\'as bloques que los que pueda soportar este recurso, dejando SP ociosos.

Finalmente, al igual que con los registros, la memoria compartida es un recurso limitado. Como
solamente se cuenta con hasta 48Kb (Fermi-Kepler) de memoria de este tipo para ser repartida entre
todos los bloques que esten corriendo en todos los SM, el scheduler debera decidir no poner a ejecutar
m\'as bloques simultanemante que los que pueda soportar la cantidad de memoria compartida.

El problema tratado dentro de esta tesis cont\'o con todos estos limitantes. Afortunadamente,
las herramientas de profiling usadas remarca estos limitantes constamente, haciendolas
fundamentales a la hora de evaluar como proseguir en la busqueda de optimizaciones de c\'odigo.

\subsection{Cambios en el threading}
%Cambiamos de blocks por puntos, a blocks por funci\'on.
Originalmente el c\'odigo generaba un bloque por cada punto en el grupo de puntos
que teniamos que solucionar, con una sola dimension por thread.
Como este era el kernel que insumia el 94\% del tiempo de todo el proceso, decidimos
atacarlo de raiz, cambiando la paralelizaci\'on. A pesar de que el profiler de CUDA nos indicaba
que tenia una buena occupancy, decidimos cambiar de fondo el encare del problema.

El cuello de botella fundamental radicaba en como se distribuia el trabajo de computo
entre los kernels. Una cosa que se nota que este mecanismo de paralelizacion cuenta con
la ventaja de que casi no se compartian los datos entre los distintos threads, lo cual,
a priori, deberia haber ocasionado que no hubiera mucho mejor forma de realizar el computo.

Sin embargo, con un detalle m\'as fino, es posible notar que este mecanismo hacia que
muchos threads hicieran cuentas innecesarias, que no contribuian a la reducci\'on total.
Esto daba una alta ocupancia ficticia, que buscamos acercar a la real. Adem\'as, a pesar
de que se compartian muy poca informaci\'on por la memoria compartida, tambien habia muchos
puntos de sincronización a nivel bloque.

XXXXXXXXX cambiamos a tener block\_height bloques por cada punto, y estos se operan
con una sola dimension de thread, donde block\_height es un parametro que se calcula
para cada grupo, y es el resultado de dividir la cantidad de funci\'ones que se van
a calcular por punto por la 2*DBS.

XXXXXXXXX Otro cambio importante que estudiamos con respecto al threading, es intentar realizar
mas de un punto por thread. Esto sirve para aprovechar un par de lecturas que son comunes
entre dos funciones. Con este cambio, se pueden ocultar algunas latencias de acceso, pero
cada grupo de threads lleva m\'as tiempo y usan m\'as registros. Esta estrategia es similar
a un loop unrolling manual, aplicado a la arquitectura GPU.

Un punto de intensa discusion es en el tema\~no de la particion de bloques para el calculo
de esta densidad. Para nuestro problema, decidimos utilizar una partici\'on por bloque
unidimensional, que sea multiplo de el tama\~no de un warp (32 threads). Esto permite
estudiar como afectan en el tiempo de procesamiento contar con uno o m\'as warps por
bloque. Una ventaja de usar bloques de 32 threads, es que el costo de la sincronizaci\'on
es exactamente cero. No se precisa sincronizar nada puesto que los threads trabajan
en lock-step sincronizados por warp. Un bloque chico ademas nos permite usar mas memoria compartida
por thread, dado que hay una cantidad fija de memoria por bloque (entre 32Kb y 64Kb).
Cuando se cuentan con muchos mas threads, se debe reducir este uso por thread de modo que
todos puedan ejecutar concurrentemente.

Dicho esto, la literatura \cite{NVIDIA_OPTIMIZATIONS} sugiere siempre que sea posible
usar bloques grandes y con threads lo m\'as independientes posibles. Una gran cantidad de threads
en un bloque permite tener muchos mas warps para schedulear de modo de esconder las latencias de
operaciones y de a accesos globales. Sin embargo, contar con muchos threads hace que las
sincronizaciones sean mucho m\'as costosas. Ademas, como cada SM no cuenta con pre-emption
de bloques, contar con muchos threads por bloque hace que los recursos se mantengan
por largos periodos.

Inicialmente, este tama\~no se habia fijado en 128 threads por bloque, 4 warps. Sin embargo,
al haber cambiado el threading a que realice dos cuentas por thread si puede, se aument\'o
el tiempo de ejecuci\'on de cada warp en promedio, pero menos que duplicar las cuentas.
Mostramos que tener solo un warp es bueno, pero mucho mejor a\'un es tener dos warps, porque
esto permite, con costo cero, poner a correr el otro para ocultar la latencia sin agrandar
demasiado el costo de la sincronizacion.


\subsection{Cambios en las memorias compartidas}
Accesos por bloque

\subsection{Cambios en los accesos globales}
La arquitectura de las placas de video estan pensadas entorno al poder de computo.
Las decisiones tomadas por los diseñadores de las GPGPU se concentran alrededor
de paralelismo a lo ancho, poniendo un gran enfasis en la cantidad de nucleos. Luego,
se dispone de menor cantidad de espacio disponible en el die para las memorias.

Esta decision implica que la amplia mayoria de la memoria de la GPGPU se encuentra
localizada externa al procesador.  No solo esta fisicamente mas lejos, sino que
adem\'as la latencia para accederla es altisima. Es decir, el paradigma de
programaci\'on de las GPGPU gira entorno a esconder la gran latencia de los accesos
a las memorias globales.

Una de las memorias intermedias entre entre el procesador y la memoria global es
la memoria de textura. La memoria de textura es un cach\'e sobre la memoria global,
que esta focalizado alrededor de los accesos a memoria en varias dimensiones.
Estas memorias reciben su nombre de su funci\'on principal, que es en el area de los
sistemas de video. Los mapas de textura suelen ser grandes matrices que definen
tanto los colores sobre las superficies de los poligonos como los relieves.
El detalle crucial de estas memorias es que un miss en estas cache, provoca
que se traigan datos no solo contiguos en memoria, como pasa en las caches de
CPU normalmente, sino que ademas se traigan los datos en posiciones logicas contiguas,
es decir, variando las distintas dimensiones de la matriz subyacente.

Las memorias de textura se ajustan bien a los problemas de GPGPU, porque se relacionan
intimamente con los los mecanismos de paralelismo de CUDA. Como los problemas se pueden
dividir en bloques con threads en $x$, $y$, $z$, entonces tiene mucho sentido pensar
que las estructuras de datos subyacentes se van a acceder usando indices multidimensionales.

En nuestro problema, la memoria de textura se presenta como una solucion para
los accesos bidimensionales de la matriz de RMM para el grupo de puntos.
Como esta matriz debe ser multiplicada por todos los valores de las funci\'ones,
derivadas primeras y segundas, se va a acceder a toda la matriz de RMM mas de
una vez por cada thread. Adem\'as, como se va a usar toda la matriz, y esta suele
tener un tamaño intermedio (es muy grande para memoria constante), el problema
suele entrar casi completamente en la memoria de textura.
La lectura bidimensional en este caso, se ajusta muy bien a los accesos por filas
y por columnas a la matriz.

El uso de la memoria de texturas agrega un recurso mas que tenemos que tener en
cuenta a la hora de profilear el c\'odigo. Para administrar los accesos a
la memoria de textura, cada multiprocesador tiene multiples "Unidades de textura".
Cuando dependemos de sobremanera de la memoria de textura para esconder la latencia,
se presentan contenciones sobre el acceso a las unidades de textura. Esto es
uno de los motivos por los cuales el procesador stallea los bloques hasta poder
ejecutarlos, cuando se liberan un poco mas los recursos.

%http://www.realworldtech.com/gt200/10/   << detalles sobre texture cache invalidation

\subsection{Cambios en la reducci\'on}
%Hubo que agregar reducci\'on de suma a nivel punto porque ya no se comparten mas la info

La unidad de GPGPU presenta grandes ventajas a la hora del computo masivamente distribuido.
Si logramos dividir el problema en pequeños kernels de ejecucion, podemos garantizar que
todos los multiprocesadores van a poder ejecutar constantemente grupos de threads, teniendo
siempre nuevos grupos listos para poder ponerlos a correr para poder esconder la latencia de uso.

El problema que se presenta al dividir tanto el computo, es que en algun momento se deberia juntar
todos los bloques independientes de ejecucion. Este proceso se conoce como reducci\'on, es decir,
aplicar una operacion a un vector de elementos tal que se obtenga un \'unico valor finalmente
como resultado de la cuenta.

En nuestro caso, al haber cambiado la paralelizacion de un bloque por punto, a un bloque
de funci\'ones, aparece una reducci\'on que antes no necesitabamos realizar. La reducci\'on que
requerida es una suma de todos los elementos de toda la columna que acabamos de procesar.
Como cada instancia del kernel pertenece a una columna de la matriz, y toda una columna se
procesa por un bloque, \textcolor{blue}{---REVISAR---}, entonces se puede compartir los datos
a nivel de bloque. Es decir, podemos hacer la reducci\'on reutilizando la memoria compartida
que usamos antes para pasarnos los distintos valores de cada fila de las funci\'ones y sus derivadas.

La reducci\'on eficiente es un tema ya muy estudiado en la bibliografia \textcolor{blue}{insertar cita a
reducci\'on y reducci\'on en cuda}. La reducci\'on en memoria compartida es fundamentalmente
una t\'ipica reducci\'on en \'arbol. Un detalle importante en este caso es que, a pesar de que
se reducen a lo sumo DENSITY\_BLOCK\_SIZE threads, es necesario hacer la reducci\'on en arbol
para poder maximizar la cantidad de threads ejecutando en paralelo la reducci\'on. Esta reducci\'on
binaria se realiza $\log_2{DBS}$ veces.

Este mecanismo de reducci\'on es generico y se puede emplear cualquier operacion binaria que
totalice las cuentas.


\subsection{Cambios en los pasajes de informaci\'on intrawarp}
%Los shuffles que no anduvieron salvo en function
La arquitectura SM35, junto con CUDA5, trajeron aparejadas una herramienta interesante
para el manejo interno de los pasajes de informaci\'on intra-warp durante la ejecuci\'on.
Las instrucciones de shuffle, como asi las denomina NVIDIA, son instrucciones que facilitan
el pasaje directo de un registro de un thread en un warp, a otro, en un solo ciclo de ejecuci\'on.
Estas instrucciones existen en diversas maneras, con distintos propositos. Principalmente se
utilizan para pasar de un thread al siguiente (modulo el warp size) un registro para poder seguir
operando. Otro uso que puede tener mucho interes proximamente son las instrucciones de votaci\'on,
donde se evalua un predicado para todos los threads, y se setea o limpia un bit en el resultado
de respuesta si se cumplio el predicado para ese thread. Con esta herramienta, no es necesario
acceder a memoria compartida para poder pasar minima informaci\'on dentro de cada warp.

Nuestro uso de las funci\'ones de shuffle consistio en intentar eliminar lo m\'as que podiamos
los accesos a la memoria compartida, una fuente de bloqueos porque, a pesar de que ya corre
todos los bloques concurrentemente, leer elementos de ahi toman 4 ciclos en vez de uno solo
como en las funci\'ones de shuffle.

Probamos pasar de a un elemento y de a uno o dos vectores de 3 elementos, para comparar
cuan notable era el impacto del acceso mas veloz.

Finalmente concluimos que era una opcion valida para el pasaje de los valores de la funci\'on,
pero que el overhead de uso para cosas como los hessianos de la funci\'on no justifica el uso.
Adem\'as, como estas funci\'ones de shuffle solo estan presentes en las ultimas placas Kepler,
consideramos que el aprovechamiento marginal de los recursos no era lo suficientemente meritorio
de romper compatibilidad con las placas de la generacion Fermi anterior.


\subsection{Cambios en el almacenamiento de matrices temporales}
Una de las principales limitaciones de las GPGPU es la cantidad fija de memoria. Esta no es
expandible dado que esta soldada a la placa. Esto era aun m\'as notable cuando las placas
contaban con menos de 1Gb de memoria (A\~nos 2007-2008).
Para problemas de calculo num\'erico, esto era un limitante muy serio; los problemas que
normalmente entraban en la memoria principal de un CPU, no entraban completos en las GPU.
La decisi\'on tomada por muchas aplicaciones de entonces es compensar esto calculando
datos intermedios y tirandolos al final; teniendo que ser recalculados en las proximas iteraciones.

Esta estrategia es claramente impractica en CPU, puesto que se cuenta con mucha mas memoria
de uso general. En GPU era necesario por el faltante de memoria, pero no es tan notorio como en CPU,
puesto que estas cuentas se pueden hacer bastante m\'as rapido en GPU.

Cuando la aplicacion original se concibio, no habia siquiera placas Tesla de GPGPU apuntadas a HPC, con
m\'as memoria disponible que los modelos de consumidores. Aprovechando estos recursos actuales,
desarrollamos un m\'etodos para poder almacenar estos resultados temporales, de manera dinamica
durante la ejecuci\'on de la aplicaci\'on, para poder aprovechar este recurso que originalmente
era limitante pero ya no.

Para poder determinar que cosas van a ser guardadas en mem\'oria y que no, se determin\'o una heuristica
que define el orden de las particiones a solucionar. Esta heuristica estima que tama\~no van a
tener las matrices temporales a almacenar y ordena las particiones de menor a mayor. Esto
esta basado en el criterio de que, si bien es proporcional el tiempo de computo de estas matrices
temporales a la cantidad de funciones por grupo y cantidad de puntos (lo que determina el tama\~no
de la partici\'on), la constante es elevada. Determinamos entonces que es m\'as conveniente
aprovechar la memoria de la placa que almacena muchas matrices temporales de particiones chicas
a que almacene solamente un par de las grandes.

Para controlar la administracion de memoria, se realiza la estimaci\'on si la placa cuenta
con la suficiente memoria libre para guardar los datos a calcular, y si puede, se guardan de manera
permanente (hasta que la partici\'on se mueva a otro dispositivo o la liberaci\'on de recursos al
finalizar el programa). Esto ademas es configurable de modo que una corrida pueda usar un porcentaje
de la memoria con la que cuenta la placa, para poder correr multiples procesos de simulaci\'on concurrentemente.

Esta sencilla mejora permite explotar el hecho de que las placas hayan aumentado dramaticamente su
capacidad de almacenamiento, un recurso que hasta recientemente venia siendo un limitante podemos
convertirlo en una aceleraci\'on notoria.


\subsection{Cambios en las operaciones matem\'aticas}
%Cambiar los vec\_type4 por 3 en los accesos a la shared es mucho mejor, no hace falta alinear ahi
Otro de los problemas existentes del c\'odigo que quisimos atacar fue el mejor empleo de las
memorias shared. Estas son un recurso finito y muy importante, ya que son un limitante de
la ocupancia de los multiprocesadores. Como pueden correr una cantidad de bloques que, a lo sumo,
no superen los 48Kb de memoria shared entre todos, es imprescindible minimizar el consumo real
de la memoria shared de modo que no estemos subutilizando los multiprocesadores.

Una cosa que probamos, con un grado de exito variante, fue disminuir el tamaño de los vectores
donde almacenamos las derivadas direccionales. Como nuestro problema es en tres dimensiones,
y los vectores estaban configurados para tener 4 valores por cada derivada, probamos llevarlos a
3, para que se ajusten a su consumo real de memoria. Este acercamiento no consider\'o el porque
se hizo asi de esta manera originalmente. Tener 4 valores consecutivos en memoria fuerza
al compilador a alinearlos a 16/32 bytes (simple y doble precision).
Esto presenta grandes ventajas a la hora de hacer transferencias de memoria global en los accesos.

Sin embargo, este criterio no aplica a las memorias shared de la GPGPU. Como los accesos
a estas memorias se realizan de a 4 bytes y no de a 16/32, entonces no tiene ninguna ventaja
en particular realizar el alineamiento. Adicionalmente, como el cuarto valor no tiene forma de marcarse
como algo mas alla de padding, todavia se opera normalmente con el, por lo que nos ahorramos una
operaci\'on de c\'alculo. Mas a\'un, se presentan un ahorro del 25\% de los recursos de la memoria compartida
en contencion, sin ninguna desventaja a la hora de acceder a estos. Principalmente
esta mejora permite aumentar la cantidad de bloques corriendo concurrentemente en los multiprocesadores,
sin estar limitados por el uso individual de las memorias compartidas.

\subsection{Cambios en los condicionales}
La arquitectura CUDA representa un m\'odelo de computo pensado en el procesamiento secuencial masivo
de datos de punto flotante. Esto es herencia de su legado de placa gr\'aficas, que era un stream
constante de datos. Al generalizar la arquitectura para que sea de proposito general, entonces
surge el problema de ejecuci\'on condicional. Como el resultado de la evaluaci\'on puede ser distinto
para cada thread, entonces surge el problema de como ejecutar un warp en lockstep cuando algunos threads
correran la rama \texttt{true} y otros la rama \texttt{false}. La soluci\'on que adopta CUDA es la
serializaci\'on implicita. Los threads que no ejecutan el \texttt{true} correr\'an \texttt{NOP}
y lo mismo se hara en el caso del \texttt{false}, al reves.

Esto trae aparejado una penalidad importante. Si esa bifurcacion contiene mucho c\'odigo no trivial,
entonces es evidente que se subutilizan importantemente los recursos disponibles.

Habiendo varios de estos casos en los kernels presentes, decidimos utilizar una t\'ecnica sugerida
por NVIDIA. Esta consiste en hacer las operaciones normalmente, como si todos los threads cumplieran
las condiciones del condicional y multiplicar por 1 o por 0 el resultado antes de acumular.
Esto hace que las cuentas que no se ejecutaban antes ahora lo hagan pero que simplemente no aporten
a la reducci\'on. Esta t\'ecnica elimina la existencia de la rama falsa de los condicionales, pero
trae dos problemas no triviales.

Uno de ellos es como solucionar el problema de los accesos a memoria.
Usualmente las guardas condicionales estan puestas para que el programa, si cumple ciertas condiciones,
no acceda a memoria que no esta definida. Por ejemplo, si \texttt{threadId > arraySize}, entonces claramente
no deberia acceder a \texttt{array[threadId]}, puesto que seria memoria invalida. Si eliminamos la guarda
entonces los accesos a memoria no pueden quedar igual. Una soluci\'on consiste en multiplicar tambien
por 1 o por 0 la direccion a la cual se va a acceder. Como CUDA maneja los arreglos de memoria
como C (es decir, basados en 0 como primer direcci\'on), esta t\'ecnica es v\'alida para hacer
siempre accesos correcto a memoria. El problema luego es en la coalescencia; como ahora algunos
threads de un warp van a acceder a una posicion de memoria muy distinta a otros, entonces el procesado
va a partir esos accesos en m\'ultiples transacciones. Esto puede hacer que la cuenta no solo no mejore
la performance, sino que puede que la empeore sustancialmente. Se debe hacer un profiling caso
por caso para poder estudiar el impacto en el kernel.

El otro problema es en cuentas que pueden dar NaN (como el tipico caso de division por cero).
Como, por estandar IEEE 754, los NaN hacen que todas las operaciones con ellos den NaN,
entonces pueden propagarse por la cuenta, incluso en con multiplicaci\'on con cero del resultado.
La soluci\'on m\'as evidante seria comprobar si son NaN antes de reducirlas, y si lo fueran, reemplazarlos
por cero. Esto puede llegar a ser inevitable en muchos casos, pero en el nuestro, si replanteamos
las cuentas, podemos evitar esos casos.

\subsection{Escalando m\'as all\'a de un GPU}
Una vez que fueron solucionados muchos limitantes de performance en los kernels del computo
de la densidad electr\'onica y del c\'alculo de la matriz RMM, nos encontramos en un punto donde
no fue posible determinar mejoras significativas en el c\'alculo para reducir tiempos.
Decidimos subir un nivel m\'as el paralelismo, de modo de poder solucionar multiples particiones
simultaneamente. Dado que es independiente el computo de cada partici\'on (salvo la acumulaci\'on
en la matriz RMM de salida y en la matriz de fuerzas interat\'omicas), nos pareci\'o que seria
interesante ver como escala distribuir el computo a lo largo de multiples GPU.

Para dividir el problema entre varios dispositivos usamos, al igual que en CPU, OpenMP. Definimos
una seccion paralela dentro del loop principal donde se soluciona cada grupo de modo que se
ejecutaran tantos threads como placas haya en la maquina. Cada
uno de los threads en el host se configurar\'a para una placa solamente. Esto se realiza con
una instrucci\'on del driver de CUDA (CudaSetDevice) que permite que durante toda la vida del
thread, todas las llamadas a kernels se realicen automaticamente al mismo dispositivo.

CUDA permite que trabajar con multiples placas de esta manera sea bastante sencillo. Las variables
definidas como \texttt{\_\_device\_\_}, que residen plenamente en la GPU, son automaticamente instanciadas
por cada dispositivo presente. De esta manera, es implicito cual variable usa cada kernel; la que
esta definida para su dispositivo actual. Esto puede ser un problema si queremos lograr comunicaciones entre placas,
pero si las cuentas son independientes, es paralelismo gratuito de costo cero.

El principal problema que surge de uso de m\'ultiples dispositivos radica, al igual que en
CPU, en como distribuir la carga de los threads de modo tal que haya una cantidad de trabajo
similar, para minimizar los tiempos de idle. Este problema no es tan grave siempre y cuando
se utilicen placas identicas dentro de la configuraci\'on del sistema ya que seria
el mismo tiempo si se corre en una o en otra. Un trabajo adicional de inter\'es ac\'a
radicaria en el uso de t\'ecnicas de estimacion de poder de computo para poder
distribuir el trabajo de manera equitativa entre modelos de placas heterog\'eneas, con distintas
configuraci\'ones de memoria, cantidad de SM y anchos de banda.

Para distribuir las tareas utilizamos dos t\'ecnicas combinadas, una para distribuir las
tareas estaticamente y otra para redistribuirlas dinamicamente de acuerdo al runtime de
cada tarea.

\subsubsection{Balanceo de cargas}

El balanceo de cargas optimo consiste en repartir $P$, el conjunto de problemas en
$n$ conjuntos disjuntos $P_i$, tal que $\bigcup_{i<n} P_i = P$. Este problema, conocido como PARTITION,
es NP-Completo en su version exacta, aunque son conocidos algoritmos pseudo-polinomiales para resolverlo.
La complejidad de ese algoritmo que usa programaci\'on din\'amica es $O(Nn)$, con $N$ el valor
de la suma de $P$ y $n$ la cantidad de elementos de $P$. Como nuestra m\'etrica para las
particiones es el runtime de estas expresados con precision de microsegundo, el costo computacional
de este algoritmo puede ser elevado en sistemas con cientos de particiciones de miles de puntos.
La principal desventaja de usar unicamente este algoritmo es que no contempla, o no al menos
en sus versiones m\'as directas, el uso de recursos asim\'etricos. Es decir, resolver la partici\'on
optima del problema de particiones cuando se sabe que el costo de resolver $P_i$ en el dispositivo
$A$ es distinto a resolverlo en el dispositivo $B$. Por este motivo, no podemos particionarlo estaticamente
usando este algoritmo y decidimos usar una soluci\'on h\'ibrida estatica y dinamica, reutilizando
la informaci\'on de runtime para decidir como rebalancear las particiones entre iteraciones.

Estaticamente definimos un orden para realizar toda la resoluci\'on del sistema y
se las distribuye de manera round-robin entre todos los dispositivos, para generar la
particion inicial y cargando a cada placa con una cantidad similar de tareas. Esto no signfica
que todas tarden lo mismo, y es posible notarlo en las mediciones.

Para hacer el balanceo dinamico, debemos determinar de ante mano la performance de cada
dispositivo con el que contemos. Para esto, se usa la tradicional t\'ecnica de definir un
caso de prueba representativo del problema, ejecutarlo en cada uno de los dispositivos y anotar
cuanto tarda en cada uno de ellos. Una gran ventaja de esto es que permite balancear facilmente las
cargas cuando se realizan operaciones de doble precision en conjuntos de dispositivos que incluyan
placas Tesla y placas Geforce. En simple precisi\'on, los topes de linea de cada una pueden tener
performance similares (por ejemplo, GTX 580 y M2090, GTX 780 y K20), pero en doble precisi\'on
usualmente las Tesla suelen ser entre 2 y 6 veces mas veloces. Como las cuentas se realizan
o todas en precision simple, o todas en precision doble, es sumamente importante particionar el
sistema tomando todo esto en cuenta.

El problema ejemplar elegido en este caso es realizar $10$ iteraciones del kernel de computo de la matriz RMM.
Consideramos que realizar este computo nos da una evaluaci\'on
emp\'irica de los recursos necesarios para resolver este problema. Este kernel utiliza muchos
de los recursos del dispositvo: realiza gran cantidad de accesos a memoria a traves
de la unidad de textura, usa principalmente instrucciones FMA, tiene una m\'axima ocupacion de los SM
y tiene un uso total de la memoria shared. No contabilizamos las copias de memoria desde y hacia la
placa de los parametros puesto que, al igual que en el c\'odigo de la simulaci\'on, casi la totalidad
de los datos se construyen directamente en la placa y se recuperan las matrices reducidas solamente al final
de la operaci\'on.

Teniendo las mediciones de tiempo para los $k$ dispositivos, se construye la matriz de
correcci\'on de runtimes con los $k^2$ los coeficientes de correcci\'on para poder estimar el tiempo de
ejecuci\'on en el nuevo dispostivo. Si los dispositivos presentes son identicos, entonces
la matriz va a tener solamente valores muy cercanos a 1, logicamente.

El pseudo c\'odigo del algoritmo de balanceo se detalla a continuaci\'on.
\begin{algorithm}
  \caption{Balanceo de duracion de threads}
  \label{ThreadBalancing}
\begin{algorithmic}
  \While{$ronda\_threads < max\_rondas\_threads$}
    \State $ronda\_threads++$

    \State $tiempo\_minimo \gets duracion[T_{min}]$
    \State $tiempo\_maximo \gets duracion[T_{max}]$

    \While {$tiempo_{max} < tiempo_{min} * k \wedge ronda\_trabajos < max\_ronda\_trabajos$}
      \State $ronda\_trabajos++$
      \State $\Delta T \gets (tiempo_{max} - tiempo_{min}) /2$
      \ForAll{$G_i \in Trabajos[T_{max}]$}
        \If{$duracion[G_i] * correccion[T_{max}][T_{min}] * penalidad\_migracion - \Delta T < tiempo_{G_{mejor\_candidato}}$}
          \State $G_{mejor\_candidato} \gets G_i$
        \EndIf
      \EndFor
     \State Liberar la memoria de las marices cacheadas de $G_{mejor\_candidato}$
     \State Mover $G_{mejor\_candidato}$ de $T_{max}$ a $T_{min}$
     \State Actualizar la duracion total de $T_{max}$ a $T_{min}$, aplicando la correcci\'on de tiempos
    \EndWhile
  \EndWhile
\end{algorithmic}
\end{algorithm}

En el algoritmo \ref{ThreadBalancing} se definen dos constantes adicionales, $k$ y $penalidad\_migracion$.
$k$ representa el coeficiente de m\'axima diferencia de tiempo entre el thread de mas larga duracion y el de menor
duracion. Para nuestros usos, un valor de $k \approx 5\%$ es aceptable.
$penalidad\_migracion$ es un coeficiente que agrega un costo al migrar el grupo a otra placa, ya que va
a tener que recalcular las matrices que ya se guardaba en memoria. Cuando cambie de placa, va a tener
que desalojarlas ya que, si bien es posible que entre las placas se accedan mutuamente a memoria global,
no vale la pena la latencia introducida, que se adicionar\'a a lo largo de todas las iteraciones de la resoluci\'on.
Este coeficiente es equivalente a definir una penalidad por romper la \textit{afinidad de las caches} en CPU.

Adicionalmente, el algoritmo \ref{ThreadBalancing} agrega dos contadores para este balanceo. El contador de
$ronda\_threads$ es el que sirve para balancear threads de a pares, necesario si hay mas de dos dispositivos.
El contador de $ronda\_trabajos$ es el que balancea, para el thread m\'as r\'apido y el m\'as lento, los
trabajos que van a migrar de un lado al otro. Es necesario agregar este contador por varios motivos. Por
un lado, si se mueven demasiados trabajos de un momento a otro, tal vez la estimaci\'on final deje de sear realista,
por lo que no tiene sentido seguir moviendo sin tener m\'as informaci\'on de otra corrida. Por otro lado, esta
heuristica puede ciclar infinitamente si no se cumple la condicion de que se puedan balancear los tiempos
con menos de diferencia $k$. En ninguno de nuestros casos de prueba nos hemos topado con que suceda esto, pero
consideramos que es correcto dejarlo.

Otra ventaja de usar este algoritmo adicionalmente a la partici\'on estatica es que, si bien se considera la matriz
de correcci\'on, la finalidad es balancear la carga minimizando los tiempos, por lo que incluso si la estimaci\'on original
de performance relativa era erronea, igualmente se redistribuiran los distintos grupos.


