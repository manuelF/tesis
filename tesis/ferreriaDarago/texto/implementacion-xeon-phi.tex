En base al c\'odigo de implementaci\'on de CPU, probamos el mismo
utilizando el Xeon Phi. Dado que nuestro inter\'es era estudiar la
practicidad del mismo para el problema, y siendo que nos concentramos
en la iteraci\'on, decidimos ver como se comportaba esta en modo
nativo, corriendo toda la aplicaci\'on LIO en el coprocesador.

El Xeon Phi utilizado para las pruebas corresponde al modelo de
61 procesadores, con 8 GB de memoria RAM GDDR5 y versi\'on de MPSS
(\textit{Multicore Platform Software Stack}) 3.2.3

Considerando los buenos resultados obtenidos con la implementaci\'on
en CPU, en t\'erminos de vectorizaci\'on del c\'odigo y de paralelizaci\'on,
usamos esta misma versi\'on del c\'odigo fuente como punto de partida. De
acuerdo a~\cite{Jeffers}, el dise\~no del Xeon Phi debiera aprovechar la
implementaci\'on realizada, teniendo en cuenta que el c\'odigo escala bien
en CPU.

\section{Caso de estudio}

Como se hizo en la secci\'on anterior, utilizaremos como ejemplo el caso de
estudio del grupo hemo, con tama\~no de cubos igual a 3.

\section{Resultados preliminares}

No podemos hacer comparaciones contra el c\'odigo original de CPU, recompilado
para Xeon Phi, por las siguientes dos razones:

\begin{itemize}
    \item El mismo utilizaba la librer\'ia GSL (GNU Scientific Library) para
    ciertas subrutinas num\'ericas en CPU. Esta librer\'ia fue eliminada puesto
    que requer\'ia ser recompilada especialmente para Xeon Phi y no era necesaria.
    \item Para la implementaci\'on de vectores $\mathbb{R}^3$, como ya explicamos,
    se utilizaban extensiones vectoriales especiales del compilador apuntando a
    SSE 4. Dado que este set de instrucciones es incompatible con el del Xeon Phi
    fue necesario remover esta optimizaci\'on.
\end{itemize}

Por lo tanto, las comparaciones se realizan contra la versi\'on optimizada para
CPU como se ha descripto en la secci\'on anterior. El c\'odigo fuente utilizado
es exactamente el mismo que en la versi\'on terminada en CPU, con la diferencia
que es compilado para la arquitectura MIC, junto con todas las librer\'ias
utilizadas.

En primera instancia obtuvimos resultados muy desalentadores en la versi\'on
\textit{monoprocesador} del c\'odigo, como puede verse en la figura~\ref{fig:prelim-xeon-phi}.
Para el caso particular de la iteraci\'on de intercambio y correlaci\'on XC pueden
verse los resultados en la figura~\ref{fig:prelim-xeon-phi-xc}. Para el resto de
los pasos de la iteraci\'on SCF, los resultados est\'an en la figura~\ref{fig:prelim-xeon-phi-scf}.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/xeonphi/xeon-xeon-phi-broad-comparison.png}
   \caption{Tiempo de ejecuci\'on para las distintas partes del paquete LIO,
   comparativamente para Xeon y Xeon Phi, en el caso del grupo hemo con un solo procesador}
   \label{fig:prelim-xeon-phi}
\end{figure}

Tanto el c\'omputo de pesos, funciones y la iteraci\'on en si para el caso de la
hemoglobina muestran una alentizaci\'on muy fuerte, incluso considerando que
todos los ciclos fueron vectorizados por el compilador de acuerdo a los reportes
generados por el mismo.

En el caso de la iteraci\'on vemos que esta alentizaci\'on es com\'un a los dos
ciclos principales de la iteraci\'on usual del c\'alculo de intercambio y
correlaci\'on: tanto para el c\'alculo de la densidad electr\'onica como la
actualizaci\'on de la matriz de Kohn Sham.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/xeonphi/xeon-xeon-phi-broad-comparison-xc.png}
   \caption{Tiempo de ejecuci\'on para las distintas partes de la iteraci\'on XC,
   comparativamente para Xeon y Xeon Phi, en el caso del grupo hemo con un solo procesador}
   \label{fig:prelim-xeon-phi-xc}
\end{figure}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/xeonphi/xeon-xeon-phi-broad-comparison-scf.png}
   \caption{Tiempo de ejecuci\'on para las distintas partes de la iteraci\'on SCF,
   excluyendo la iteraci\'on de XC, comparativamente para Xeon y Xeon Phi, en el
   caso del grupo hemo con un solo procesador}
   \label{fig:prelim-xeon-phi-scf}
\end{figure}

Corroboramos entonces la pobre \textit{performance} del Xeon Phi en c\'odigo
principalmente serial, incluso con c\'odigos sencillos y vectorizados por el
compilador.

Este factor, la baja performance con respecto al Xeon, impacta fuertemente en toda
la aplicaci\'on, incluso en la parte paralelizada. Al poner m\'as presi\'on en
el uso efectivo de los procesadores, incrementa muy fuertemente las perdida
predicha por la ley de Amdahl, resultando en no solo una baja \textit{performance}
sino en una baja escalabilidad.

Un ejemplo de estos efectos puede verse en la paralelizaci\'on de los grupos chicos
en cargas para cada procesador. Al ser el c\'odigo serial en el Xeon Phi muy poco
performante, se vuelve necesario utilizar m\'as procesadores para un mismo grupo.
Esto pone presi\'on en usar un \textit{threshold} m\'as chico para considerar
m\'as grupos grandes. Pero esto impacta negativamente en la escalabilidad ya que:

\begin{itemize}
    \item Utilizar m\'as threads de los que pueden ser aprovechados por un grupo
    disminuye la escalabilidad, porque deja procesadores ociosos.
    \item El algoritmo de partici\'on cuenta con menos grupos para manejar, con
    lo cual tiene menos capacidad de balanceo entre los procesadores. El
    desbalanceo tambi\'en produce que procesadores esten ociosos innecesariamente.
\end{itemize}

Por ejemplo, en la figura~\ref{fig:partition-balance} se puede ver el balance
de la partici\'on resultante de utilizar 60 procesadores y la funci\'on de costo
y valores de divisi\'on calculados en la secci\'on anterior. Se detalla tanto
el desbalance te\'orico (predicho por la partici\'on) como el balance de tiempo.
El balance de tiempo es especialmente pronunciado, y puede verse un incremento
muy pronunciado del tiempo de ejecuci\'on tambi\'en para grupos chicos. Al ser
los mismos ejecutados serialmente, este incremento es consistente con los
resultados anteriores, pero disminuye el atractivo de la divisi\'on en cargas de
los grupos chicos en el Xeon Phi.

\begin{figure}[htbp]
   \centering
   \begin{subfigure}[b]{\plotwidthtres}
     \includegraphics[width=\textwidth]{plots/xeonphi/xeon-phi-partition-balance-theoretical.png}
     \caption{Costo total predicho para la partici\'on}
   \end{subfigure}
   \begin{subfigure}[b]{\plotwidthtres}
       \includegraphics[width=\textwidth]{plots/xeonphi/xeon-phi-partition-balance-empirical.png}
     \caption{Tiempo de ejecuci\'on efectivo}
   \end{subfigure}
   \caption{Comparaci\'on entre la menor y mayor de las cargas para \textit{threads},
   seg\'un el tipo de parametro, para una iteraci\'on del grupo hemo en Xeon Phi, usando
   la funci\'on de costo y divisi\'on en grupos obtenida, en 60 threads.}
   \label{fig:partition-balance}
\end{figure}

De lo anterior se desprende tambi\'en la necesidad de modificar la funci\'on de costo
para el Xeon Phi. En esta arquitectura todo c\'omputo serial es muy fuerte, con
lo cual ahora no se tiene un \textit{overhead} por cubo tan pesado con relaci\'on
al c\'omputo. Por esto la funci\'on de costo anterior no resulta \'util, como
puede verse en la figura~\ref{fig:cost-function-xeon-phi}, y que tambi\'en explica
porque el desbalance se acrecent\'o tanto en la figura~\ref{fig:partition-balance}.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/xeonphi/cost-fit-xeon-phi.png}
   \caption{Tiempo de resoluci\'on en milisegundos en funci\'on del costo predicho
   por el algoritmo para CPU, para cada grupo, en el caso del grupo hemo en Xeon Phi.
   Como puede verse, no hay una correlaci\'on fuerte entre estos valores.}
   \label{fig:cost-function-xeon-phi}
\end{figure}

Adicionalmente, tampoco resulta \'util el balance de cargas en este caso: La baja
performance serial provoca que cada c\'alculo sea m\'as pesado. Por lo tanto, las
cargas m\'as pesadas (incluso para varios valores del \textit{threshold}) dominan
un procesador entero y atrasan a todos los dem\'as, y como la carga consiste de
un solo grupo balancearla no tiene sentido.

Es tambi\'en importante notar que la escalabilidad para los cubos
y esferas grandes es menor que para la divisi\'on entre procesadores de los cubos
chicos (debido a componentes seriales
en la iteraci\'on) con lo cual incrementar la cantidad de cores produce un
incremento correspondiente a la ley de Amdahl. Como la partici\'on para grupos
chicos se vuelve menos efectiva, esta parte pasa a ser m\'as importante, bajando
entonces la escalabilidad del c\'odigo.

El aumento de \textit{threads} y su caracter serial (de movimiento de memoria
principalmente) hace m\'as pesadas las operaciones de juntado de resultados intermedios.
A diferencia de CPU, donde este paso de reducci\'on de las matrices a las matrices
globales era despreciable, en el Xeon Phi pasa a tener un valor considerable, como
puede verse en la figura~\ref{fig:enditer-xeon-phi}. Esto disminuye a\'un m\'as
las ventajas obtenidas por la paralelizaci\'on externa, ya que m\'as procesadores
involucrados m\'as matrices es necesario reducir.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/xeonphi/enditer-xeon-phi.png}
   \caption{Tiempo de reducci\'on de las matrices de fuerzas y Khom-Sham para
   la iteraci\'on del grupo hemo en Xeon Phi, seg\'un la cantidad de threads.}
   \label{fig:enditer-xeon-phi}
\end{figure}

Como conclusi\'on entonces, vemos que en el caso de Xeon Phi no tiene sentido
realizar una paralelizaci\'on de cubos y esferas chicos y grandes como la realiza
la implementaci\'on CPU. Para hacer corresponder esto, empleamos un \textit{threshold}
con lo cual todo grupo es considerado grande y ejecutado con todos los
threads disponibles.

Finalmente entonces, utilizamos solamente paralelizaci\'on dentro de cada grupo
para el Xeon Phi con estas razones, en base a los \textit{trade-offs} presentados.
