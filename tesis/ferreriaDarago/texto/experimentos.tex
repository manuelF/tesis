\section{Aceleraciones alcanzadas en CPU}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/escalabilidad-caroteno.png}
   \caption{Grupo Caroteno}
   \label{fig:cpu-scalability-caroteno}
 \end{subfigure}
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/escalabilidad-fullereno.png}
   \caption{Grupo Fullereno}
   \label{fig:cpu-scalability-fullereno}
 \end{subfigure}
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/escalabilidad-hemoglobina.png}
   \caption{Grupo Hemoglobina}
   \label{fig:cpu-scalability-hemo}
 \end{subfigure}
 \caption{\textit{Speedup} conseguidos para la iteraci\'on XC en Xeon, seg\'un
 la clase de problema examinado.}
 \label{fig:cpu-scalability}
\end{figure}

Las figuras~\ref{fig:cpu-scalability-hemo},~\ref{fig:cpu-scalability-caroteno} y
~\ref{fig:cpu-scalability-fullereno} muestran el \textit{speedup} conseguido en
el c\'odigo de CPU en Xeon para el grupo Hemo, Caroteno y Fullereno respectivamente.

Los an\'alisis se realizaron utilizando el mismo Xeon que en los an\'alisis
realizados anteriormente. Puesto que queremos considerar escalabilidad en cantidad
de n\'ucleos sim\'etricos de procesamiento, se deshabilit\'o Turbo Boost y
\textit{Hyperthreading} para estos experimentos.

Los resultados obtenidos son muy buenos, logr\'andose una escalabilidad de 11.39,
11.14 y 11.53 en Fullereno, Caroteno y Hemoglobina respectivamente.

Dada la heterogeneidad de estos grupos podemos concluir entonces que el c\'odigo
hace un excelente uso de las prestaciones de procesadores multicore tradicionales
de la arquitectura Xeon en una iteresante variedad de casos.

\section{Aceleraciones alcanzadas en CPU}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/initial-scf-hemo.png}
   \caption{Pre-optimizaciones.}
   \label{fig:initial-cpu-scf}
 \end{subfigure}
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/final-scf-hemo.png}
   \caption{Post-optimizaciones.}
   \label{fig:final-cpu-scf}
 \end{subfigure}
 \caption{Porcentajes de tiempo de SCF corriendo Hemoglobina en Xeon E5-2620 antes y despu\'es de
 las optimizaciones realizadas.}
 \label{fig:cpu-scf}
\end{figure}

Los tiempos finales de ejecuci\'on para CPU en el caso del servidor Xeon utilizado se encuentran
en la figura~\ref{fig:cpu-scf} para hemoglobina. El \textit{speedup} final alcanzado por XC es del
\'orden de 22 veces. Tambi\'en podemos ver una ligera mejora en el tiempo de SCF,
producto de la vectorizaci\'on utilizada por MKL (\textit{Math Kernel Library}) de
Fortran.

La divisi\'on en partes de la iteraci\'on XC se encuentra en la figura~\ref{fig:cpu-parts}. La
proporci\'on de tiempo se mantiene, con lo cual esfuerzos adicionales de optimizar deber\'ian
guiarse en primera instancia en el resto de la iteraci\'on de SCF y luego en el c\'alculo de
la densidad electr\'onica en XC.

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/initial-iteration-parts-hemo.png}
   \caption{Pre-optimizaciones.}
   \label{fig:initial-cuda-parts}
 \end{subfigure}
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cpu/final-iteration-parts-hemo.png}
   \caption{Post-optimizaciones.}
   \label{fig:final-cuda-parts}
 \end{subfigure}
 \caption{Porcentajes de tiempo de los pasos de XC corriendo Hemoglobina en una Xeon ES-2620 antes y despu\'es de
 las optimizaciones realizadas.}
 \label{fig:cpu-parts}
\end{figure}

\section{Aceleraciones alcanzadas en CUDA}
\label{resultados-cuda}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cuda/initial-scf-hemo.png}
   \caption{Pre-optimizaciones.}
   \label{fig:initial-cuda-scf}
 \end{subfigure}
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cuda/final-scf-hemo.png}
   \caption{Post-optimizaciones.}
   \label{fig:final-cuda-scf}
 \end{subfigure}
 \caption{Porcentajes de tiempo de SCF corriendo Hemoglobina en una Tesla K40 antes y despu\'es de
 las optimizaciones realizadas.}
 \label{fig:cuda-scf}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cuda/initial-iteration-parts-hemo.png}
   \caption{Pre-optimizaciones.}
   \label{fig:initial-cuda-parts}
 \end{subfigure}
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/cuda/final-iteration-parts-hemo.png}
   \caption{Post-optimizaciones.}
   \label{fig:final-cuda-parts}
 \end{subfigure}
 \caption{Porcentajes de tiempo de los pasos de XC corriendo Hemoglobina en una Tesla K40 antes y despu\'es de
 las optimizaciones realizadas.}
 \label{fig:cuda-parts}
\end{figure}

Las figuras \ref{fig:cuda-parts} y \ref{fig:cuda-scf} muestran la fracci\'on de tiempo que insum\'ia el
calculo de XC dentro de SCF, corrido con la GPU mas potente en el mercado, la Tesla K40.
Dentro del calculo del termino de intercambio correlaci\'on, el calculo de la densidad electr\'onica
representaba, por mucho, el cuello de botella. Al reducirse tanto ese y guardar las funciones en
memoria para no tener que recalcularlas constantemente, se obtuvo una aceleraci\'on alrededor de 8
veces en Kepler y de 5 veces en Fermi, como se ve en la figura \ref{fig:cuda-final}.

La implementaci\'on de t\'ecnicas de multi-GPU tambi\'en obtuvieron grandes mejoras, como se vio en la
secci\'on \ref{cuda-multiplaca}. Incluso no teniendo una mejora lineal en el calculo de simple
precisi\'on, esta t\'ecnica permite explotar todos los recursos disponibles, haciendo que el termino
XC no sea ya el mas costoso de la iteraci\'on SCF y abriendo nuevas posibilidades de mejoras en el
resto de los c\'alculos.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{plots/cuda/final.png}
   \caption{Aceleraci\'on en veces del c\'alculo de XC de correr Hemoglobina comparando implementaci\'on original en CUDA contra
   las versiones optimizadas finales. El (*) marca resultados teoricos extrapolados de las
 aceleraciones alcanzadas en la seccion \ref{cuda-multiplaca}}
   \label{fig:cuda-final}
\end{figure}


\section{Performance contra Xeon Phi}

\section{Tama\~no de los grupos}
\label{tamgrupos}
Un par\'ametro que se experimento durante el desarrollo del trabajo fue el tama\~no de las particiones.
Los cubos y las esferas que agrupan los puntos fueron descriptos en la secci\'on \ref{implementacion}. El par\'ametro
del tama\~no de los cubos siempre se considero que deb\'ia ser ``grande'' para GPU mientras que deb\'ia ser
``chico'' para CPU. Para comprobar el impacto, analizamos para cada arquitectura el tiempo de ejecuci\'on
de cada grupo en funci\'on del costo computacional, definido en la secci\'on \ref{PredictorCPU}. Este costo ademas
sirve para determinar cuales grupos deben hacerse en paralelo, y en cuales usar el paralelismo interno. Estos
los clasificamos como grupos \textit{chicos} y \textit{grandes} respectivamente. En GPU no se realiza esa
distinci\'on. Luego, evaluamos el tiempo de ejecuci\'on acumulado para cada una de estas cuatro categor\'ias.


\begin{figure}[htbp]
\centering
\begin{tabular}{cc}
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/otros/cpu-gpu-size-3.png}
   \caption{Grupos de tama\~no 3.}
 \end{subfigure} &
 \begin{subfigure}[b]{\plotwidthtres}
   \includegraphics[width=\textwidth]{plots/otros/cpu-gpu-size-7.png}
   \caption{Grupos de tama\~no 7.}
 \end{subfigure} \\
 \end{tabular}
 \caption{Tiempos acumulados de ejecutar Hemoglobina, separados por tama\~no de grupos y
 caracterizaci\'on.}
 \label{fig:group-times}
\end{figure}

La figura \ref{fig:group-times} aclara el por que usar grupos chicos para CPU y grupos grandes
para GPU. El overhead que se ve en GPU al hacer grupos chicos es sumamente elevado, a pesar de que
el costo computacional sea relativamente bajo. Esto se puede ver tanto en grupos de tama\~no
3 como tama\~no 7. Este overhead, sin embargo, se vuelve despreciable cuando se analiza el costo
de resolver los grupos de mayor tama\~no. Consistentemente el GPU obtiene un menor tiempo de ejecuci\'on
con respecto al CPU usando la paralelizaci\'on interna. Esto indica que si la partici\'on genera demasiados
grupos, GPU no va a poder superar el overhead de resolver cada uno, haciendo que el tiempo acumulado sea
mayor que el de CPU que tiene una peor performance en particiones grandes.

\section{?`Qu\'e me conviene comprar?}
Luego de todas las mejoras realizadas a la aplicaci\'on LIO, queda la interrogante pendiente. Para
correr simulaciones de QM, ?`Qu\'e versi\'on me conviene correr, CPU o GPU?

Esta pregunta es siempre capciosa y es susceptible a cambios tecnol\'ogicos muy r\'apidamente. Intentaremos
responderla utilizando dos \textit{tiers} donde correr LIO, uno para una estaci\'on de trabajo
y el otro para un servidor de c\'omputo. Definimos, para cada configuraci\'on, una versi\'on para GPU y otra
para CPU de modo de priorizar los recursos invertidos. Se busc\'o que los costos de ambos niveles
sean parejos, de modo de hacer la comparaci\'on realista de que conviene invertir.

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Configuraci\'on& Estaci\'on de trabajo                                                                                                      & Servidor                                                                                              \\ \midrule
GPU                      & \begin{tabular} [c]{@{}l@{}} Intel Core i5-4460 @ 3.2GHz\\ 8GB RAM DDR3\\ 1 o 2 x GeForce GTX 780 3GB\end{tabular} & \begin{tabular}[c]{@{}l@{}}Intel Xeon E5-2609\\ 16GB RAM DDR3\\ 1 o 2 x NVIDIA Tesla K40 12GB\end{tabular} \\ \hline
CPU                      & \begin{tabular}[c]{@{}l@{}}Intel Core i7-3770 CPU @ 3.40GHz \\ 16GB RAM DDR3\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}2 x Intel Xeon E5-2620 v2 @ 2.10GHz\\ 32GB RAM\end{tabular}                             \\ \bottomrule
\end{tabular}
\label{tbl:configs}
\caption{Distintas configuraciones de estaciones de trabajo y servidores para computo de QM/MM usando LIO}
\end{table}

%en hemo k40
%total iter = 1s 331302
%iteration = 522937
%TODO especificar bien en algo medible.

Lo que primero se debe notar en GPU es que se usan placas que tienen potencia de c\'alculo casi equivalente
para las cuentas en simple precisi\'on. Sin embargo, elegimos una Tesla para configuraci\'on de servidor
porque son las placas mejor preparadas para HPC. Estas cuentan con cuatro veces mas memoria en la placa
que tiene ECC y con mayor MTFB (\textit{Mean Time Between Failures}), factor vital en servidores
que deben correr confiablemente.

La m\'etrica que usaremos para comparar los distintos sistemas es cuantas iteraciones del c\'alculo de
SCF se pueden ejecutar por d\'ia. Esto lo hacemos para medir estrictamente la performance
de QM, importante en simulaciones de \textit{Time-Dependant Density Functional Theory} por ejemplo, y no
hablar de las implementaciones de los sistemas de QM/MM que utilizan LIO.
Esta m\'etrica es similar a las que se usan en el \'area de MM. Programas como Amber\cite{Amber} la usan para comparar
distintas configuraciones de hardware sobre la cual correr mejor.

\begin{figure}[htbp]
\centering
  \begin{subfigure}[b]{\plotwidthtres}
    \includegraphics[width=\plotwidthtres]{plots/otros/its-xc-dia.png}
    \caption{Precisi\'on simple.}
  \end{subfigure}
  \begin{subfigure}[b]{\plotwidthtres}
    \includegraphics[width=\plotwidthtres]{plots/otros/its-xc-dia-double.png}
    \caption{Precisi\'on doble.}
  \end{subfigure}
  \caption{Cantidad de iteraciones de XC por d\'ia realizables usando distintas configuraciones corriendo Hemoglobina usando los
            par\'ametros \'optimos correspondientes despues de todas las optimizaciones. El (*) marca resultados teoricos extrapolando
            las aceleraciones alcanzadas en la secci\'on \ref{cuda-multiplaca}.}
  \label{fig:its-xc-dia}
\end{figure}


\begin{figure}[htbp]
\centering
  \begin{subfigure}[b]{\plotwidthtres}
    \includegraphics[width=\plotwidthtres]{plots/otros/its-dia.png}
    \caption{Precisi\'on simple.}
  \end{subfigure}
  \begin{subfigure}[b]{\plotwidthtres}
    \includegraphics[width=\plotwidthtres]{plots/otros/its-dia-double.png}
    \caption{Precisi\'on doble.}
  \end{subfigure}
    \caption{Cantidad de iteraciones de SCF por d\'ia realizables usando distintas configuraciones corriendo Hemoglobina usando los
              par\'ametros \'optimos correspondientes despues de todas las optimizaciones. El (*) marca resultados teoricos extrapolando
              las aceleraciones alcanzadas en la secci\'on \ref{cuda-multiplaca}.}
    \label{fig:its-dia}
\end{figure}

Un factor a tomar en cuenta para analizar la performance de diversas configuraciones, como en la figura \ref{fig:its-dia},
es que el c\'alculo de SCF incluye contribuciones que no son solamente las relativas a intercambio correlaci\'on
(las estudiadas en esta tesis), por los cuales los tiempos de ejecuci\'on totales van
a presentar menores aceleraciones que las presentadas anteriormente. La cantidad de iteraciones de XC se ven
exclusivamente en la figura \ref{fig:its-xc-dia}. Mostramos de manera independiente el calculo de XC porque
creemos que es posible conseguir mejoras similares en el resto de las contribuciones de SCF.

La principal ventaja que se obtiene de correr en las estaciones de trabajo con GPU es la potencia
de c\'alculo concentrado en los dispositivos GPU. La mayor aceleraci\'on del calculo de XC se obtiene
usando dos GeForce GTX 780, en comparaci\'on con unicamente usando el procesador. Esto se condice con las hip\'otesis discutidas
anteriormente sobre que los n\'ucleos del procesador se encuentran completamente ocupados.
En cambio, las estaciones de trabajo con CPU cuentan con la ventaja de que se acelera tambi\'en el
resto de las operaciones que componen una iteraci\'on de SCF. Estos c\'alculos son de menor costo computacional
comparada con las de XC, pero luego de las aceleraciones alcanzadas por este trabajo, el peso de estas
es comparativamente grande. Como estas otras contribuciones de SCF no est\'an
aceleradas con GPU, ese recurso se subutiliza fuera de XC.

Cuando se comparan las configuraciones de servidores, el panorama cambia sustancialmente.
Las aceleraciones obtenidas en la versi\'on GPU provienen principalmente de un solo factor,
la mayor cantidad de memoria en la GPU. Esta memoria permite entonces mantener las matrices
de funciones en memoria sin tener que recalcularlas. Esta optimizaci\'on se detall\'o en la secci\'on
\ref{GuardarFunctionsGPU}. Como la GeForce GTX 780 cuenta con una velocidad de clock mayor a
la K40 (probablemente porque en la linea de placas para v\'ideo juegos sea m\'as importante
la performance que la estabilidad) las operaciones se realizan incluso mas r\'apido en la
versi\'on de consumidor. Sin embargo, la Tesla K40 tiene un costo alrededor de ocho veces mayor que la
GeForce GTX 780.

La configuraci\'on de CPU en cambio, la performance escala linealmente en la cantidad de cores.
Dadas las optimizaciones realizadas en CPU priorizando la escalabilidad, es de notar la gran
diferencia de performance con respecto a la configuraci\'on de estaci\'on de trabajo. Priorizar CPU
favorece tambi\'en a todo el resto del c\'alculo de SCF. Las as contribuciones que no son
de intercambio-correlaci\'on se resuelven, o bien a trav\'es de librer\'ias BLAS en CPU, o bien
aprovechando las t\'ecnicas de paralelizaci\'on autom\'atica que brindan los compiladores usados.

Comparando ambas configuraciones de servidores, hoy en d\'ia es mucho mas f\'acil encontrar
nodos de computo de HPC usando m\'ultiples procesadores que nodos con varias GPU por varios motivos.
Primero, porque son mas generales: los nodos se pueden usar para m\'ultiples aplicaciones de HPC,
todav\'ia mayormente basados en CPU. Segundo, motivos energ\'eticos: el consumo energ\'etico de un cluster
HPC es muy elevado y las GPU pueden tener altos consumos que hacen muy dif\'icil la disipaci\'on t\'ermica.
Finalmente, el costo: las placas GPU de la linea de servidores cuestan mas de seis veces que los
procesadores de muy alta gama que se usan en HPC. Si las aplicaciones no van a hacer uso constante
de estas y tener aceleraciones comparables con el costo, entonces puede incluso no ser rentables
para las aplicaciones .


