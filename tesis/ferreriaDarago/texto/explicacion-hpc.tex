Una parte significativa del impacto de las computadoras en los distintos aspectos de las
ciencias, e incluso de la vida diaria, se debe al crecimiento de su
poder de c\'omputo desde la aparici\'on de los primeros microprocesadores hasta el
d\'ia de hoy.

Entre 1986 y 2002, la performance de los procesadores crec\'io en promedio, un 50\%
por a\~no~\cite{Pacheco}, en parte gracias a la denominada \textit{Ley de Moore}, que
establec\'ia que la densidad de transistores por circuito integrado se duplicaba
cada 18 meses~\cite{HennessyPatterson}. Esto puede verse en la figura~\ref{moore-law},
que compara la cantidad de transistores de distintos procesadores desde 1971 a la actualidad.

Este crecimiento constante permiti\'o a los desarrolladores de procesadores incrementar la potencia
de c\'alculo, duplicando la performance cada 24 meses. Esto benefici\'o durante mucho tiempo a los desarrolladores
de aplicaciones, que escribian programas de manera serial, y al poco tiempo los procesadores
mejorar\'ian, y con ellos los tiempos de ejecuci\'on de sus c\'odigos de aplicaci\'on (fen\'omeno denominado
\textit{hardware free lunch}).

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\textwidth]{images/moore-law.pdf}
   \caption{Cantidad de transistores en procesadores emblematicos desde 1971}
   \label{moore-law}
\end{figure}

Sin embargo, a medida que los transistores disminuyen su tama\~no, aumentan su
disipaci\'on t\'ermica por unidad de superficie.  Esto limita la cantidad que se pueden ubicar de los mismos
en un circuito sin producir que este se comporte de manera err\'atica producto
de la elevada temperatura de operaci\'on. El mismo motivo impide el crecimiento de frecuencia de reloj, uno de los
principales motores de avance en eficiencia. Los problemas t\'ermicos han implicado que
desde el 2002, la tasa de crecimiento de la performance de los monoprocesadores haya disminuido a un 20\% anual.
Consecuentemente, los principales fabricantes de procesadores han modificado el enfoque de investigaci\'on y dise\~no, empezando
a hacerse m\'as y m\'as com\'un el uso de m\'ultiples procesadores por chip para mantener las mejoras de rendimiento.

La preocupaci\'on por la disipaci\'on y el consumo energ\'etico insustentables han sido motivadores
de dise\~nos con menor frecuencia de clock, pero aprovechando las a\'un crecientes densidades de
transistores para incrementar las unidades de soporte. Esta estrategia ha resultado en que, en un
CPU moderno, menos del 20\% de todos los transistores disponibles se utilicen para realizar c\'alculos.

Adicionalmente, las mejoras de performance debidas al paralelismo
a nivel de instrucciones (mediante t\'ecnicas como ejecuci\'on fuera de \'orden,
ejecuci\'on especulativa, \textit{pipelining}, etc.) han sido
progresivamente menores. Actualmente los esfuerzos invertidos en ese area se han concentrado en
el paralelismo a nivel de datos (vectorizaci\'on) y paralelismo a nivel de tareas (multiprocesadores).~\cite{HennessyPatterson}

Este enfoque en dise\~no de arquitecturas hacia otros tipos de paralelismo puede verse tanto
en nuevos productos en las l\'ineas establecidas (por ejemplo los procesadores Intel i3, i5 e i7)
asi como tambien en nuevos desarrollos que apunten a computo de alta performance.
La revalorizaci\'on de las placas gr\'aficas (GPUs) para problemas de computo intensivo, y los desarrollos
nuevos como la arquitectura MIC (\textit{Many Integrated Core Architecture}) de Intel son
claros ejemplos de esta tendencia.

El impacto de este enfoque hacia multiples hilos de ejecuci\'on en paralelo en el
desarrollo de aplicaciones es significativo. En simulaciones para las \'areas de biolog\'ia,
medicina, qu\'imica o meteorolog\'ia es de invaluable utilidad minimizar los tiempos
de ejecuci\'on, para permitir mejores implementaciones de los modelos utilizados,
permitiendo realizar predicciones de mayor calidad. Aprovechar las nuevas
arquitecturas multiprocesador requiere modificaciones en el c\'odigo no triviales,
a diferencia del crecimiento en velocidad de \textit{clock} que no requer\'ia modificaciones
en el dise\~no del programa. Los intentos de escribir programas que conviertan
programas seriales (dise\~nados para un solo procesador) a paralelos, en lenguajes
de prop\'osito general como C, C++ o Fortran, han sido relativamente infructosos.

El resultado es que es necesario trabajo a nivel de escritura del \textit{software}
para utilizar multiples procesadores. La aparici\'on de nuevas herramientas ayudan
al programador en esta tarea. Un ejemplo de esto es Nvidia CUDA (\textit{Compute
Unified Device Architecture}), que provee un lenguaje de programaci\'on unificado
para el desarrollo de aplicaciones que exploten la arquitectura de unidades GPGPU
(\textit{General Purpose Graphical Processing Units}). Otros ejemplos los podemos
ver en APIs y librer\'ias unificadas de desarrollo como OpenMP o MPI
(\textit{Message Passing Interface}), trabajando conjuntamente con compiladores
optimizantes como Intel ICC y PGI Fortran.

Estas herramientas, sin embargo, no son \textit{silver bullets}. Todavia la divisi\'on
del trabajo es inherente al problema a resolver, en base a las dependencias de las tareas
a realizar para la soluci\'on del mismo. Realizar esta division es una labor que, hasta
el dia de hoy, se relega en el programador especializado.

En este trabajo buscaremos comparar distintas arquitecturas de hardware y como
las caracter\'isticas espec\'ificas de la simulaci\'on qu\'imica a realizar permiten
o impiden paralelizaci\'on de trabajo empleando los distintos recursos que cada
arquitectura provee.
