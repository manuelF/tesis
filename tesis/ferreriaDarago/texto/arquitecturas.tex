\section{CPU}

Los microprocesadores ganan en prevalencia desde principios de los 70, cuando Intel introduce los modelos 4004 y 8008.
Actualmente las arquitecturas de procesadores de 64 bits basadas en la l\'inea x86-64 de Intel dominan no solo el mercado de computadoras personales sino que tambi\'en el de servidores y \textit{clusters} de c\'omputo.
Dado que utilizaremos el CPU est\'andar como punto de comparaci\'on para las dem\'as arquitecturas, y que estas est\'an basadas en parte en su dise\~no, daremos una breve rese\~na de los aspectos m\'as importantes y desarrollos modernos con respecto a la performance dentro de los procesadores modernos.

\subsection{Tipos de paralelismo}

Existen tres categor\'ias de paralelismo que hoy en d\'ia una arquitectura puede aprovechar para mejorar la \textit{performance} de una aplicaci\'on:
\begin{itemize}
    \item \textit{Instruction Level Parallelism}: Este tipo de optimizaciones buscan
    ejecutar la mayor cantidad de instrucciones en un mismo hilo de ejecuci\'on simult\'aneamente.
    Optimizaciones de este estilo incluyen:
    \begin{itemize}
      \item \textit{pipelines} de procesador: se separan las instrucciones en distintas etapas para ejecutar m\'ultiples instrucciones de manera solapada.
      \item ejecuci\'on superescalar fuera de orden: para ejecutar m\'ultiples instrucciones que utilizan unidades del procesador distintas o que no dependen una de la otra.
      \item ejecuci\'on especulativa: se basa en la predicci\'on de resultados todav\'ia no finalizados de procesar.
    \end{itemize}

    \item \textit{Data Level Parallelism}: Consideran las optimizaciones cuyo prop\'osito es lograr aplicar una misma operaci\'on a cada elemento de un conjunto datos simult\'aneamente
    en un mismo hilo de ejecuci\'on. Esta t\'ecnica se denomina SIMD (\textit{Single Instruction, Multiple Data}).

    \item \textit{Thread Level Parallelism}: Concierne al uso de m\'ultiples hilos de ejecuci\'on simult\'aneos, lo cual requiere el uso de procesadores que usualmente
    comparten la memoria principal (arquitectura SMP, \textit{Symmetric Multiprocessing}).
    Esto normalmente conlleva esfuerzo adicional para mantener consistencia y coherencia.
\end{itemize}

En base a estos tipos de paralelismo, surgieron grandes avances en la arquitectura de procesadores desde 1950 en adelante.
Estos se detallan cronol\'ogicamente en la tabla \ref{tbl:historia-cpu}, como tambi\'en la motivaci\'on de su existencia.

\newcommand{\blap}[1]{\begin{minipage}[t]{3in}#1\end{minipage}}


\begin{table}[h]
\renewcommand{\arraystretch}{2.0}
\centering
\begin{tabular}{lll}
\hline
A\~no & Evento & Motivo \\ \hline
1950s & Pipelines &  \blap{Usar independientemente las distintas unidades del procesador para aumentar las instrucciones por ciclo.} \\
1970s & Vectorizaci\'on & \blap{Poder realizar las mismas operaciones en muchos datos simult\'aneamente.} \\
1980s & Cach\'e & \blap{La diferencia de velocidad entre la memoria y el procesador se segu\'ia acrecentando.} \\
1987 & M\'ultiples ALUs & \blap{Poder compensar las ALU que mas latencia tienen, se multiplican las unidades para procesar mas instrucciones en paralelo.} \\
1988 & Predicci\'on de Saltos & \blap{Disminuir y hasta evitar los costos de tener que recalcular el pipeline cuando hay saltos.} \\
1995 & Fuera de Orden & \blap{Seguir ejecutando instrucciones mientras se esperan datos anteriores.} \\
2002 & SMT & \blap{Poder procesar de a mas de un thread de OS en un solo core.} \\
2005 & Multi-core & \blap{Aumentar el poder de computo sin aumentar la velocidad de clock ni consumo.} \\
\end{tabular}
  \caption{Cronolog\'ia de los avances de arquitectura de procesadores.}
  \label{tbl:historia-cpu}
\end{table}

A continuaci\'on detallamos algunos aspectos de cada una de estas t\'ecnicas.

\subsection{Pipeline y Ejecuci\'on fuera de orden}

Los primeras implementaciones de paralelismo a nivel de un solo procesador fueron a nivel de instrucciones mediante el uso de \textit{pipelines} de m\'ultiples etapas.
Por ejemplo, en la arquitectura del Intel Pentium 4 se llegaron a utilizar $20$ etapas distintas de pipelining.
Cada etapa corresponde a una actividad distinta en el proceso de ejecutar una instrucci\'on.
Al tiempo que una instrucci\'on es decodificada, por ejemplo, otra instrucci\'on puede estar siendo le\'ida de memoria ya que, idealmente, las etapas previas no dependen de las posteriores.
Este mecanismo funciona bien si una instrucci\'on no depende de los resultados de otra anterior a ella.
Sin embargo, ocurre habitualmente que existe una dependencia entre instrucciones, produci\'endose entonces un \textit{pipe stall} que requiere ejecutar las instrucciones de manera no solapada (con el costo de \textit{throughput} de instrucciones que ello implica).

Esta t\'ecnica llevada a su conclusi\'on l\'ogica se conoce como ejecuci\'on fuera de orden (\textit{Out of Order Execution}).
Mediante el uso de algoritmos y circuitos complejos, un procesador puede detectar las dependencias entre las instrucciones y cambiar el orden de ejecuci\'on para minimizar la aparici\'on de pipe stalls y, de esta manera, lograr que la mayor parte de las unidades del procesador esten ocupadas el mayor tiempo posible.

Mejoras en este nivel eran usualmente invisibles al programador, dependiendo de los compiladores optimizantes, pero sus ventajas fueron disminuyendo a partir del a\~no 2000.

\subsection{Extensiones vectoriales}

Si bien las t\'ecnicas SIMD fueron desarrolladas por las supercomputadoras de los a\~nos 70 y 80, su aparici\'on en los microprocesadores x86 modernos ocurre en 1996 con el nombre MMX (\textit{MultiMedia eXtensions}), con refuerzos luego en las extensiones SSE y AVX.
AVX y AVX2 representan la \'ultima versi\'on de las instrucciones de vectorizaci\'on y est\'an presentes en la l\'inea Intel Xeon de procesadores de alta gama y en las m\'as recientes generaciones de procesadores disponible para los consumidores.

El paralelismo de datos puede ser explotado por el compilador, que analiza los ciclos de programa y detecta cuando hay operaciones independientes que pueden ser realizadas en simult\'aneo, dividiendo la cantidad de instrucciones totales que tiene que realizar un procesador.

El uso de operaciones sobre m\'ultiples valores ha tomado importancia como uno de los m\'etodos de incrementar la performance de ejecuci\'on.
La longitud de registros SIMD de las extensiones (128 bits para SSE, 256 para AVX) se ha duplicado cada 4 a\~nos, con lo cual es importante para una aplicaci\'on que sus operaciones sean lo m\'as vectorizables posible~\cite{HennessyPatterson}.
Para esto es ideal que las operaciones sean lo m\'as regulares y los ciclos sean claros y con las m\'inimas dependencias posibles, de modo de hacer mejor uso de estas facilidades.

\subsection{Caches}

A diferencia de los procesadores, la velocidad de acceso de las memorias principales no aument\'o de una manera tan significativa, como se puede ver en la figura~\ref{fig:cpu_vs_mem}. Como consecuencia, la memoria empez\'o a convertirse en un serio cuello de botella a la velocidad de ejecuci\'on de los programas (llamado en la jerga \textit{memory wall}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{images/cpu_vs_memory.png}
    \caption{Comparaci\'on entre la \textit{performance} de CPU y memoria seg\'un el a\~no, la diferencia de entre ellos se denomina \textit{memory gap}.}
    \label{fig:cpu_vs_mem}

\end{figure}
\todo{Poner de d\'onde se tom\'o la figura y arreglar los bordes que aparecen cosas feas}

El concepto de \textit{localidad espacial} corresponde con la la observaci\'on de que los datos con los que opera una secci\'on de un programa suelen estar \emph{cerca} en memoria.
Los dise\~nos de procesadores empezaron a incluir distintos tipos de caches para sacar provecho de esta situaci\'on: memorias r\'apidas, pr\'oximas al CPU y de menor tama\~no para contener el subconjunto de los datos recientemente usados.
Su eficacia impuls\'o el establecimiento de una jerarqu\'ia en orden creciente de tama\~no y decreciente en velocidad, empezando por las caches L1 y siguiendo por las L2 y L3.

El tama\~no de una cache L1 moderna est\'a en el orden de los 64 KB, una cache L2 en el \'orden de los 2 MB y una L3 en el orden de 6 MB en adelante.

Si bien la aparici\'on y utilizaci\'on de caches es transparente al programador en los procesadores de la l\'inea x86\_64, los accesos irregulares a la memoria pueden producir que la cache se cargue con datos que no volver\'an a ser utilizados, esto puede causar que datos que s\'i se vayan a utilizar sean desalojados de la cache y deban ser buscados nuevamente en la memoria principal (evento que se conoce como \textit{cache miss}).
Por esto es que la regularidad de los accesos a memoria para hacer buen uso de caches es fundamental para obtener una buena performance.

\subsection{Multiprocesadores}

Los procesadores MIMD (\textit{Multiple Instruction Multiple Data}) implicaron una revoluci\'on en la computaci\'on, pero cada procesador contin\'ua las l\'ineas anteriores.\todo{Supongo que est\'an haciendo referencia a ser compatible para atr\'as. Esto es cierto para las PC's, pero hay muchos otros procesadores que no cumplen con eso, solo piensen en los micros de celulares}
Los dise\~nos m\'as utilizados se basan en un arquitectura tipo SMP (\textit{Symmetric Multiprocessing}), en la cual todos los procesadores son iguales y comparten una misma memoria principal.
Cada procesador tiene sus propios registros y se comunica con los dem\'as mediante memoria compartida o interrupciones.

Por ejemplo, el procesador Intel Xeon E7-8800 posee $12$ procesadores (cores) que pueden ejecutar dos hilos simult\'aneamente cada uno, mediante el uso de la tecnolog\'ia denominada \textit{Hyper-threading}.

A diferencia de los otros m\'etodos, las mejoras posibles mediante el procesamiento paralelo en tareas son sustanciales, pero dependen del programador en gran medida.
Un programa serial no se beneficiar\'a de m\'ultiples \textit{cores}, incluso siendo recompilado, a menos que este paralelismo se aproveche expl\'icitamente.
Otro aspecto importante es la \textit{escalabilidad}, que consiste en que la divisi\'on de tareas mantenga a todos los procesadores disponibles ocupados, aunque crezca la cantidad de cores que intervengan.

Un resultado importante a tener en cuenta es la denominada \textit{Ley de Amdahl}~\cite{Amdahl1967}, que establece una relaci\'on entre el \textit{speedup} m\'aximo alcanzable mediante un incremento en la cantidad de procesadores disponibles, el porcentaje
de la aplicaci\'on que es paralelizable y el porcentaje que no lo es. Matem\'aticamente,

\begin{equation}
    \label{eq:amdahl}
    S(n) = \frac{1}{(1 - B) + \frac{B}{n}}
\end{equation}

Donde $S$ es el porcentaje m\'aximo de mejora alcanzable, $B$ es la fracci\'on del algoritmo a ejecutar que se encuentra
paralelizada, y $n$ la cantidad de hilos de ejecuci\'on paralelos que se dispone.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{plots/cpu/amdahl.png}
    \caption{Aceleraci\'on te\'orica m\'axima (en veces) dada por la ley de Amdahl, seg\'un la cantidad de n\'ucleos de procesamiento.}
    \label{fig:amdahl_plot}
\end{figure}

Un ejemplo de esta ley en acci\'on es que si $95 \%$ del problema es paralelizable entonces el l\'imite te\'orico de
mejora es de 20 veces (el programa corriendo sobre infinitos cores se ejecutar\'ia en un veinteavo del tiempo que originalmente
requer\'ia). La figura ~\ref{fig:amdahl_plot} muestra el comportamiento de la ecuacion \ref{eq:amdahl} que define la ley de Amdahl
con distintas fracciones de c\'odigo paralelo.

La ley de Amdahl describe el pico te\'orico de mejora y es una simplificaci\'on, ya que asume que todos los cores tienen
trabajo perfectamente distribuido sin comunicaci\'on entre ellos por motivos de sincronizaci\'on.

Por otro lado, la presencia de un componente com\'un (la memoria) puede introducir cuellos de botella en el acceso a los
datos, ya que si el \textit{bus} de memoria es saturado con pedidos los procesadores deben detener su ejecuci\'on hasta que
los datos est\'en listos, elimin\'andose entonces el procesamiento paralelo.

Otro elemento de conflicto son los caches. Como los procesadores deben tener una visi\'on unificada y consistente de la
memoria, a veces es necesario que estos sincronicen los valores de sus caches, especialmente ante una escritura de memoria.
Esto se conoce como \textit{coherencia de caches} e involucra una sincronizaci\'on de alto \textit{overhead}, ya
que implica coordinaci\'on entre dos o m\'as procesadores a trav\'es de un bus de memoria.

Es en este punto que el impacto del paralelismo en el comportamiento del programa puede ser tan fuerte como sutil. Un fen\'omeno que
ilustra esto es el de \textit{false sharing}. Este fen\'omeno sucede cuando una variable no compartida entre threads
reside en la misma l\'inea de cache con una que si, requiriendo entonces que sea pasada de lado a lado entre cores aunque
nunca fuese esto necesario, decrementando la escalabilidad del algoritmo a muchos procesadores y siendo
dif\'icil de descubrir al depender intr\'insecamente del sistema en el que se corre.

Los desaf\'ios generados por la adici\'on de cores influencia fuertemente los dise\~nos de una arquitectura, no solo en
procesadores est\'andar sino tambi\'en en aceleradores como las GPGPUs y coprocesadores num\'ericos.

\section{CUDA}

\subsection{Introducci\'on}

Una de las arquitectura analizadas en este trabajo es la arquitectura GPGPU desarrollada por \nvidia, conocida
como CUDA por las siglas en ingles de \textit{Compute Unified Device Architecture}.
CUDA surge naturalmente de la aplicaci\'on del hardware desarrollado para problemas gr\'aficos, pero aplicados al computo cient\'ifico.

Las placas de v\'ideo aparecen en 1978 con la introducci\'on de Intel del iSBX 275, permitiendo dibujar lineas,
arcos y bitmaps y comunicada por DMA al procesador principal. En 1985, la Commodore Amiga inclu\'ia un coprocesador
gr\'afico que podr\'ia ejecutar instrucciones independientemente del CPU, un paso importante en la separaci\'on
y especializaci\'on de las tareas. En la d\'ecada del 90, m\'ultiples
avances surgieron en la aceleraci\'on 2D para dibujar las interfaces gr\'aficas de los sistemas operativos,
y para mediados de la d\'ecada, muchos fabricantes estaban incursionando en las aceleradoras 3D como
add-ons a las placas gr\'aficas tradicionales 2D. A principios de la d\'ecada del 2000, se agregaron los
\textit{shaders} a las placas, peque\~nos programas independientes que corr\'ian nativo en el GPU,
y se pod\'ian encadenar entre si, uno por pixel en la pantalla.~\cite{CG} Este paralelismo es el desarrollo fundamental
que llevaba a las GPU a poder procesar operaciones gr\'aficas ordenes de magnitud m\'as r\'apidas que el CPU.

En el 2006, \nvidia  introduce la arquitectura G80,
que es el primer GPU que deja de resolver \'unicamente problemas de gr\'aficos
para pasar a un motor gen\'erico donde cuenta con un set de instrucciones consistente para todos los
tipos de operaciones que realiza (geometria, vertex y pixel shaders).~\cite{cudaHandbook} Como subproducto de esto,
el GPU deja de tener pipelines especializados y pasa a tener procesadores sim\'etricos m\'as sencillos y m\'as
f\'aciles de construir. Esta arquitectura es la que se ha mantenido y mejorado en el tiempo, permitiendo
a las GPU escalar masivamente en procesadores simples, de un bajo clock de una disipaci\'on t\'ermica
manejable.

Los puntos fuertes de las GPGPU modernas consisten en poder atacar los problemas de paralelismo
de manera pseudo-explicita, y con esto poder escalar ``f\'acilmente'' si solamente se corre en una
placa mas r\'apida. ~\cite{} T\'ecnicamente, esta arquitectura cuenta con entre cientos y miles de procesadores
especializados en c\'alculo de punto flotante, procesando cada uno un \textit{thread} distinto pero
trabajando de manera sincr\'onica agrupados en bloques. Cada procesador a su vez cuenta con entre
64 a 256 registros~\cite{NvidiaFermi}~\cite{NvidiaKepler}, como porci\'on de un register file de 64KB.
Las placas cuentas con m\'ultiples niveles de cach\'e y memorias especializadas (subproducto de
su dise\~no fundamental para gr\'aficos). Estos no poseen instrucciones SIMD, ya que su dise\~no primario
esta basado en cambio, en SIMT (\textit{Single Instruction Multiple Thread}), las cuales se ejecutan en los
bloques sincr\'onicos de procesadores. De este modo, las placas modernas como la K40 alcanzan
poder de computo de 4.3 TFLOPs en c\'alculos de precisi\'on simple, 1.7 TFLOPs en precisi\'on doble y 288GB/seg de
transferencia, usando 2880 CUDA Cores.~\cite{NvidiaKeplerDatasheet} Para poner en escala la concentraci\'on
de poder de calculo, estas prestaciones har\'ian de una computadora usando solo dos de estas placas
la supercomputadora m\'as potente del mundo en Noviembre 2001.~\cite{Top500November2001}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{images/cuda-gflops.png}
    \caption{Picos te\'oricos de performance en GFLOPS/s. Tomado de~\cite{cudaProgrammingGuide}.}
    \label{cuda-gflops}
\end{figure}

Para poder correr programas explotando la arquitectura CUDA, se deben escribir de manera que
el problema se pueda particionar usando el modelo de grilla de bloques de threads. Esto implica una
reescritura completa de los c\'odigos actuales en CPU y un cambio de paradigma importante, al
dejar de tener vectorizaci\'on, paralelizaci\'on autom\'atica y otras t\'ecnicas tradicionales
de optimizaci\'on en CPU. Sin embargo, este trabajo ha rendido sus frutos en muchos casos:
en los \'ultimos 6 a\~nos, la literatura de HPC con aplicaciones en GPU ha explotado con
desarrollos nuevos basados en la aceleraci\'on de algoritmos num\'ericos (su principal uso).
% ~\cite{meter refs a gpu montecarlos}
Adem\'as, no todas las aplicaciones deben reescribirse de manera completa. Con la introducci\'on
de las librer\'ias CuBLAS y CuFFT, se han buscado reemplazar con m\'inimos cambios las hist\'oricas
librer\'ias BLAS y FFTw, piedras fundamentales del computo HPC.~\cite{cublas}~\cite{cufft}

Nuevas soluciones para la portabilidad se siguen desarrollando: las librer\'ias como Thrust ~\cite{thrust},
OpenMP4.0 ~\cite{OpenMPspec} y OpenACC 2.0 ~\cite{OpenACCSpec} son herramientas que buscan hacer el
c\'odigo agn\'ostico al acelerador de computo que usen. Estas permiten definir las operaciones de
manera gen\'erica y dejan el trabajo pesado al compilador para que subdivida el problema de la manera
que el acelerador (CPU, GPU, MIC) necesite. Obviamente, los ajustes finos siempre quedan pendiente para
el programador especializado, pero estas herramientas representan un avance fundamental al uso
masivo de t\'ecnicas de paralelizaci\'on autom\'aticas, necesarias hoy d\'ia y potencialmente
imprescindibles en el futuro.

\subsection{Organizaci\'on de procesadores}

Los procesadores GPGPU dise\~nados por \nvidia han sido reorganizados a lo largo de su
existencia m\'ultiples veces pero mayormente tienen una distribuci\'on similar. Vamos a describir a continuaci\'on
la organizaci\'on definida en la arquitectura Fermi y luego analizaremos las diferencias con Kepler.

La arquitectura GPGPU se centran en el uso de una cantidad escable de procesadores
multithreaded denominados \textit{Streaming Multiprocessors (SMs)}. Un multiprocesador esta dise\~nado
para ejecutar cientos de threads concurrentemente, usando sus unidades aritm\'eticas llamadas \textit{Streaming
Processors (SPs)}. Las instrucciones se encadenan para aprovechar el paralelismo
a nivel instrucci\'on (ILP) dentro de un mismo threads, y funcionando en conjunto con el paralelismo
a nivel de thread, usado de manera extensa a trav\'es del multithreading por hardware. Todas las instrucciones
son ejecutadas en orden y no hay predicci\'on de saltos ni ejecuci\'on especulativa, todo se ejecuta solamente
cuando se lo necesita. ~\cite{CudaOverview}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{images/fermi-sm.pdf}
    \caption{Diagrama de bloques del SM de GF100 Fermi. Tomado de~\cite{NvidiaFermi}.}
    \label{fermi_sm}
\end{figure}

Los SM son unidades completas de ejecuci\'on. Estos tienen 32 SPs interconectados entre si, que operan sobre
un register file com\'un a todos. Los SM cuentan adicionalmente con m\'ultiples unidades de Load/Store,
que permiten realizar accesos a memoria independientes. Existen 4 unidades de SFU (\textit{Special Function
Unit}) por SM, para realizar r\'apidamente operaciones matem\'aticas trascendentales (trigonom\'etricas, potencias,
ra\'ices, etc.). Cada SM ejecuta simult\'aneamente una cantidad fija de threads, llamado \textit{warp}
con cada uno de estos corriendo en un SP. Las unidades de despacho de warps se encargan de mantener registro de que
threads est\'an disponibles para correr en un momento dado y permiten realizar cambios de contexto por hardware
eficientemente ($<25 \mu$s)~\cite{PattersonFermi}. Con esto, se pueden ejecutar
concurrentemente dos warps distintos para esconder la latencia de las operaciones. En precisi\'on doble,
esto no se puede, as\'i que hay solamente un warp corriendo a la vez.

Un SM cuenta con una memoria com\'un de 64KB que se puede usar de forma autom\'atica tanto como memoria
compartida com\'un a todos los threads como cach\'e L1 para todos los accesos a memoria.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{images/fermi-gpu-block.png}
    \caption{Diagrama de bloques de GF100 Fermi. Tomado de~\cite{NvidiaFermi}.}
    \label{fermi_gpu_block}
\end{figure}


Por motivos de como funciona un pipeline gr\'afico, los SM se agrupan de 4, en un GPC (\textit{Graphics
Processing Cluster}). Cada GPC se encarga de ejecutar los distintos pasos del pipeline gr\'afico,
cargando a los SM con las tareas que tienen que realizar para rasterizar (es decir, convertir los
gr\'aficos vectorialmente definidos en gr\'aficos renderizados). En aplicaciones de CUDA, estos pasos
no importan, puesto que se trata cada SM independientemente por software.

Todos los accesos a memoria global (la memoria off-chip del dispositivo) se realizan a trav\'es de la memoria
L1 de cada SM, y a trav\'es de la L2 del todo el procesador. Esta L2 consiste de 6 bancos de 128KB, compartidas.
Estas cach\'e son write-through y se comunican de manera directa tanto con la DRAM propia de la placa como
con el bus PCI Express por el cual pueden comunicarse dos placas entre si, sin pasar por CPU.

En Fermi no se pueden ejecutar simult\'aneamente instrucciones de precisi\'on doble y simple, y como
las de doble requieren ambos pipelines, suelen disminuir considerablemente el aprovechamiento
de los cores. En Kepler, al usar 4 unidades de despacho de warp, puede elegir una mejor
combinaci\'on para poder ejecutar simult\'aneamente instrucciones que no dependan de las mismas
unidades funcionales en el pipeline. ~\cite{NvidiaKepler}

Como estos procesadores implementan el est\'andar IEEE754-2008, cuentan con precisi\'on simple y
doble correcta, por lo cual dentro de las operaciones nativas, cuentan con instrucciones FMA
(Fused multiply-add) que no pierden precisi\'on en etapas intermedias de redondeo.



%El c\'odigo escrito para CUDA se puede compilar a dos targets distintos; uno es
%el codigo binario nativo de la placa target donde se va a correr y el otro es un
%codigo intermedio, llamado c\'odigo PTX, que se JIT compila por el driver CUDA
%antes de enviar a la placa, de modo que sea portable entre placas y arquitecturas
%(retrocompatibles).

%Cuando se lanza un kernel de ejecuci\'on, este c\'odigo binario se carga en la DRAM propia
%de la placa. Este codigo va a ser ejecutado por todos los bloques que se hayan definido cuando
%se lanz\'o el kernel. El ``Thread Engine'' del GPGPU se encarga de repartir los bloques a los
%SM (Streaming Multiprocessors). Cada SM luego va a ejecutar de a grupos de 32 threads, cada uno
%de ellos corriendo sobre un SP (Streaming Processor). A traves de sus 2 o 4 unidades de dispatch de warp
%(de acuerdo a la generaci\'on de los chips), el SM puede dinamicamente cambiar el warp que se esta ejecutando en sus SP,
%escondiendo la latencia de los stalls de pipelines forzosos para la ejecucion de instrucciones de multiples clocks.
%Estos threads a su vez obtienen sus registros de un register file com\'un a todos los SP. Un scoreboard
%es mantenido por cada dispatcher de warps para poder determinar que threads estan listos para correr. En Kepler
%este scoreboard es simplificado ya que las latencias de las operaciones matem\'aticas es conocido, por
%lo cual se puede reemplazar por contadores m\'as sencillos. ~\cite{NvidiaKepler}
%Como el c\'odigo se ejecuta de manera sincr\'onica entre todos los threads del warp, las instrucciones
%condicionales proveen un problema para esta arquitectura. Como las distintas ramas del condicional
%son instrucciones excluyentes, no las deben ejecutar todos los threads simultaneamentes. El procesador
%deshabilitaba los cores que manejaban los threads de las ramas que no se ejecutaban del
%condicional~\cite{NvidiaTesla}. Para disminuir la cantidad de instrucciones de condicionales
%que se ejecutan, se cuenta con ~\cite{NvidiaFermi} predicaci\'on en todas las instrucciones de la ISA.


\subsection{Organizaci\'on de la memoria}

%La arquitectura CUDA esta enfocada a procesamiento de grandes cantidades de datos
%de puntos flotante. El procesador GPGPU cuenta con cientos de ALU sincronizadas
%por bloques, permitiendo un paralelismo adaptativo a distintos problemas.
%
%El procesamiento GPGPU es similar al procesamiento vectorial
%realizado por las supercomputadoras Cray y IBM que surgio en los 1960's, pero
%en vez de usar VLIW para procesamiento masivo, CUDA usa multiples hilos de ejecuci\'on
%que trabajan simultaneamente sobre los datos.
%El procesamiento consiste en un funcionamiento hibrido entre compilador y procesador. Se determina
%un conjunto de elementos a procesar y se elije de manera explicita una manera de particionar el
%problema a la hora de ser enviado para procesado a la placa.
%
%Para realizar el computo, esta arquitectura cuenta a su vez con multiples clases de memorias
%que se adaptan de maneras diferentes a los distintos procesos. Estas incluyen:
La memoria de la GPGPU es uno de los puntos cruciales de esta arquitectura. Esta se subdivide
entre memorias on-chip y memorias on-board, de acuerdo a su ubicaci\'on y tiempos de acceso.

Las memorias se subdividen en cuatro categor\'ias distintas:

\begin{itemize}
  \item Registros
  \item Memoria global
  \item Memoria local
  \item Memoria compartida
\end{itemize}

Los registros son la unidad b\'asica de almacenamiento de los threads de ejecuci\'on.
Cada thread de ejecuci\'on cuenta con una cantidad limitada de registros de punto flotante de
32 bits con latencia de un par de ciclos de clock. A su vez, existen una cantidad finita de
registros totales que cuenta un chip (oscilan entre 16.535 y 65.535 registros).

La memoria global es la memoria principal fuera del chip del GPGPU. Esta es de gran tama\~no (de
entre 2Gb y 12Gb) y es compartida por todos los SM de la GPGPU y los CPU que integran el
sistema. Es decir, tanto los GPGPU y los CPU pueden invocar las funciones del runtime CUDA
para poder transferir datos entre la placa y la memoria RAM. La latencia de acceso a la memoria global
es de cientos de ciclos~\cite{Demystifying}, sumamente lenta en comparaci\'on con el procesador.
La memoria global tambi\'en puede ser mapeada, o \textit{pinneada}. Esto quiere decir que va a existir
una copia de esa reserva de memoria en la placa en la memoria principal del procesador. El driver
de CUDA va a mantener la consistencia entre ambas de manera as\'incrona, evitando la necesidad de hacer
copias de memoria explicitas. No es ilimitada la cantidad de memoria mapeada posible, por lo que
es importante saber elegir que elementos van a ir en esta.

La memoria local es una memoria propia de cada thread, y se encuentra almacenada dentro de la
memoria global. Esta memoria es definida autom\'aticamente por el compilador y sirve como \'area de \textit{spilling}
de registros cuando se acaban. Cuenta con las mismas desventajas que la memoria global, incluyendo
su tiempo de acceso.

La memoria compartida, o \textit{shared}, es una memoria que es visible para todos los threads dentro
de un mismo SM. Cada thread puede escribir en cualquier parte de la memoria compartida dentro de su bloque y
puede ser le\'ido por cualquier otro thread de este. Es una memoria muy r\'apida, on-chip, y
que tarda aproximadamente 40 ciclos de acceso.~\cite{Demystifying} Esta memoria es compartida con la cach\'e
L1, de capacidad de entre 16KB y 64KB configurable por software. Esta memoria se encuentra dividida
en 32 bancos de 4 bytes de tama\~no, permitiendo que cada uno de los 32 threads acceda independientemente
a un float. Si hubiera conflicto, los accesos a ese banco se serializar\'ian, aumentando la latencia
de la llamada.~\cite{farberCuda}

Adicionalmente, la GPGPU cuenta con m\'ultiples niveles de memorias cach\'e para poder aminorar el hecho
de que el principal cuello de botella del computo es la latencia en los accesos a memoria global.
Estas se dividen en tres:

\begin{itemize}
  \item Cach\'e L1
  \item Cach\'e L2
  \item Cach\'e constante
  \item Cach\'e de textura
\end{itemize}

La cach\'e L1 es dedicada por SM. Esta cach\'e fue introducida en Fermi y su dise\~no hace que
tambi\'en esta dedicada a la memoria compartida, por lo que es posible en tiempo de ejecuci\'on
darle directivas a la GPGPU que asigne m\'as memoria cach\'e o m\'as memoria compartida,
permitiendo a los bloques tener mayores espacios de memorias compartidas o mayores hit rate de caches.

La cach\'e L2 es com\'un a todos los SM de la GPGPU, donde, a partir de Fermi en \nvidia, todos
los accesos de lectura y escritura a memoria global y textura pasan a trav\'es de esta. ~\cite{NvidiaFermi}

La cach\'e constante es una cach\'e sobre la memoria global dedicada solamente a lecturas de memoria. Esta
es muy reducida (solo cuenta con 64KB) y esta optimizada para muchos accesos a la misma direcci\'on. Cuando
un thread lee esta memoria, se retransmite a los dem\'as threads del warp que est\'en leyendo esa misma direcci\'on, reduciendo
el ancho de banda necesario. Si, en cambio, los threads leen distintas direcciones los accesos se serializan.
Cuando hay un miss de esta memoria, la lectura tiene el costo de una lectura de memoria global.

La cach\'e de textura es una cach\'e sobre la memoria global que presenta no solo localidad espacial,
como la mayor\'ia de las caches de procesadores normales (es decir, dato-1, dato, dato+1, etc.), sino que se le
puede agregar el concepto de dimensiones, para poder modelar datos en mas de una dimensi\'on.
Esto se adapta de muy bien a los problemas de gr\'aficos en el 2D y 3D, por lo tanto se convierte
en una herramienta clave a la hora de minimizar los accesos a matrices
no solo por filas sino por columnas. Esta cach\'e se debe definir en momento de compilaci\'on en
en el c\'odigo, ya que tiene limites espaciales (necesarios para poder definir \'areas de memoria
sobre la cual operar) y a su vez se debe acceder a los datos subyacentes a trav\'es de funciones especificas. Una caracter\'istica adicional
de esta cach\'e es que como necesita resolver estos accesos no convencionales a la memoria, cuenta
con una unidad propia de resoluci\'on de direcciones. Esta unidad tiene limitantes a cuanto podemos
exigirle, ya que no posee un ancho de banda suficiente como para resolver todos los
accesos a memoria globales podr\'ia servir, as\'i que hay que usarla juiciosamente.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=250px]{images/cuda-memories.png}
    \caption{Jerarqu\'ia de memorias en GPU. Tomado de~\cite{farberCuda}.}
    \label{fig:cuda-memories}
\end{figure}

\subsection{Esquema de paralelismo}
Al ser una arquitectura masivamente paralela desde su concepci\'on, CUDA presenta varios niveles de paralelismo, para
agrupar l\'ogicamente el computo y poder dividir f\'isicamente su distribuci\'on. Los principales son:
\begin{itemize}
  \item Bloques de threads
  \item Grilla de bloques
  \item Streams
  \item M\'ultiples placas
\end{itemize}

El paralelismo a nivel de bloque instancia una cantidad de threads, subdivididos l\'ogicamente en 1D, 2D o 3D.
Los threads internamente se agrupan de a 32 threads (un \textit{warp}).
Cada uno de estos threads va a contar su propio threadId y con el mismo blockId, identific\'andolos un\'ivocamente.
Estos threads van a correr simult\'aneamente en el mismo SM y van a ser puestos y sacados de ejecuci\'on de a un warp
din\'amicamente por el scheduler de hardware que cuenta cada SM. Para compartir informaci\'on entre
ellos, se puede utilizar la memoria shared o las instrucciones de comunicaci\'on de threads intrawarp.

El paralelismo a nivel de grilla determina una matriz de bloques de ejecuci\'on que partici\'on
el dominio del problema. El GigaThread Scheduler va a ejecutar cada bloque en un SM hasta
el final de la ejecuci\'on de todos los threads de este. Los bloques entre si no comparten informaci\'on
sino a trav\'es de la memoria global. Como no se garantiza ning\'un orden de ejecuci\'on no se puede
confiar en sincronizar entre si, ya que si hay m\'as bloques que SM y no hay pre-emption en los SM,
puede haber deadlock.

El paralelismo de stream es una herramienta empleada para hacer trabajos concurrentes usando una
sola placa. Esta t\'ecnica permite que m\'ultiples kerneles o copias de memoria independientes est\'en
encolados, para que el driver pueda schedulearlas simult\'aneamente si se est\'an subutilizando
los recursos de forma de minimizar tiempo ocioso del dispositivo. Los streams permiten kernels
concurrentes pero cuentan con importantes restricciones que generan sincronizaci\'on impl\'icita,
lo cual hay que tener presente si se desea mantener el trabajo de forma paralela.

El paralelismo a nivel de placa consiste en poder distribuir la carga del problema entre distintas
GPGPU dispuestas en un mismo sistema compartiendo una memoria RAM com\'un como si fuera un software
multithreaded tradicional. CUDA no cuenta con un modelo impl\'icito de paralelismo entre distintas placas,
pero es posible hacerlo manualmente eligiendo de manera explicita que dispositivo usar. Las placas
se pueden comunicar as\'incronamente entre si, tanto accediendo a las memorias globales de cada una
como ejecutando c\'odigo remotamente. En las versiones modernas del driver de CUDA, tambi\'en pueden
directamente comunicarse las placas entre si a trav\'es de la red, permitiendo escalar multinodo f\'acilmente.

Los algoritmos de GPGPU deben considerar todos estos niveles de paralelismo para poder maximizar la
performance, empleando as\'i todos los recursos con los que cuentan estas arquitecturas.

\subsection{Diferencias entre Tesla, Fermi, Kepler}

Hasta ahora enunciamos la arquitectura vista desde el punto de vista Fermi, la segunda
arquitectura GPGPU dise\~nada por \nvidia . Fermi es la evoluci\'on de Tesla,
construida para desacoplar a\'un mas los conceptos de gr\'aficos de modo de hacerla
un procesador m\'as escalable y de prop\'osito general.


\begin{table}[h]
  \begin{tabular}{@{}llll@{}}
  \toprule
  Caracter\'isticas        & Tesla (GT200)   & Fermi (GF100)   & Kepler (GK110)   \\ \midrule
  A\~no introducci\'on     & 2006            & 2010            & 2012             \\
  Transistores             & 1400 millones   & 3000 millones   & 3500 millones    \\
  Tecnolog\'ia fabricaci\'on & 65 nm           & 40 nm           & 28 nm            \\
  SMs                      & 30              & 16              & 15               \\
  SP / SM                  & 8               & 32              & 192              \\
  Cach\'e L1               & -               & 16 - 48KB       & 16 - 32 - 48KB   \\
  Cach\'e L2               & -               & 768KB           & 1536KB           \\
  Memoria Shared/SM      & 16KB            & 16 - 48KB       & 16 - 32 - 48KB   \\
  Registros/Thread         & 63              & 63              & 255              \\
  Pico Precisi\'on Simple    & 240 MAD / clock & 512 FMA / clock & 2880 FMA / clock \\
  Pico GFLOPS Simple       & 933             & 1345            & 3977             \\
  GFLOPS/Watt              & 3.95            & 5.38            & 15.9             \\ \bottomrule
  \end{tabular}
\caption{Tabla comparativa de las caracter\'isticas mas prominentes de las tres arquitecturas de CUDA.}
\label{tab:CudaGenerations}
\end{table}

En la tabla \ref{tab:CudaGenerations} vemos una comparaci\'on de las recursos que est\'an m\'as directamente
relacionadas a la performance de un dispositivo GPGPU. Se puede apreciar como muchos recursos
crecieron dr\'asticamente gracias a las tecnolog\'ias de fabricaci\'on, que permitieron aumentar la
cantidad de transistores. Tambi\'en podemos comprobar que, a diferencia de los CPU, las arquitecturas GPGPU
decidieron utilizar esos nuevos transistores disponibles para mas n\'ucleos de procesamiento, en vez
de dedicarlas a aumentar las memorias cach\'e, que crecieron m\'inimamente (comparando contra las caches de CPU).

Una de las diferencias mas notables entre Tesla y Fermi es la presencia de FMA (\textit{Fused Multiply-Add})
contra el MAD (\textit{Multiply - Add}). El MAD realiza la multiplicaci\'on y la acumulaci\'on
en dos pasos (pero m\'as r\'apidos que hacerlos independientemente por tener hardware dedicado en los SP)
debe redondear entre los pasos, perdiendo precisi\'on y haciendo que respete plenamente el IEEE754-2008. El FMA,
en cambio, lo hace en una sola operaci\'on, y sin redondeos intermedios.

La m\'etrica usada por \nvidia para publicitar la performance de estos dispositivos y poder
compararlos entre si, y contra CPU, son los GFLOPS. Esta unidad mide cuantas operaciones de punto
flotante de precisi\'on simple se pueden realizar por segundo. Esta unidad es utilizada tambi\'en
por los clusters en el ranking TOP500, donde se ordenan de acuerdo al pico de performance medido. No solo
es notable como se cuadruplico la performance (te\'orica) en solamente 6 a\~nos (algo inalcanzable para
los CPU modernos) sino que a\'un mas notorio es como mejoro la performance por Watt. Esto
tambi\'en se ve en que Kepler tiene menos SM que Fermi o Tesla, pero son mucho mas poderosos y
eficientes. La tecnolog\'ia de fabricaci\'on ha ayudado a la disminuci\'on del consumo, un
problema que acechaba a los dise\~nos Fermi, ya que sus consumos superiores a 200W por
dispositivo los hac\'ian muy dif\'iciles de refrigerar incluso en clusters de HPC. Se puede
notar la estrategia de mercado de \nvidia de introducirse en las supercomputadoras de todo
el mundo, donde el consumo y la refrigeraci\'on son factores limitantes (mucho m\'as aun que,
por ejemplo, computadoras de escritorio).

\subsection{CUDA, Herramientas de desarrollo, profiling, exploraci\'on}

Para soportar una arquitectura masivamente paralela, se debe usar una ISA
(\textit{Instruction Set Architecture}) dise\~nada especialmente para el problema. Esta ISA, conocida como PTX,
debe poder soportar conceptos fundamentales del computo GPGPU: grandes cantidades de registros,
operaciones en punto flotante de precisi\'on simple y doble y fused multiply-add. Adem\'as,
el c\'odigo compilado para GPGPU debe ser agn\'ostico al dispositivo que lo va a correr, por
lo cual la paralelizaci\'on no debe estar demasiado atada a este, sino que el dispatching
lo debe poder determinar el driver de la placa en tiempo de ejecuci\'on.  Un \'ultimo
requerimiento clave de esta ISA es que debe poder soportar poder realizar ajustes manuales,
para poder construir partes claves de ciertas librer\'ias frecuentemente usadas (como las BLAS)~\cite{NvidiaFermi}.

El lenguaje que desarrollo \nvidia para poder programar estos dispositivos a un nivel m\'as alto
que el assembler especifico del procesador es CUDA. Este lenguaje es una extensi\'on de C++, con ciertas
features agregadas para poder expresar la subdivisi\'on de los kernels en threads y bloques, junto
con poder especificar que variables y funciones van a ejecutarse en el GPGPU y en el CPU. Una caracter\'istica
de CUDA es que todas las llamadas a los kernels de ejecuci\'on son asincr\'onicas, por lo que es relativamente
sencillo solapar c\'odigo en GPU y CPU. A su vez se cuenta con m\'ultiples funciones opcionales, con distinta
granularidad, que permiten esperar a que todas las llamadas as\'incronas a GPU finalicen, agregando determinismo
en forma de barreras al lenguaje.

El c\'odigo CUDA compila usando \texttt{nvcc}, una variante del \texttt{GNU gcc} que se
encarga de generar el c\'odigo PTX para las funciones marcadas como que se van a ejecutar
en los GPGPU. Este c\'odigo objeto despu\'es se linkea normalmente con el resto del c\'odigo que corre en CPU
y se genera un binario ejecutable.

\nvidia, ademas, provee herramientas de profiling para explorar como se est\'an utilizando los
recursos durante la ejecuci\'on. Estas son esenciales para optimizar, puesto que los limitantes
de GPU son sumamente distintos a los de CPU, presentando dificultades incluso para programadores
experimentados. Las herramientas de profiling no solo muestran runtime, sino que sirven para
ver donde hay accesos a memoria excesivos, puntos de sincronizaci\'on costosos, limitantes
en los registros y como se superponen las llamadas asincr\'onicas.

El uso de todas estas herramientas fue vital para poder entender como funciona la arquitectura por dentro,
como medir performance y utilizaci\'on y como nuestros cambios impactan en las distintas generaciones
de dispositivos.

\subsection{Requerimientos de un problema para GPGPU}
Dada la organizaci\'on de un procesador GPGPU, es posible ver como esta apuntado a ejecutar
algoritmos de dominios muy espec\'ificos. Un problema debe exhibir al menos las siguientes
caracter\'isticas para que valga la pena pensar en correrlo para GPGPU.
\begin{enumerate}
  \item \label{req:paralelo} El problema debe tener una gran parte paralelizable.
  \item \label{req:float} El problema debe consistir mayormente de operaciones num\'ericas de punto flotante.
  \item \label{req:matrix} El problema debe poder ser modelado mayormente en arreglos o matrices.
  \item \label{req:transf} El tiempo de computo debe ser muy superior al tiempo de transferencia de datos.
\end{enumerate}

Item \ref{req:paralelo} se refiere a que debe existir alguna forma de partir el problema
en subproblemas que puedan realizarse simult\'aneamente, sin que haya dependencias de
resultados entre si. Si el problema requiere partes seriales, lo ideal es que se las
pueda concebir las partes paralelas sean etapas de un pipeline de procesos, donde
cada una de estas exhiban caracter\'isticas fuertemente paralelas. Como las arquitecturas
masivamente paralelas tienen como desventaja una menor eficiencia por core, si el
problema no se puede dividir para maximizar la ocupaci\'on de todos los cores disponibles,
va a ser muy dif\'icil superar en eficiencia a los procesadores seriales.

Item \ref{req:float} habla de que el m\'etodo de resoluci\'on de los problemas debe
consistir del uso de m\'etodos num\'ericos. El set de instrucciones de las arquitecturas
de GPGPU est\'an fuertemente influenciados por las aplicaciones 3D que las impulsaron
en un principio. Estas consisten mayormente de transformaciones de \'algebra lineal
para modelar luces, hacer renders o mover puntos de vistas. Todos estos problemas
son inherentemente de punto flotante, por lo cual el set de instrucciones, las ALUs
internas y los registros est\'an optimizados para este caso de uso. Las operaciones
en n\'umeros enteros no son el fuerte de esta arquitectura y suelen ser realizados
m\'as eficientemente por procesadores de prop\'osito general.

Item \ref{req:matrix} menciona que los problemas que mejor se pueden tratar en esta
arquitectura se pueden representar como operaciones entre arreglos o matrices de
dos, tres o cuatro dimensiones. Las estructuras de datos que no est\'an secuenciales
en memoria pueden incurrir en m\'ultiples accesos a esta para recorrerlas, y en las
arquitecturas GPGPU son estos los que generan el mayor cuello de botella. Ademas,
suelen ser dif\'iciles de paralelizar en m\'ultiples subproblemas. Tener como par\'ametros de
entrada matrices o arreglos que se puedan partir f\'acilmente incurre en m\'inimos
overheads de computo y permite aprovechar mejor las memorias caches y las herramientas de
prefetching que brinda la arquitectura.

Item \ref{req:transf} ataca uno de los puntos cr\'iticos de esta arquitectura. Para poder
operar con datos, se requiere que estos est\'en en la memoria de la placa, no la memoria
de prop\'osito general de la computadora. Esto quiere decir, que se deben hacer copias
explicitas entre las memorias, ya que ambas tienen espacios de direcciones independentistas.
Esta copia se realiza a trav\'es de buses que, a pesar de tener un enorme throughput de
datos, tambi\'en tienen una gran latencia (orden de milisegundos). Por lo tanto, para minimizar
el tiempo de ejecuci\'on de un programa usando las GPGPU, se debe considerar tambi\'en el
tiempo de transferencia de datos a la hora de determinar si el beneficio de computar en
menor tiempo lo justifica. Las nuevas versiones de CUDA buscan brindar nuevas herramientas
para simplificar este requerimiento, proveyendo espacio de direccionamiento \'unico y
memoria unificada ~\cite{farberCuda}, pero siguen siendo copias de memoria a trav\'es de los
buses (aunque asincr\'onicas).

Estas caracter\'isticas limitan enormemente la clase de problemas que una GPGPU puede
afrontar, y suelen ser una buena heur\'istica para determinar de antemano si vale la pena
invertir el tiempo necesario de la implementaci\'on y ajuste fino.


\subsection{Diferencia entre CPU y GPU - Procesadores especulativos}
Hasta ahora, solo se consideraron a los GPGPU de forma aislada, viendo las prestaciones del hardware y un m\'inimo
de como se escriben los programas para esta arquitectura. La esencia de GPGPU se puede apreciar mejor
compar\'andola contra los motivos de la evoluci\'on de CPU, y los problemas que se fueron enfrentando
los dise\~nos siguiendo la historia de los componentes que fueron apareciendo en estos. Esto se vio
en la tabla \ref{tbl:historia-cpu}, que detalla algunos de los eventos m\'as importantes que aceleraron
la performance de los CPU.

Lo importante ac\'a es observar el patr\'on: ``no desechar algo que pudi\'eramos necesitar pronto'',
``intentar predecir el futuro de los condicionales'', ``intentar correr m\'ultiples instrucciones a la vez
porque puede llegar a bloquear en alguna de ellas.''

Todos estos problemas han convertido al CPU en un dispositivo que gira alrededor de la especulaci\'on,
de los valores futuros que pueden tener las ejecuciones, del probable reuso de datos.
En un CPU moderno (por ej. Intel Xeon E7-8800~\cite{XeonE78800Spec}) las unidades que verdaderamente realizan las operaciones l\'ogico-aritm\'eticas,
las ALU, son muy pocas en comparaci\'on con la cantidad de dispositivos de soporte que est\'an
sobre el silicio del CPU.

En contraste, los dispositivos GPGPU son verdaderos procesadores de computo masivo. Est\'an dise\~nadas para
resolver constantemente operaciones muy bien definidas (instrucciones de punto flotante en su mayor\'ia).
Comparativamente con un CPU, las ALU de las GPU son bastante pobres y lentas. No funcionan a las mismas
velocidades de clock (rara vez superan 1.1Ghz) y sus SP deben estar sincronizados entre si. Pero la gran ventaja esta en la cantidad.
Un CPU cuenta con pocas ALU por core, dependiendo de la cantidad de cores y del tama\~no de sus operaciones SIMD
(alrededor de 16 cores por die de x86 es el tope de linea ofrecido actualmente, procesando de a 32 bytes simult\'aneamente).
Un GPU cuenta con miles de ALUs en total (m\'as de 2500 CUDA Cores en Tesla K20). El dise\~no de esta
arquitecturas concibe la escalabilidad cuantitativa de los unidades de computo como la caracter\'istica esencial a tener,
tanto por su \'enfasis fundamental, las aplicaciones gr\'aficas, como para su aspecto de coprocesador num\'erico
de prop\'osito general.

Por contrapartida, los GPU disponen de pocas unidades de soporte del procesamiento. Estos no disponen pipelines
especulativos, el tama\~no de las caches est\'an a ordenes de magnitud de las de CPU, la latencia a las memorias
principales de la GPU est\'an a centenas de clocks de distancia, etc. La arquitectura asume que siempre va a tener m\'as trabajo
para hacer, por lo cual en vez de intentar solucionar los pitfalls de un grupo de threads, directamente
los reschedulea para m\'as adelante y continua procesando otro warp de threads. Se puede notar que durante del
dise\~no de la arquitectura GPGPU buscaron resolver el problema del computo masivo pensando en hacer
m\'as cuentas a la vez y recalcular datos, si fuera necesario. Esto es una marcada diferencia contra
los CPU, que est\'an pensado en rehacer el menor trabajo posible y intentar mantener todos los datos que pueda en
las memorias caches masivas.

Nuevamente, ac\'a se ve el legado hist\'orico de los CPU. Al tener que poder soportar cualquier aplicaci\'on, no pueden
avocarse de lleno a una sola problem\'atica. Para las arquitecturas GPGPU, el hecho de no tener que dise\~nar un
procesador de prop\'osito general retro compatible permiti\'o un cambio radical a la hora de concebir
una arquitectura de gran throughput auxiliar al procesador, no reemplaz\'andolo sino mas bien adicionando poder de computo.~\cite{GlaskowskyFermi}

Las arquitecturas Tesla, Fermi y Kepler han concebido el dise\~no de un procesador de alto desempe\~no.
Su meta principal es poder soportar grandes cantidades de paralelismo, mediante el uso de procesadores
sim\'etricos, pero tomando la fuerte restricci\'on de ``no siempre tiene que andar bien''. Es decir, ellos
mismos asumen que el c\'odigo que van a correr esta bien adaptado a la arquitectura y no disponen
casi de mecanismos para dar optimizaciones post-compilaci\'on. Relajar esta restricci\'on
les permiti\'o romper con el modelo de computo de CPU y definir nuevas estrategias de paralelismo,
que no siempre se adaptan bien a todos los problemas, pero para el subconjunto de los desaf\'ios que se
presentan en el \'area de HPC y de v\'ideo juegos han probado ser un cambio paradigm\'atico.

\subsection{Idoneidad para el problema}
El problema de QM/MM enfrentado en este trabajo cuenta con m\'ultiples operaciones matem\'aticas de gran
volumen de c\'alculos. En particular, las operaciones matriciales son los cuellos de botella en esta aplicaci\'on.

Para obtener los valores num\'ericos de densidad buscados en los puntos, se deben obtener las derivadas primeras
y segundas, lo cual implica hacer m\'ultiples operaciones de multiplicaci\'on matricial. Estos problemas est\'an
fuertemente estudiados como paralelizarlos en la literatura, existen muchas aplicaciones de \'algebra lineal que
los requieren constantemente.
En nuestro caso, se requieren para un sistema miles de estas multiplicaciones, algunas con matrices de mas de
$500^2$ elementos. Al consistir este proyecto un sistema de resoluci\'on num\'erica de las f\'ormulas de QM/MM,
los problemas enfrentados eran casi en su totalidad de operaciones de punto flotante. Luego, dados las
caracter\'isticas de contar con un fuerte nivel de paralelismo en los cuellos de botella y de ser operaciones
mayormente de punto flotante, determinamos que el uso de las GPGPU era id\'oneo, en comparaci\'on con arquitecturas
de prop\'osito general con menos poder de c\'omputo.


\section{XeonPhi}

\subsection{Introducci\'on}

La arquitectura Xeon Phi es la culminaci\'on de un trabajo iniciado por Intel en 2004, previendo
la necesidad de paralelismo masivo para aplicaciones futuras. Saliendo al mercado al final de 2012,
con el prop\'osito de competir en computo intensivo con \nvidia CUDA, ha ganado gran tracci\'on dentro
de HPC a pesar de ser muy reciente. Por ejemplo, ha sido fuertemente utilizada en la supercomputadora Tianhe-2
de la Universidad de Sun Yat-Sen en China, listada en Top 500 como la supercomputadora m\'as r\'apida del
mundo en Junio 2013, Noviembre 2013 y Junio 2014~\cite{Top500XeonPhiJune2013},~\cite{Top500XeonPhiNov2013},~\cite{Top500XeonPhiJune2014}.
Los 16000 nodos de esta supercomputadora contienen, adem\'as de dos Ivy Bridge Xeon, 3 coprocesadores
Xeon Phi, dando un poder total de computo te\'orico de 54.9 petaflops.

\subsection{Microarquitectura general}

En la concepci\'on del Xeon Phi se tuvieron en cuenta diversos factores, entre los cuales
se incluye el consumo energ\'etico. Uno de los objetivos fue aumentar la relaci\'on de
poder de c\'omputo por Watt de los procesadores de Xeon de la \'epoca, manteniendo un entorno
de desarrollo de prop\'osito general como el de x86.

Si bien el consumo no se ve\'ia impactado por el set de instrucciones,
si se busc\'o eliminar diversos componentes del procesador, manteniendo las caracter\'isticas
que sirvieran al tipo de aplicaci\'on a la que se estaba
apuntando (programas altamente paralelos a nivel de datos y tareas)~\cite{BookXeonPhi}.

La microarquitectura de este coprocesador se basa en muchos (m\'as de 50) procesadores sim\'etricos que comparten la memoria, lo
cual justifica su nombre MIC (\textit{Many Integrated Core}). Cada procesador esta basado
en el dise\~no del Intel Pentium, con una ISA (\textit{Instruction Set Architecture}) similar a la IA-32 con
soporte para direccionamiento a 64 bits y nuevas instrucciones de vectorizaci\'on.

Los procesadores tienen un \textit{clock rate} de 1.0 GHz aproximadamente, comparativamente lentos frente
a otros procesadores de Intel. Por ejemplo, los cores de un Intel Xeon CPU E5-2620 tienen un \textit{clock rate}
de 2.10 GHz, m\'as del doble.

Cada uno de los cores permite hasta 4 \textit{threads} simult\'aneos, con el prop\'osito de esconder la latencia de memoria y del tiempo de ejecuci\'on de
las instrucciones vectoriales. Adicionalmente, el uso de dos \textit{pipelines}, $U$ y $V$ permite que se ejecuten hasta 2
instrucciones por ciclo de clock.  Algunas instrucciones, sin embargo, solo pueden ser despachadas en uno de ambos:
por ejemplo las instrucciones de vectorizaci\'on solo pueden ser ejecutadas en la \textit{U-pipe}. Para esto se dispone
de una unidad de vectorizaci\'on (VPU, \textit{Vector Processing Unit}) con 32 registros SIMD (\textit{Single Instruction
Multiple Data}) de 512 bits por thread, con lo cual cada uno puede te\'oricamente realizar 16 operaciones de punto flotante de
32 bits al mismo tiempo. La latencia de estas instrucciones es de 4 ciclos de clock. Sin embargo gracias a su micro
arquitectura de \textit{pipeline} se puede obtener un \textit{throughput} te\'orico de 1 instrucci\'on vectorial por ciclo.

\subsection{Pipeline}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{images/xeon-phi-pipeline.png}
   \caption{Pipeline del procesador Intel Xeon Phi, tomado de~\cite{BookXeonPhi} }
   \label{fig::xeon_phi_pipeline}
\end{figure}

El \textit{pipeline} de instrucciones tiene siete etapas  para las instrucciones escalares, y las
instrucciones vectoriales ocupan otras siete. Un esquema de estas puede verse en la figura~\ref{fig::xeon_phi_pipeline}.

El pipeline tiene las fases usuales del ciclo \textit{fetch-decode-execute}) con algunas modificaciones.

El \textit{instruction fetch} esta dividido en dos fases para elegir el \textit{thread} por hardware a ejecutar:
\textit{Prethread picker function} (PPF) y \textit{Picker function} (PF). En la fase PPF se mueve la instrucci\'on
a uno de los 4 buffers de \textit{prefetch} que tiene cada procesador. Por cada thread hay adem\'as dos streams de
instrucciones. Si un stream es \textit{stalleado} por el pipeline, se puede proseguir con el otro stream.

La etapa PF selecciona el thread a ejecutar, usando el buffer de prefetch. Cada buffer tiene espacio para dos
instrucciones (porque puede despacharse una instrucci\'on por la \textit{U-pipe} y otra por la \textit{V-pipe}). PF
funciona de manera \textit{round robin} en los buffers de prefetch. Recargar este buffer (por ejemplo cuando hay un
\textit{miss} de cache de instrucciones) toma aproximadamente 4 o 5 ciclos.

Los 4 threads de un core son multiplexados por el pipeline del mismo, de manera de que si uno de los threads esta
\textit{stalleado} se elija a otro que tenga trabajo para hacer. Se sugiere entonces el uso de al menos dos
\textit{threads} por core para esconder latencias de memoria y de instrucciones vectoriales, y as\'i aumentar la performance.

Una vez que una instrucci\'on ha sido elegida para decodificarse, pasa a las etapas D0 y D1, a una velocidad de
decodificaci\'on de 2 instrucciones por ciclo de \textit{clock}. De ahi son enviadas a cada pipeline para ejecutarse.
Por \'ultimo se pasa a la etapa de \textit{writeback} (WB). No necesariamente cuando una instrucci\'on llega a esta
fase ha terminado de ejecutarse, puesto que si la operaci\'on es vectorial reci\'en termina de ejecutarse en la unidad
vectorial 5 ciclos despu\'es.

Este pipeline corto (7 etapas frente a las 20 de la arquitectura Pentium 5 en la que se basa Xeon Phi) contribuye a que
los \textit{branch mispredictions} tengan menor latencia, y las instrucciones escalares tengan poca latencia~\cite{IntelXeonPhiWhitePaper}.

\subsection{Estructura de cache}

Adem\'as de la unidad de vectorizaci\'on y la unidad escalar, cada procesador cuenta con 32 Kb de cache L1 y 512 Kb de cache
L2 unificada para datos y c\'odigo. Estas caches son \textit{set associative} \textit{8-way} con una linea de cache de 64 bytes.
La cache de datos es no bloqueante, de manera que un \textit{miss} de cache de un thread en un core,
no se produce un \textit{flush} del pipeline en los dem\'as threads.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=\plotwidth]{images/xeon-phi-ring.jpg}
   \caption{Detalle de la estructura de anillo del Xeon Phi, tomado de~\cite{XeonPhiArchArticle}}
   \label{fig::xeon_phi_arch_ring}
\end{figure}

La cache L2 mantiene su coherencia mediante el uso de un directorio distribuido de \textit{tags}. Un esquema del mismo puede
verse en la figura~\ref{fig::xeon_phi_arch_ring}. El mismo esta dividido en 64 secciones y une tanto los segmentos del
directorio de tags con los cores y los controladores de memoria principal, utilizando una topolog\'ia de anillo bidireccional,
contando con un anillo para datos, uno para direcciones y uno para confirmaciones (\textit{acknowledgements}).

Cuando ocurre un \textit{miss} de cache L2, la direcci\'on es enviada al anillo de direcciones. Si la l\'inea de cache
es de otro core, se env\'ia un request de \textit{forwarding} y los datos son enviados por el anillo de datos. Si ning\'un
core tiene esa l\'inea de cache se env\'ia un pedido a uno de los varios controladores de memoria. Cada uno de estos controladores
maneja un subconjunto del espacio de direcciones con el prop\'osito de reducir los cuellos de botella y aumentar el ancho de banda.

Cada cache L2 esta divida en dos bancos, con una latencia de 2 ciclos para leer 64 bytes (una l\'inea) y 1 ciclo para escribir 64 bytes.

%Una cosa interesante de esta arquitectura, es que la pol\'itica de cach\'e puede modificarse mediante \textit{hints} al coprocesador. Por defecto la pol\'itica del subsistema
%de cach\'e es pseudo-LRU (\textit{Least Recently Used}). Antes de escribir una l\'inea de cache el procesador debe leerla a su propia
%cache (pol\'itica conocida como RFO, \textit{Read For Ownership}). Dado que esto puede implicar que datos \'utiles de cache
%sean removidos para dejar lugar a datos que solo se van a escribir, la arquitectura da soporte a stores \textit{streameados},
%que pasan de largo a los caches. Tambi\'en se dispone de un \textit{prefetcher} por hardware.

\subsection{Arquitectura de instrucciones}

Si bien la base del conjunto de instrucciones de la arquitectura del Xeon Phi es la P54C de Pentium (IA-32), nuevas instrucciones se han incluido con el prop\'osito de aumentar la idoneidad del procesador para computo de alta performance.
Estas operaciones incluyen implementaciones por \textit{hardware} de operaciones comunes en HPC: rec\'iproco de un valor,
ra\'iz cuadrada, potencia y exponenciaci\'on, y operaciones m\'as relacionadas con la memoria como por ejemplo \textit{scatter and gatter}
y stores \textit{streameados} de manera de aprovechar mejor el ancho de banda de memoria que permite la arquitectura.

El dise\~no de las instrucciones vectoriales es ternario, con dos operandos fuentes y uno destino codificados en la instrucci\'on.
Esta configuraci\'on permite una ganancia de hasta 20\% sobre la configuraci\'on usual binaria de otras arquitecturas SIMD (como
por ejemplo SSE o AVX)~\cite{BookXeonPhi}. Tambi\'en el \textit{set} de instrucciones cuenta con operaciones FMA (\textit{Fused
Multiply Add}) que utilizan los tres operandos como fuente y uno de ellos como el destino.

Una expansi\'on adicional para aprovechar los registros amplios son los registros de m\'ascara (\textit{masked registers}).
Estos registros de 16 bits se utilizan en varias de las instrucciones vectoriales para habilitar o deshabilitar elementos
de los 16 (como m\'aximo) que tiene un registro SIMD del Xeon Phi como parte de un c\'omputo, contribuyendo a la generalidad de
las instrucciones adicionales de la arquitectura. Si un elemento no estuviera en el registro de m\'ascara, el valor resultado de la operaci\'on
sobre los dos elementos correspondientes de los operandos fuentes es escrito en el lugar correspondiente del operando destino.

As\'i como ocurre con los 32 registros SIMD, los 8 registros de m\'ascara son
por \textit{thread}. El uso de estas m\'ascaras esta contemplado en las instrucciones, requiriendo un ciclo de clock adicional
para pasar por la \textit{Mask pipeline}.

Las instrucciones especiales para exponenciaci\'on forman parte de la EMU (\textit{Extended Math Unit}), utilizando cuadratura
num\'erica y tablas para aproximar las funciones trascendentales por \textit{hardware}. Debido a la prevalencia en c\'omputo
cient\'ifico de estas instrucciones se busc\'o la disminuci\'on en el uso de ciclos de reloj para su c\'alculo.

La adici\'on de instrucciones sobre memoria no secuencial (\textit{scatter and gather}) resultan interesantes por su utilizaci\'on
en HPC y porque facilitan la tarea de generaci\'on de c\'odigo vectorial en los \textit{backends} de compiladores. Tambi\'en, con
el prop\'osito de permitir un control m\'as fino de los cach\'es, los operandos en memoria para operaciones vectoriales permiten
la inclusi\'on de un atributo denominado \textit{eviction hint}. Este atributo permite determinar si los datos ser\'an o no
reusados, escribi\'endolos directamente a memoria si no fuese as\'i. De esta manera se evita la contaminaci\'on de cache con datos
que no ser\'an vueltos a utilizar en c\'alculos posteriores. Como su nombre lo indica, este atributo es un consejo al coprocesador,
y este puede ignorarlo.

Por \'ultimo, tambi\'en la unidad vectorial implementa un \textit{prefetcher} por software, tanto para los caches L1 y L2. Estos
pueden combinarse con instrucciones de \textit{gather} y \textit{scatter} para disminuir los accesos a memoria y los \textit{stalls}
en retiro de instrucciones.

\subsection{Organizaci\'on de la memoria}

La memoria principal del Xeon Phi consiste de memoria RAM GDDR5 en la placa, de 8 GB. Los cores y la memoria principal son comunicados mediante el uso de 8
controladoras, conectadas mediante un anillo bidireccional de dos canales a 5.5 Gb/s. Cada transferencia realizada es de 4 bytes.
Esto nos da un l\'imite de ancho de banda te\'orico de 352 GB/s pero detalles de implementaci\'on de los chips limitan este valor a 200 GB/s.

Las controladores reciben los pedidos mediante el anillo de direcciones y convierten estos en comandos para GDDR5, retornando finalmente los datos en
el anillo de datos. Los dispositivos GDDR5 tienen el espacio de direcciones interlineado entre distintos bancos de memoria para distribuir mejor la carga de pedidos.

La memoria esta dividida en bancos.

\subsection{Conexi\'on Host - Coprocesador}

El Xeon Phi se conecta con su host mediante el uso de un bus PCI Express 2.0 de 16 lineas. Las transferencias pueden ser
mediante I/O programada o DMA (\textit{Direct Memory Access}). Este bus permite la transferencia no solo al host sino a
otros coprocesadores, permitiendo tener m\'ultiples placas en una misma computadora. La velocidad de transferencia alcanzada
es de m\'as de 6 GB/s. DMA ocurre a velocidad de \textit{clock} del procesador, y los 8 canales de DMA pueden transferir
en paralelo. Cada transacci\'on de DMA es entre 64 y 256 bytes.

Siendo que el coprocesador dispone de su propio sistema operativo basado en Linux, lo cual incluye una implementaci\'on
de un \textit{stack} completo de TCP/IP, el bus PCI se puede utilizar para comunicar el sistema operativo del host con
el del Xeon Phi a trav\'es de protocolos como SSH (Secure SHell). Esto facilita la administraci\'on de la placa y la
transferencia de c\'odigo y datos para ejecuci\'on, y el uso del coprocesador como nodo de computaci\'on adicional en un
esquema de cluster.

Como el Xeon Phi es aut\'onomo con respecto al SO del host, no tiene acceso a perif\'ericos, en particular dispositivos de almacenamiento.
Para esto se puede utilizar un sistema NFS (\textit{Network File System}) con el prop\'osito de almacenar datos en el host que sean visibles
desde el Xeon Phi.

\subsection{Modos de ejecuci\'on}

Existen tres m\'etodos para ejecutar c\'odigo en el coprocesador.

\begin{enumerate}
    \item Nativo: El Xeon Phi permite correr c\'odigo directamente. Esto es gracias a la presencia de BusyBox Linux como sistema operativo,
    lo cual da soporte de sistema de archivos y entorno de ejecuci\'on. El c\'odigo puede ser enviado al Xeon Phi desde el host, y por defecto se utiliza un sistema
    de archivos montado sobre la propia RAM.
    \item Offloading: El \textit{host} puede delegar la ejecuci\'on de ciertas porciones de c\'odigo al coprocesador.
    Esto requiere que los datos necesarios para el c\'omputo sean copiados del \textit{host} al Xeon Phi, lo cual puede implicar que el bus puede ser un cuello de botella
    importante (puesto que los datos de entrada, y la salida deben ser movido al Xeon Phi y recuperados al finalizar el c\'omputo).
    \item Sim\'etrico: En este modo de ejecuci\'on se piensa al Xeon Phi y su host como dos nodos en un \textit{cluster} de c\'omputo, y al bus PCIe como una red de alta velocidad.
    Este modo es especialmente interesante si se dispone de m\'as de un Xeon Phi en un mismo host,
    y se utiliza una interfaz de pasado de mensajes entre ellos como por ejemplo MPI (\textit{Message Passing Interface}).
\end{enumerate}

\subsection{Consumo}

El consumo energ\'etico es uno de los principales factores considerados para el dise\~no de la arquitectura MIC. La filosof\'ia
utilizada para lograr un m\'aximo equilibrio de potencia y consumo es que la aplicaci\'on dictamina al hardware como
manejar los estados de poder que dispone.

Existen varios estados de consumo energ\'etico: Turbo, P-states, C-states y M-states. En modo Turbo, seg\'un la cantidad de
cores ejecutando, se puede incrementar la frecuencia de reloj de los mismos. La frecuencia de operaci\'on de todos los procesadores
puede modificarse usando los distintos P-states, que rigen la velocidad del clock general. Adicionalmente, cada core puede estar en
distintos C-states. El procesador puede correr a su m\'axima capacidad (C0), puede no recibir el clock general pero si interrupciones
(C1), o puede estar apagado en su totalidad (C6). Existen estados similares para los controladores de memoria (M-states).

Con el objetivo de proveer informaci\'on de temperatura y uso al sistema operativo y aplicaciones, el coprocesador dispone de
hardware dedicado de colecci\'on de estos datos para uso posterior. Con esto se puede hacer uso inteligente de los diferentes
estados del procesador de acuerdo a la carga de trabajo del sistema y su comportamiento.

Esta interfaz con el hardware es utilizada por tanto el sistema operativo incluido en la placa como por el \textit{stack} de
aplicaci\'on (MPSS, \textit{Manycore Platform Software Stack}) para controlar el consumo energ\'etico. Por ejemplo, cuando el
coprocesador se encuentra \textit{idle}, sin trabajo que realizar.

\subsection{Herramientas de desarrollo y profiling}

El desarrollo de aplicaciones para el Xeon Phi es muy similar al desarrollo de aplicaciones para las arquitecturas
de la linea Xeon de Intel. Esto es gracias a la existencia de compiladores de C, C++ y Fortran (los lenguajes m\'as usados en
el \'ambito de HPC) que pueden producir c\'odigo para el Xeon Phi, y la presencia de un sistema operativo y \textit{stack}
de aplicaciones que facilitan la tarea de ejecuci\'on y mantenimiento del c\'odigo. A diferencia de \nvidia CUDA, no es necesario
el uso de un lenguaje de programaci\'on separado.

Los distintos modos de uso para el Xeon Phi reciben soporte de diversas librer\'ias. El uso mediante \textit{offloading}, por ejemplo,
es provisto mediante el uso de \textit{pragmas} y opciones de compilaci\'on que permiten indicar que computo y como deben ser transferidos
a la placa. Estos pragmas permiten control fino de como enviar los datos a la placa, si deben ser transmitidos de vuelta de la placa,
etc.

Asimismo los compiladores de Intel pueden generar c\'odigo para la arquitectura MIC, el cual se puede ejecutar directamente sobre
el Xeon Phi a trav\'es del sistema operativo.

Por \'ultimo, Intel tambi\'en provee librer\'ias portadas como OpenMP, MPI, Thread Building Blocks, etc. Librer\'ias ya existentes
como la MKL (\textit{Math Kernel Library}), que implementa las conocidas rutinas de BLAS y LAPACK, tambi\'en tienen su versi\'on
pensada para Xeon Phi, e incluso pueden utilizar de manera transparente el Xeon Phi.

\subsection{Idoneidad para la tarea}

Siendo que el mercado principal para Xeon Phi son las aplicaciones de computo cient\'ifico, esto hace que la
aplicaci\'on LIO sea ya de por si una posible candidata al uso de esta tecnolog\'ia. Otros aspectos que
hacen interesante portar la misma al uso del Xeon Phi son la existencia de una gran cantidad de c\'odigo ya
implementada en C++ y Fortran, la cual ya puede ser movida a Xeon Phi sin requerir m\'as que una compilaci\'on,
seg\'un Intel.

El c\'odigo existente en LIO tiene una performance fuertemente atada al costo del c\'omputo asociado,
siendo esta tarea altamente paralelizable (como se demostr\'o en las implementaciones ya realizadas usando CUDA) y
vectorizable. Estas son las caracter\'isticas que requiere un problema para que valga la pena considerar la
implementaci\'on en Xeon Phi.
