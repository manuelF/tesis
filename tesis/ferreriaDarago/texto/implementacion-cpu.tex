Asi como ya exist\'ia una base de c\'odigo original de CUDA, el
trabajo sobre CPU tambi\'en se realiz\'o sobre el c\'odigo original
existente. El mismo fue dise\~nado de manera uniprocesador, y teniendo
en mente un set de instrucciones SIMD anterior a AVX, donde el tama\~no
de los registros de procesador era de 128 bits.

Con el prop\'osito de adaptar el c\'odigo para procesadores
paralelos y vectoriales como lo son los de la gama Xeon y Xeon Phi de
Intel, se busc\'o vectorizar y paralelizar el c\'odigo tanto como fuese
posible. En particular, se prioriz\'o lograr una gran escalabilidad en
n\'umero de procesadores, especialmente en las pruebas realizadas en el
Xeon.

De acuerdo a la bibliografi\'ia~\cite{Jeffers}, es necesario lograr que
el c\'odigo este no solamente bien vectorizado sino que escale con la
cantidad de procesadores para poder hacer uso de las prestaciones del
coprocesador Xeon Phi. Por lo tanto, las pruebas iniciales se concentraron
en lograr buena escalabilidad y vectorizac\'on en CPU.

Puesto que muchas decisiones arquitecturales del c\'odigo se realizaron en
base a experimentos con prototipos representativos de las diversas operaciones,
incluiremos algunos detalles del dispositivo utilizado para las pruebas.

La computadora utilizada para las pruebas fue un servidor \textit{dual-socket}
con 12 procesadores Intel Xeon CPU E5-2620 v2, dividos en dos grupos de 6. Los
procesadores corren a una frecuencia de clock de 2.10 GHz, y soportan el set
de instrucciones x86-64 con AVX 1.
Cada procesador cuenta con 64 Kb de cache L1, 2 Mb de cache L2 para cada par de cores, y
15 Mb de cache L3 compartida.
El mismo contaba con 32 GB de memoria RAM DDR3 a 4 canales de memoria, dando 
una transferencia te\'orica m\'axima de 42.6 GB/s, con ciclo de clock a 1.3 MHz.

La familia de procesadores Xeon cuenta con tecnolog\'ia Turbo Boost. La misma 
incluye una funcionalidad en el chip que ajusta din\'amicamente la frecuencia
de los procesadores de acuerdo a la temperatura y potencia empleadas. Esto, si
bien a \textit{a priori} es algo \'util ya que mejora la performance de los 
programas, dificulta el an\'alisis de escalabilidad ya que al utilizar m\'as
procesadores, aumenta el consumo energ\'etico y temperatura y por tanto la
frecuencia de los procesadores disminuye. Para evitar este efecto en nuestro
an\'alisis se deshabilito Turbo Boost al realizar las pruebas.

Asimismo, originalmente los procesadores Intel Xeon cuentan con Hyperthreading,
dando un total de 24 procesadores virtuales en vez de 12 n\'ucleos f\'isicos.
Sin embargo, los dos hilos de ejecuci\'on (\textit{hyperthreads}) en un mismo
procesador comparten unidades b\'asicas como las ALU. Al no ser totalmente
independientes, esto tambi\'en dificulta el an\'alisis. Para no tomar esto en
cuenta se trabajo con Hyperthreading deshabilitado.

Las pruebas tambi\'en se ajustaron en base al coprocesador Xeon Phi que estaba
conectado al Xeon como host. El modelo de coprocesador usado contaba con 61 
cores y 8 Gb de memoria RAM, los valores est\'andar para la l\'inea actual de
Xeon Phi.

La secci\'on de c\'odigo trabajada corresponde a la parte del procesamiento
de LIO optimizada mediante CUDA. Esta parte del c\'odigo estaba ya implementada
en C++, utilizando extensiones de Intel ICC para vectorizaci\'on . No se busc\'o
optimizar otras secciones de c\'odigo ya que las mismas no contaban con una
contrapartida en CUDA, haciendo entonces imposible comparar esta arquitectura con
Intel Xeon y Xeon Phi. Si bien el tiempo de ejecuci\'on de esas secciones, por su
caracter serial y poco optimizado, terminan consumiendo una fracci\'on del tiempo
no despreciable, se consider\'o fuera del enfoque de este trabajo trabajar sobre
ellas. La optimizaci\'on de las mismas para aprovechar de las arquitecturas
estudiadas queda como trabajo a futuro.

Como caso de estudio para las modificaciones, se utiliz\'o como ejemplo el c\'omputo
de DFT sobre una mol\'ecula de hemoglobina. El caso es considerado de tama\~no
mediano para las pruebas usuales que se realizan con el paquete LIO. Si bien la 
cantidad de \'atomos es peque\~na, el n\'umero de electrones y sus interacciones
hacen que sea necesario muchas funciones y muchos puntos de integraci\'on para
modelarlo correctamente. En particular, uno de los \'atomos (el \'atomo de hierro)
posee muchas capas electr\'onicas y por ende una gran cantidad de puntos y 
funciones a calcular. Los datos espec\'ificos se encuentran en el ap\'endice.

% TODO(jpdarago): Agregar datos de hemoglobina en el apendice

\subsection{Estructura original del c\'odigo}

Un esquema de alto nivel del computo m\'as intensivo realizado por el paquete
G2G se encuentra en la figura~\ref{algo:lio-iteration}. Este c\'omputo corresponde
a una iteraci\'on de c\'omputo de la matriz de Kohn-Sham. El pseudoc\'odigo
corresponde a la implementaci\'on en C++ del mismo para los casos donde se
utiliza GGA (\textit{Gradient Global Approximation}) y se calculan tanto las
fuerzas como la matriz resultado.

El marco general de la aplicaci\'on a nivel pseudoc\'odigo se encuentra en la
figura~\ref{algo:lio-general-schematics}. Corresponde al detalle del esquema
a alto nivel mostrado en la figura~\ref{fig:lio-steps}.

Las matrices $G$ y $H$ corresponden a matrices de vectores $\mathbb{R}^3$, y
la matriz $F$ de valores de funciones es de escalares. Las operaciones entre
vectores y vectores y escalares tienen la sem\'antica esperable de algebra
lineal. El producto entre dos vectores debe ser interpretado como producto
componente a componente, no como producto escalar o vectorial.

La implementaci\'on de las operaciones entre vectores merece particular atenci\'on.
Para aprovechar el set de instrucciones SSE 4, la ultima versi\'on disponible al
momento de realizar la implementaci\'on, la clase que implementa un vector de 3
componentes (\texttt{cvector3}) se adapt\'o mediante herencia para mapear a un
registro de SSE 4. Dado que el ancho de estos registros es de 128 bits, los mismos
permiten realizar calculos de a 4 punto flotante a la vez. Al ser 3, uno de los
elementos del registro SSE no se utiliza.

La representaci\'on de un vector de tres componentes de esta manera, si bien da
un \textit{speedup} significativo, no es portable ni escalable. Al incrementar el
ancho de registro SIMD m\'as de los campos deben ser ignorados, malgastando cada
vez m\'as espacio. Asimismo, se desperdician oportunidades por parte del compilador
para optimizar mejor haciendo uso de todos los registros y operaciones que dispone
la arquitectura. 

Uno de los computos, \texttt{ssyr}, utilizaba la GSL (\textit{GNU Scientific Library}).
La misma se reemplazo por la MKL (\textit{Math Kernel Library}) de Intel ya que
la operaci\'on corresponde a una subrutina de BLAS nivel 2 muy conocida. Esto se
realiz\'o principalmente para evitar tener que hacer una compilaci\'on y linkeo
de la librer\'ia GSL en el Xeon Phi, y para disminuir la cantidad de c\'odigo a
optimizar dado que la MKL ya se utilizaba en otras secciones del programa.

\begin{algorithm}[H]
        \caption{Pseudoc\'odigo de la iteraci\'on original de LIO}
        \label{algo:lio-iteration}
        \begin{algorithmic}
            \Function{iteration}{$PG : PointGroup, Forces: \mathbb{R}^{fr \times fc}, RMMO: \mathbb{R}^{rr \times rc}$}
              \State $RMM^{IN} \gets rmm\_input(G)$
              \State $F \gets functions(PG)$
              \State $G \gets gradient(PG)$
              \State $H \gets hessian(PG)$
              \State $energy \gets 0$
              \State $n,m \gets \# functions(PG), \# points(G)$
              \ForAll{$p \in [0..m)$}
                  \State $pd, dxyz,dd1,dd2 \gets 0, (0,0,0), (0,0,0), (0,0,0)$
                  \ForAll{$i \in [0..n)$}
                      \State $w \gets \Sigma_{0 \leq j \leq i} F_{p,j} \cdot RMM_{ij}$
                      \State $w3 \gets \Sigma_{0 \leq j \leq i} G_{p,j} \cdot RMM_{ij}$
                      \State $ww1 \gets \Sigma_{0 \leq j \leq i} H_{p,2j} \cdot RMM_{ij}$
                      \State $ww2 \gets \Sigma_{0 \leq j \leq i} H_{p,2j+1} \cdot RMM_{ij}$
                      \State $pd \gets pd + F_{p,i} w$
                      \State $dxyz \gets dxyz + G_{p,i} \cdot w + w3 \cdot F_{p,i}$
                      \State $dd1 \gets dd1 + 2 \cdot w3 \cdot G_{p,i} + H_{p,2i} \cdot w + ww1 \cdot F_{p,i}$
                      \State $FgXXY \gets (G_{p,i}^x, G_{p,i}^x, G_{p,i}^y)$
                      \State $w3XXY \gets (w3^x, w3^x, w3^y)$
                      \State $w3YYZ \gets (w3^y, w3^y, w3^z)$
                      \State $FgYZZ \gets (G_{p,i}^y, G_{p,i}^z, G_{p,i}^z)$
                      \State $dd2 \gets dd2 + FgXXY \cdot w3YYZ + FgYYZ \cdot w3XXY + H_{p,2j+1} \cdot w + ww2 \cdot F_{p,i}$
                  \EndFor 
                  \State $exch, corr, y2a \gets potencial(pd, dxyz, dd1, dd2)$
                  \State $energy \gets energy + (weight(p) \cdot pd) \cdot (exch + corr)$
                  \State $FR_p \gets y2a \cdot weight(p)$
              \EndFor
              \State $RMM^{OUT}_{i,j} \gets \Sigma_{k < m} F_k \cdot F_k' \cdot FR_k$
              \ForAll{$i,j \gets rmm\_indexes(PG)$}
                  \State $RMMO_{i,j} \gets RMMO_{i,j} + RMM^{OUT}_{i,j}$
              \EndFor
            \EndFunction
        \end{algorithmic}
\end{algorithm}

\subsection{Cambios en la vectorizaci\'on}

Uno de los primeros cambios fue la estructura de vectorizaci\'on para el ciclo 
interno de la iteraci\'on de LIO.

En base a la necesidad de que el c\'odigo escale correctamente con el ancho de registros
SIMD (256 bits para AVX 1 en Xeon y 512 bits para AVX 2 en el Xeon Phi), se elimin\'o el
uso de clases SIMD expl\'icitas para vectores de 3 componentes. 

Esto deleg\'o al compilador la tarea de vectorizar el c\'odigo apropiadamente. Inicialmente
esto produjo una degradaci\'on importante de performance, dado que la organizaci\'on del
c\'odigo no permite al compilador vectorizar efectivamente. 

Para revertir esta regresi\'on, se modific\'o la organizaci\'on del c\'odigo de manera de
realizar los c\'omputos de manera m\'as amena a la optimizaci\'on. La observaci\'on clave
para esto es que los computos se realizan siempre componente a componente. En particular,
el ciclo m\'as intensivo de c\'omputo (el interior de la iteraci\'on) puede dividirse
en cada componente para convertirse en 10 operaciones de suma sobre un vector de
elementos. 

Una heur\'istica de optimizaci\'on para estos casos es convertir los arreglos de
estructuras (como por ejemplo las matrices de gradiente $G$ y de hessiano $H$, que
son matrices densas de estructuras de tres valores) en estructuras de arreglos.
En este caso esto implica partir las matrices en 3 matrices (una para cada
componente), sino tambi\'en dividir el hessiano en sus componentes pares e impares.
De esta manera, cada componente es un valor escalar y los ciclos pueden reescribirse
para utilizar operaciones de punto flotante usuales sobre cada elemento.

El c\'odigo resultante para el ciclo m\'as interno puede verse en el pseudoc\'odigo
de la figura~\ref{fig:lio-pseudo-dividir}. 

%TODO(jpdarago): Agregar pseudocodigo para esta parte

La figura~\ref{fig:lio-post-partir-mats} muestra los resultados de realizar esta
optimizaci\'on en el caso de prueba de hemoglobina en la computadora de prueba.

%TODO(jpdarago): Agregar este grafico con regresion.

\subsection{Almacenamiento de las matrices}

Una mejora considerable surge tambi\'en de utilizar una estrategia de \textit{caching}
similar a la utilizada en el c\'odigo de GPU para evitar el c\'alculo de las mismas
en cada iteraci\'on. A diferencia de GPU, la memoria principal es de f\'acil y
amplia disponibilidad para procesadores est\'andar, con lo cual es posible en ese
caso calcular para cada grupo de puntos, el gradiente hessiano y valores de 
funciones una \'unica vez antes de empezar las iteraciones.

De manera de poder controlar esto, se habilit\'o como una opci\'on en tiempo de
compilaci\'on al programa.

Esta modificaci\'on es sencilla ya que podemos utilizar el mismo flag 
\textit{inGlobal} de GPU para marcar que las funciones de un grupo se encuentran
ya calculadas y almacenadas en memoria principal. El impacto en el ciclo de 
iteraci\'on es m\'inimo y solo consiste en remover el c\'alculo de las funciones
del mismo.

La figura~\ref{fig:lio-post-cachear} muestra la diferencia entre el programa
optimizado en la secci\'on anterior, y el actual despu\'es de esta modificaci\'on.

%TODO(jpdarago): Agregar este grafico con regresion.

\subsection{Alineamiento de matrices}

De modo de facilitar la vectorizaci\'on de las operaciones realizadas en el 
ciclo principal de la iteraci\'on, se tuvo en cuenta la alineaci\'on de las 
matrices de funciones, hessiano y gradiente. La alineaci\'on a 64 bytes facilita
el uso de instrucciones vectoriales especiales del procesador para tomar los 
valores de la memoria principal~\cite{AutovectorizationGuide}.

Para alinear el comienzo de todas las matrices a 64 bytes se emplea la funci\'on
de la librer\'ia MKL de Intel \texttt{mkl\_malloc}.

Esto sin embargo no resulta sufiicente en el caso del ciclo m\'as interno de
la iteraci\'on. Para esto, necesitamos que cada fila de las matrices esten alineadas.
Dado que la cantidad de columnas de una matriz corresponde a la cantidad de funciones,
alineamos las mismas con un \texttt{padding} de ceros hasta el multiplo m\'as 
cercano de 64 bytes. Esto introduce un m\'aximo de 16 valores nulos de precisi\'on 
simple u 8 de precisi\'on doble (de acuerdo a cual se este empleando) en cada
fila.

La figura~\ref{fig:lio-post-align} muestra el resultado de aplicar esta optimizaci\'on
en precisi\'on simple para el ejemplo utilizado y la m\'aquina de prueba usada.

%TODO(jpdarago): Agregar este grafico con regresion

\subsection{Prototipos de paralelizacion}

Con la optimizaci\'on anterior observamos que los ciclos principales del c\'odigo
estaban vectorizados y buscamos entonces obtener mejor \textit{performance} haciendo
uso de los multiples procesadores disponibles.

Con el prop\'osito de simplificar la tarea sin perder control o performance, 
usamos el soporte para OpenMP del compilador ICC de Intel. OpenMP corresponde a
un \textit{estandar} de compiladores para paralelismo asistido por el usuario,
mediante \texttt{pragmas} de compilador y una librer\'ia de soporte sobre 
primitivas del sistema operativo. 

Un ejemplo de como se puede utilizar esta librer\'ia para paralelizar un ciclo
sencillo se encuentra en el c\'odigo de C++ de la figura~\ref{fig:openmp-example}.

El caracter asistido de OpenMP implica que nosotros debemos indicar como 
realizar la partici\'on de trabajo. En un primer momento vimos dos posibles rutas 
hacia paralelizar el c\'odigo: dividir los grupos entre \textit{threads} de 
procesamiento, o utilizar multiples procesadores para dividirse el trabajo 
correspondiente los puntos y funciones dentro de un mismo grupo. Esta segunda
posibilidad corresponde a lo que se realiza ya en la implementaci\'on para placas
de video.

La decisi\'on no es trivial debido a las caracter\'sticas del trabajo a realizar
y la arquitectura de los procesadores Xeon y Xeon Phi. A diferencia de las GPGPU,
estos no cuentan con una gran cantidad de procesadores y exhiben un costo alto
para lanzar un procesador. Esto hace entonces, en primera instancia, inviable
realizar una partici\'on id\'entica a la de los \textit{kernels} implementados
en CUDA. 


La grilla de integraci\'on, como ya hemos explicado, se divide en cubos y 
esferas. Las esferas corresponden a los n\'ucleos de los \'atomos del sistema.
Los cubos son determinados por la grilla de integraci\'on y tienen tama\~no
variable. El caracter heterog\'eneo y variable de estos grupos es lo que hace

Tomamos como ejemplo el caso de la hemoglobina, el empleado para ajustar los
par\'ametros de nuestra implementaci\'on. Un histograma de la cantidad de 
funciones por grupo

\subsection{An\'alisis de cargas}

\subsection{Cambios en paralelismo}

\subsection{Algoritmo de particionado}

\subsection{Algoritmo de balanceo}
