Asi como ya exist\'ia una base de c\'odigo original de CUDA, el
trabajo sobre CPU tambi\'en se realiz\'o sobre el c\'odigo original
existente. El mismo fue dise\~nado de manera uniprocesador, y teniendo
en mente un set de instrucciones SIMD anterior a AVX, donde el tama\~no
de los registros de procesador era de 128 bits.

Con el prop\'osito de adaptar el c\'odigo para procesadores
paralelos y vectoriales como lo son los de la gama Xeon y Xeon Phi de
Intel, se busc\'o vectorizar y paralelizar el c\'odigo tanto como fuese
posible. En particular, se prioriz\'o lograr una gran escalabilidad en
n\'umero de procesadores, especialmente en las pruebas realizadas en el
Xeon.

De acuerdo a la bibliografi\'ia~\cite{Jeffers}, es necesario lograr que
el c\'odigo este no solamente bien vectorizado sino que escale con la
cantidad de procesadores para poder hacer uso de las prestaciones del
coprocesador Xeon Phi. Por lo tanto, las pruebas iniciales se concentraron
en lograr buena escalabilidad y vectorizac\'on en CPU.

Puesto que muchas decisiones arquitecturales del c\'odigo se realizaron en
base a experimentos con prototipos representativos de las diversas operaciones,
incluiremos algunos detalles del dispositivo utilizado para las pruebas.

La computadora utilizada para las pruebas fue un servidor \textit{dual-socket}
con 12 procesadores Intel Xeon CPU E5-2620 v2, dividos en dos grupos de 6. Los
procesadores corren a una frecuencia de clock de 2.10 GHz, y soportan el set
de instrucciones x86-64 con AVX 1.
Cada procesador cuenta con 64 Kb de cache L1, 2 Mb de cache L2 para cada par de cores, y
15 Mb de cache L3 compartida.
El mismo contaba con 32 GB de memoria RAM DDR3 a 4 canales de memoria, dando 
una transferencia te\'orica m\'axima de 42.6 GB/s, con ciclo de clock a 1.3 MHz.

La familia de procesadores Xeon cuenta con tecnolog\'ia Turbo Boost. La misma 
incluye una funcionalidad en el chip que ajusta din\'amicamente la frecuencia
de los procesadores de acuerdo a la temperatura y potencia empleadas. Esto, si
bien a \textit{a priori} es algo \'util ya que mejora la performance de los 
programas, dificulta el an\'alisis de escalabilidad ya que al utilizar m\'as
procesadores, aumenta el consumo energ\'etico y temperatura y por tanto la
frecuencia de los procesadores disminuye. Para evitar este efecto en nuestro
an\'alisis se deshabilito Turbo Boost al realizar las pruebas.

Asimismo, originalmente los procesadores Intel Xeon cuentan con Hyperthreading,
dando un total de 24 procesadores virtuales en vez de 12 n\'ucleos f\'isicos.
Sin embargo, los dos hilos de ejecuci\'on (\textit{hyperthreads}) en un mismo
procesador comparten unidades b\'asicas como las ALU. Al no ser totalmente
independientes, esto tambi\'en dificulta el an\'alisis. Para no tomar esto en
cuenta se trabajo con Hyperthreading deshabilitado.

Las pruebas tambi\'en se ajustaron en base al coprocesador Xeon Phi que estaba
conectado al Xeon como host. El modelo de coprocesador usado contaba con 61 
cores y 8 Gb de memoria RAM, los valores est\'andar para la l\'inea actual de
Xeon Phi.

La secci\'on de c\'odigo trabajada corresponde a la parte del procesamiento
de LIO optimizada mediante CUDA. Esta parte del c\'odigo estaba ya implementada
en C++, utilizando extensiones de Intel ICC para vectorizaci\'on . No se busc\'o
optimizar otras secciones de c\'odigo ya que las mismas no contaban con una
contrapartida en CUDA, haciendo entonces imposible comparar esta arquitectura con
Intel Xeon y Xeon Phi. Si bien el tiempo de ejecuci\'on de esas secciones, por su
caracter serial y poco optimizado, terminan consumiendo una fracci\'on del tiempo
no despreciable, se consider\'o fuera del enfoque de este trabajo trabajar sobre
ellas. La optimizaci\'on de las mismas para aprovechar de las arquitecturas
estudiadas queda como trabajo a futuro.

Como caso de estudio para las modificaciones, se utiliz\'o como ejemplo el c\'omputo
de DFT sobre una mol\'ecula de hemoglobina. El caso es considerado de tama\~no
mediano para las pruebas usuales que se realizan con el paquete LIO. Si bien la 
cantidad de \'atomos es peque\~na, el n\'umero de electrones y sus interacciones
hacen que sea necesario muchas funciones y muchos puntos de integraci\'on para
modelarlo correctamente. En particular, uno de los \'atomos (el \'atomo de hierro)
posee muchas capas electr\'onicas y por ende una gran cantidad de puntos y 
funciones a calcular. Los datos espec\'ificos se encuentran en el ap\'endice.

% TODO(jpdarago): Agregar datos de hemoglobina en el apendice

\subsection{Estructura original del c\'odigo}

Un esquema de alto nivel del computo m\'as intensivo realizado por el paquete
G2G se encuentra en la figura~\ref{algo:lio-iteration}. Este c\'omputo corresponde
a una iteraci\'on de c\'omputo de la matriz de Kohn-Sham. El pseudoc\'odigo
corresponde a la implementaci\'on en C++ del mismo para los casos donde se
utiliza GGA (\textit{Gradient Global Approximation}) y se calculan tanto las
fuerzas como la matriz resultado.

El marco general de la aplicaci\'on a nivel pseudoc\'odigo se encuentra en la
figura~\ref{algo:lio-general-schematics}. Corresponde al detalle del esquema
a alto nivel mostrado en la figura~\ref{fig:lio-steps}.

Las matrices $G$ y $H$ corresponden a matrices de vectores $\mathbb{R}^3$, y
la matriz $F$ de valores de funciones es de escalares. Las operaciones entre
vectores y vectores y escalares tienen la sem\'antica esperable de algebra
lineal. El producto entre dos vectores debe ser interpretado como producto
componente a componente, no como producto escalar o vectorial.

La implementaci\'on de las operaciones entre vectores merece particular atenci\'on.
Para aprovechar el set de instrucciones SSE 4, la ultima versi\'on disponible al
momento de realizar la implementaci\'on, la clase que implementa un vector de 3
componentes (\texttt{cvector3}) se adapt\'o mediante herencia para mapear a un
registro de SSE 4. Dado que el ancho de estos registros es de 128 bits, los mismos
permiten realizar calculos de a 4 punto flotante a la vez. Al ser 3, uno de los
elementos del registro SSE no se utiliza.

La representaci\'on de un vector de tres componentes de esta manera, si bien da
un \textit{speedup} significativo, no es portable ni escalable. Al incrementar el
ancho de registro SIMD m\'as de los campos deben ser ignorados, malgastando cada
vez m\'as espacio. Asimismo, se desperdician oportunidades por parte del compilador
para optimizar mejor haciendo uso de todos los registros y operaciones que dispone
la arquitectura. 

Uno de los computos, \texttt{ssyr}, utilizaba la GSL (\textit{GNU Scientific Library}).
La misma se reemplazo por la MKL (\textit{Math Kernel Library}) de Intel ya que
la operaci\'on corresponde a una subrutina de BLAS nivel 2 muy conocida. Esto se
realiz\'o principalmente para evitar tener que hacer una compilaci\'on y linkeo
de la librer\'ia GSL en el Xeon Phi, y para disminuir la cantidad de c\'odigo a
optimizar dado que la MKL ya se utilizaba en otras secciones del programa.

\begin{algorithm}[H]
        \caption{Pseudoc\'odigo de la iteraci\'on original de LIO}
        \label{algo:lio-iteration}
        \begin{algorithmic}
            \Function{iteration}{$PG : PointGroup, Forces: \mathbb{R}^{fr \times fc}, RMMO: \mathbb{R}^{rr \times rc}$}
              \State $RMM^{IN} \gets rmm\_input(G)$
              \State $F \gets functions(PG)$
              \State $G \gets gradient(PG)$
              \State $H \gets hessian(PG)$
              \State $energy \gets 0$
              \State $n,m \gets \# functions(PG), \# points(G)$
              \ForAll{$p \in [0..m)$}
                  \State $pd, dxyz,dd1,dd2 \gets 0, (0,0,0), (0,0,0), (0,0,0)$
                  \ForAll{$i \in [0..n)$}
                      \State $w \gets \Sigma_{0 \leq j \leq i} F_{p,j} \cdot RMM_{ij}$
                      \State $w3 \gets \Sigma_{0 \leq j \leq i} G_{p,j} \cdot RMM_{ij}$
                      \State $ww1 \gets \Sigma_{0 \leq j \leq i} H_{p,2j} \cdot RMM_{ij}$
                      \State $ww2 \gets \Sigma_{0 \leq j \leq i} H_{p,2j+1} \cdot RMM_{ij}$
                      \State $pd \gets pd + F_{p,i} w$
                      \State $dxyz \gets dxyz + G_{p,i} \cdot w + w3 \cdot F_{p,i}$
                      \State $dd1 \gets dd1 + 2 \cdot w3 \cdot G_{p,i} + H_{p,2i} \cdot w + ww1 \cdot F_{p,i}$
                      \State $FgXXY \gets (G_{p,i}^x, G_{p,i}^x, G_{p,i}^y)$
                      \State $w3XXY \gets (w3^x, w3^x, w3^y)$
                      \State $w3YYZ \gets (w3^y, w3^y, w3^z)$
                      \State $FgYZZ \gets (G_{p,i}^y, G_{p,i}^z, G_{p,i}^z)$
                      \State $dd2 \gets dd2 + FgXXY \cdot w3YYZ + FgYYZ \cdot w3XXY + H_{p,2j+1} \cdot w + ww2 \cdot F_{p,i}$
                  \EndFor 
                  \State $exch, corr, y2a \gets potencial(pd, dxyz, dd1, dd2)$
                  \State $energy \gets energy + (weight(p) \cdot pd) \cdot (exch + corr)$
                  \State $FR_p \gets y2a \cdot weight(p)$
              \EndFor
              \State $RMM^{OUT}_{i,j} \gets \Sigma_{k < m} F_k \cdot F_k' \cdot FR_k$
              \ForAll{$i,j \gets rmm\_indexes(PG)$}
                  \State $RMMO_{i,j} \gets RMMO_{i,j} + RMM^{OUT}_{i,j}$
              \EndFor
            \EndFunction
        \end{algorithmic}
\end{algorithm}

\subsection{Cambios en la vectorizaci\'on}

Uno de los primeros cambios fue la estructura de vectorizaci\'on para el ciclo 
interno de la iteraci\'on de LIO.

En base a la necesidad de que el c\'odigo escale correctamente con el ancho de registros
SIMD (256 bits para AVX 1 en Xeon y 512 bits para AVX 2 en el Xeon Phi), se elimin\'o el
uso de clases SIMD expl\'icitas para vectores de 3 componentes. 

Esto deleg\'o al compilador la tarea de vectorizar el c\'odigo apropiadamente. Inicialmente
esto produjo una degradaci\'on importante de performance, dado que la organizaci\'on del
c\'odigo no permite al compilador vectorizar efectivamente. 

Para revertir esta regresi\'on, se modific\'o la organizaci\'on del c\'odigo de manera de
realizar los c\'omputos de manera m\'as amena a la optimizaci\'on. La observaci\'on clave
para esto es que los computos se realizan siempre componente a componente. En particular,
el ciclo m\'as intensivo de c\'omputo (el interior de la iteraci\'on) puede dividirse
en cada componente para convertirse en 10 operaciones de suma sobre un vector de
elementos. 

Una heur\'istica de optimizaci\'on para estos casos es convertir los arreglos de
estructuras (como por ejemplo las matrices de gradiente $G$ y de hessiano $H$, que
son matrices densas de estructuras de tres valores) en estructuras de arreglos.
En este caso esto implica partir las matrices en 3 matrices (una para cada
componente), sino tambi\'en dividir el hessiano en sus componentes pares e impares.
De esta manera, cada componente es un valor escalar y los ciclos pueden reescribirse
para utilizar operaciones de punto flotante usuales sobre cada elemento.

El c\'odigo resultante para el ciclo m\'as interno puede verse en el pseudoc\'odigo
de la figura~\ref{fig:lio-pseudo-dividir}. 

%TODO(jpdarago): Agregar pseudocodigo para esta parte

La figura~\ref{fig:lio-post-partir-mats} muestra los resultados de realizar esta
optimizaci\'on en el caso de prueba de hemoglobina en la computadora de prueba.

%TODO(jpdarago): Agregar este grafico con regresion.

\subsection{Almacenamiento de las matrices}

Una mejora considerable surge tambi\'en de utilizar una estrategia de \textit{caching}
similar a la utilizada en el c\'odigo de GPU para evitar el c\'alculo de las mismas
en cada iteraci\'on. A diferencia de GPU, la memoria principal es de f\'acil y
amplia disponibilidad para procesadores est\'andar, con lo cual es posible en ese
caso calcular para cada grupo de puntos, el gradiente hessiano y valores de 
funciones una \'unica vez antes de empezar las iteraciones.

De manera de poder controlar esto, se habilit\'o como una opci\'on en tiempo de
compilaci\'on al programa.

Esta modificaci\'on es sencilla ya que podemos utilizar el mismo flag 
\textit{inGlobal} de GPU para marcar que las funciones de un grupo se encuentran
ya calculadas y almacenadas en memoria principal. El impacto en el ciclo de 
iteraci\'on es m\'inimo y solo consiste en remover el c\'alculo de las funciones
del mismo.

La figura~\ref{fig:lio-post-cachear} muestra la diferencia entre el programa
optimizado en la secci\'on anterior, y el actual despu\'es de esta modificaci\'on.

%TODO(jpdarago): Agregar este grafico con regresion.

\subsection{Alineamiento de matrices}

De modo de facilitar la vectorizaci\'on de las operaciones realizadas en el 
ciclo principal de la iteraci\'on, se tuvo en cuenta la alineaci\'on de las 
matrices de funciones, hessiano y gradiente. La alineaci\'on a 64 bytes facilita
el uso de instrucciones vectoriales especiales del procesador para tomar los 
valores de la memoria principal~\cite{AutovectorizationGuide}.

Para alinear el comienzo de todas las matrices a 64 bytes se emplea la funci\'on
de la librer\'ia MKL de Intel \texttt{mkl\_malloc}.

Esto sin embargo no resulta sufiicente en el caso del ciclo m\'as interno de
la iteraci\'on. Para esto, necesitamos que cada fila de las matrices esten alineadas.
Dado que la cantidad de columnas de una matriz corresponde a la cantidad de funciones,
alineamos las mismas con un \texttt{padding} de ceros hasta el multiplo m\'as 
cercano de 64 bytes. Esto introduce un m\'aximo de 16 valores nulos de precisi\'on 
simple u 8 de precisi\'on doble (de acuerdo a cual se este empleando) en cada
fila.

La figura~\ref{fig:lio-post-align} muestra el resultado de aplicar esta optimizaci\'on
en precisi\'on simple para el ejemplo utilizado y la m\'aquina de prueba usada.

%TODO(jpdarago): Agregar este grafico con regresion

\subsection{Prototipos de paralelizacion}

Con la optimizaci\'on anterior observamos que los ciclos principales del c\'odigo
estaban vectorizados y buscamos entonces obtener mejor \textit{performance} haciendo
uso de los multiples procesadores disponibles. Para esto, buscamos una soluci\'on
que nos permitiera escalar lo mejor posible en cantidad de procesadores: Utilizar
$n$ procesadores debiera mejorar la performance lo m\'as cerca de $n$-veces posible.

%TODO(jpdarago): Esta parte de OpenMP no deberia ir en la introduccion?

Con el prop\'osito de simplificar la tarea sin perder control o performance, 
usamos el soporte para OpenMP del compilador ICC de Intel. OpenMP corresponde a
un \textit{estandar} de compiladores para paralelismo asistido por el usuario,
mediante \texttt{pragmas} de compilador y una librer\'ia de soporte sobre 
primitivas del sistema operativo. 

Un ejemplo de como se puede utilizar esta librer\'ia para paralelizar un ciclo
sencillo se encuentra en el c\'odigo de C++ de la figura~\ref{fig:openmp-example}.
En el ejemplo, las iteraciones del ciclo principal ser\'an divididas entre los
distintos \textit{threads} en porciones iguales y consecutivas, y cada uno
ejecutara el cuerpo interno del ciclo para sus iteraciones. La cantidad de 
hilos de ejecuci\'on a lanzar y como se dividen las iteraciones son controladas
mediante los par\'ametros del pragma pasado al compilador.

No solo al encontrarse la secci\'on cr\'itica los hilos de ejecuci\'on deben
sincronizarse para iniciar el trabajo, sino que al terminar cada uno debe esperar
a que los otros concluyan para que uno solo de ellos prosiga con el resto del
programa. Esto quiere decir que el inicio y fin de un \textit{loop} paralelizado
con OpenMP son puntos de sincronizaci\'on y tienen un \textit{overhead} asociado.

\begin{figure}[htbp]
    \label{fig:openmp-example}
    \begin{lstlisting}
        #pragma omp parallel for num_threads(12) schedule(static)
        for(int i = 0; i < n; i++) {
            int result = 0;
            for(int j = 0; j < m; j++) {
                result += a[i][j] * b[j];
            }
            results[i] = result;
        }
    \end{lstlisting}
    \caption{Ejemplo de programa paralelizado con OpenMP para correr en 12 threads. Cada thread
    sera asignado una cantidad fija de iteraciones consecutivas.}
\end{figure}

El caracter asistido de OpenMP implica que nosotros debemos indicar como 
realizar la partici\'on de trabajo. En un primer momento vimos dos posibles rutas 
hacia paralelizar el c\'odigo: dividir los grupos entre \textit{threads} de 
procesamiento, o utilizar multiples procesadores para dividirse el trabajo 
correspondiente los puntos y funciones dentro de un mismo grupo. Esta segunda
posibilidad corresponde a lo que se realiza ya en la implementaci\'on para placas
de video.

La decisi\'on no es trivial debido a las caracter\'sticas del trabajo a realizar
y la arquitectura de los procesadores Xeon y Xeon Phi. A diferencia de las GPGPU,
estos no cuentan con una gran cantidad de procesadores y exhiben un costo alto
para lanzar un hilo de ejecuci\'on. Los hilos de ejecuci\'on comparten memorias
cach\'e y, si su cantidad es mayor que la cantidad de procesadores disponible,
deben ser asignados a procesadores por parte del \textit{scheduler} del sistema
operativo.

Esto hace entonces, en primera instancia, inviable realizar una partici\'on id\'entica a 
la de los \textit{kernels} implementados en CUDA, donde la cuenta se realiza para 
grupos chicos de puntos.

Por otro lado, tampoco es trivial partir los grupos entre los procesadores disponibles,
y tampoco es trivial dividir los puntos de un grupo entre procesadores para resolverlos.
El motivo es la forma que tiene la grilla de integraci\'on con la que se trabaja.

Esta grilla de integraci\'on, como ya hemos explicado, se divide en cubos y 
esferas. Las esferas corresponden a los n\'ucleos de los \'atomos del sistema.
Los cubos son determinados por la grilla de integraci\'on y tienen tama\~no
variable. 

Tomamos como ejemplo el caso de la hemoglobina, el empleado para ajustar los
par\'ametros de nuestra implementaci\'on. Un histograma de la cantidad de 
funciones por grupo, y la cantidad de puntos por grupo, puede verse en la 
figura~\ref{fig:lio-histo-groups}.

\begin{figure}[htbp]
   \centering
   \begin{subfigure}[b]{\plotwidth}
     \includegraphics[width=\textwidth]{plots/cpu/histogram-functions-hemo.png}
     \caption{}
   \end{subfigure}
   \begin{subfigure}[b]{\plotwidth}
     \includegraphics[width=\textwidth]{plots/cpu/histogram-points-hemo.png}
     \caption{}
   \end{subfigure}
   \caption{Histogramas para la cantidad de puntos y funciones para los grupos correspondientes al caso de prueba
   de hemoglobina}
   \label{fig:lio-histo-groups}
\end{figure}

Como puede verse, tanto la cantidad de grupos como de funciones para un grupo es altamente
variable. Los grupos m\'as pesados como las esferas tienen una cantidad dentro de los
miles de puntos y cientos de funciones, mientras ha 748 grupos (65\%
del total) con menos de 50 funciones a calcular por punto, y 927 (80\% del total)
que poseen menos de 100 puntos.

Adicionalmente, el grupo m\'as grande en cantidad de puntos tiene 5238 veces m\'as
puntos que el m\'as chico, y el m\'as grande en cantidad de funciones tiene 24
veces m\'as puntos (debido al ajuste de alineaci\'on que se explic\'o en la 
secci\'on anterior).

Por lo tanto, identificamos las siguientes dificultades:

\begin{itemize}
    \item Dividir los grupos entre hilos de ejecuci\'on tiene la dificultad de que
    los grupos m\'as grandes dominan a los m\'as chicos, pudiendo producir un serio
    desbalance de carga de trabajo, especialmente cuando se incrementa la cantidad de
    procesadores en juego.
    \item Recorrer cada grupo y dividir los puntos a procesar entre procesadores
    tiene como desventaja que muchos grupos tienen una muy peque\~na cantidad de
    puntos, con lo cual incrementar la cantidad de procesadores no nos ser\'a util
    para resolver los mismos (porque no hay un punto que darle a uno o m\'as procesadores).
    Este desbalance tambi\'en nos parece indeseable. 
    
    Adem\'as, como ya se\~nalamos, el costo de sincronizaci\'on de hilos de 
    ejecuci\'on al terminar un ciclo paralelizado con OpenMP no es menor, por
    lo tanto si la cantidad de trabajo para cada thread es menor a este 
    \textit{overhead} no vale la pena incrementar la cantidad de \textit{threads}.
\end{itemize}

La soluci\'on propuesta a este problema es un h\'ibrido entre estas dos
estrategias: Los puntos que sean demasiado chicos en cantidad de trabajo son
reunidos y divididos entre los procesadores disponibles, y los que si sean lo
suficientemente grandes son procesados secuencialmente, pero los procesadores se
dividen el trabajo de los puntos a procesar para cada uno de los grupos.

\subsection{An\'alisis de cargas de grupos}

La divisi\'on de los grupos chicos entre procesadores es una tarea de complejidad
no trivial, puesto que el tiempo de ejecuci\'on total corresponde al tiempo m\'aximo
que uno de los procesadores tome en resolver su partici\'on, ya que todo hilo de 
ejecuci\'on que termine su trabajo antes debe esperarlo para sincronizarse.

Puesto que antes de empezar las iteraciones sabemos cuales son los grupos a 
procesar, podemos usar estar informaci\'on para hacer una partici\'on de los mismos
una vez antes de empezar a iterar. Para esto necesitamos un estimativo del costo
y un algoritmo que, dados los costos de los grupos y la cantidad de threads a 
utilizar, asigne cada grupo a cada thread.

Para tener una idea del costo de cada grupo, usamos el estimador de operaciones
dado por~\cite{LIO} junto con un ajuste fijo para considerar overheads fijos a 
cada grupo. Matem\'aticamente el estimador utilizado es:

\begin{equation}
    Costo(PG) = \frac{\#f(PG) \cdot \#p(PG) \cdot (\#p(PG) + 1)}{2} + C
\end{equation}

donde $f(PG)$ son las funciones del grupo y $p(PG)$ los puntos del grupo. La 
constante fue ajustada experimentalmente de acuerdo a los ejemplos disponibles y
su impacto en el tiempo de ejecuci\'on para los mismos ser\'a descripto en 
secciones posteriores.

\subsection{Algoritmo de particionado}

El algoritmo de particionado recibe como entrada un vector $C = {C_1, \cdots, C_n}$
de costos y un valor $m$, la cantidad de threads a utilizar, y devuelve una 
partici\'on de los mismos $P_1, \dots, P_m$ tal que

\begin{align}
    \bigcup P_i & = C \\
    P_i \bigcap P_j & = \emptyset, \qquad \forall 1 \leq i,j \leq m, i \neq j
    \label{eq:partition-conditions}
\end{align}

y que minimiza el valor m\'aximo de peso para una partici\'on, es decir:

\begin{align}
    \max \sum 
\end{align}

Un aspecto importante a se\~nalar de este problema es que pertenece a la clase
de problemas computacionales \textit{NP-hard}. Esta clase de problemas forma una
clase de equivalencia con respecto a su dificultad e incluye problemas para los 
cuales no se conocen soluciones que esten acotadas por una cantidad polinomial 
de operaciones en funci\'on de la entrada.

Esto es desalentador en un principio puesto que un algoritmo exacto para resolver
este problema y que sea pr\'actico para utilizar en entradas no triviales no es
conocido, y la pregunta de si existe es parte de una de las preguntas abiertas
m\'as conocidas de la computaci\'on~\cite{Cormen}.

En particular, este problema es muy similar a otro problema muy conocido, 
denominado \textit{Bin Packing Problem}. Este problema consiste en, dados $n$ 
objetos con vol\'umenes $V_1, \dots, V_n$, y disponiendo de contenedores de
vol\'umen m\'aximo $C$, dar la m\'inima cantidad de contenedores necesarios para
empaquetar todos los objetos.

La relaci\'on entre ambos problemas puede hacerse expl\'icita mediante una simple
reducci\'on de uno al otro.

Observemos que si conocieramos, para nuestro problema de dividir el trabajo entre
hilos de ejecuci\'on, el valor $M$ de carga m\'axima que uno de estos tiene que
procesar, podemos usar un algoritmo que resuelva \textit{Bin packing} para 
obtener la m\'inima cantidad de hilos de ejecuci\'on que necesitamos para distribuir
el trabajo y que ninguno de ellos este cargado m\'as que $M$. 

Para determinar $M$ usamos un simple algoritmo de b\'usqueda binaria en base a los
siguientes tres observaciones:

\begin{itemize}
    \item Es imposible que $M$ sea m\'as chica que el menor de los costos de los
    trabajos a procesar. Esto es trivialmente demostrable.
    \item Dado que siempre disponemos de al menos un procesador, sabemos que $M$
    es como mucho el total de todos los trabajos. Esto tambi\'en es trivialmente
    cierto.
    \item Sabemos que si tenemos una soluci\'on tal que cada thread esta cargado
    hasta $M$ unidades de trabajo, tenemos una soluci\'on para toda carga m\'axima
    $M', M' \geq M$. An\'alogamente, sabemos que si no podemos encontrar una 
    soluci\'on para $M$, es imposible que obtengamos una para $M', M' \leq M$.
\end{itemize}

Con esto, utilizamos busqueda binaria para encontrar el $M$. En cada paso, para
un $M$ candidato, usamos un algoritmo de \textit{bin packing} para obtener 
cuantos hilos de ejecuci\'on necesitariamos para dividir el trabajo de manera que
ning\'un procesador reciba m\'as trabajo que $M$. Si esta cantidad es mayor que
la cantidad de n\'ucleos de procesamiento m\'aximos que disponemos, entonces 
necesitamos que al menos uno de los procesadores este m\'as cargado. Sino, podemos
intentar cargarlos menos.

Un pseudoc\'odigo para esta secci\'on del algoritmo esta disponible en la figura~\ref{algo:partition-algo}.

Un problemad de esta estrategia es que, como se se\~nalo anteriormente, 
\textit{Bin packing} es un problema \textit{NP-hard}~\cite{NPCompleteness}. Sin
embargo, se conocen buenos algoritmos aproximados para resolver el problema. Los
mismos pueden devolver una respuesta que no es \'optima, pero razonablemente cerca
de la \'optima.

Una estrategia posible de este estilo es la estrategia denominada \textit{First
fit decreasing}, que es la utilizada en este trabajo. Esta estrategia consiste en 
iterativamente ubicar los objetos, en \'orden de mayor a menor en costo, 
en el primer contenedor en el que entre. De no disponerse ninguno, se utiliza
uno nuevo y se repite el algoritmo. Se sabe que este algoritmo da una respuesta
que como m\'aximo es $\frac{11}{9} O + 1$, donde $O$ es el \'optimo para el 
problema a resolver~\cite{FFDDemo}.

Esta secci\'on del algoritmo tambi\'en puede verse en pseudoc\'odigo en~\ref{algo:partition-algo}.
La complejidad resultante es $O(M n \log n)$ con $n$ la cantidad de elementos y $M$ la suma
de todos los costos. Dado que $M$ y $n$ son de tama\~no razonablemente peque\~no y el algoritmo
solo debe ser ejecutado antes de la primer iteraci\'on, decidimos utilizarlo para
nuestra implementaci\'on.

\begin{algorithm}[H]
    \caption{Pseudoc\'odigo del algoritmo para particionar trabajo entre \textit{threads}}
    \label{algo:partition-algo}
    \begin{algorithmic}
        \Function{partition}{$C = \{C_1, \dots, C_n\}, m$}
            \State $sort(C)$
            \State $L = min(C)-1, R = sum(C)$
            \While{$R - L > 1$}
                \Comment{Invariante: $(L, \dots, R]$ contiene la capacidad maxima.}
                \State $Capacity \gets \frac{L+R}{2}$
                \State $Partition \gets splitbins(C,Capacity)$
                \If{$\# Partition \leq m$}
                    \State $R \gets Capacity$
                \Else
                    \State $L \gets Capacity$
                \EndIf
            \EndWhile
            \Return{$splitbins(C, R)$}
        \EndFunction

        \Function{splitbins}{$C = \{C_1, \dots, C_n\}, m$}
            \State $Bins \gets []$
            \ForAll{$c \in C$}
                \State $Fits \gets [v \mid Space(v) \geq c]$
                \If{$Fits = \emptyset $}
                    \State $Fits \gets EmptyBin \bigcup Fits$
                \EndIf 
                \State $Fits_1 \gets Fits_1 \bigcup c$
            \EndFor
            \Return $Bins$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Cambios en paralelismo interno}

\subsection{Algoritmo de balanceo}
