
\section{XeonPhi}

\subsection{Introducci\'on}

La arquitectura Xeon Phi es la culminaci\'on de un trabajo iniciado por Intel en 2004, previendo
la necesidad de paralelismo masivo para aplicaciones futuras. Saliendo al mercado al final de 2012,
con el prop\'osito de competir en computo intensivo con \nvidia CUDA, ha ganado gran tracci\'on dentro
de HPC a pesar de ser reciente. Por ejemplo, ha sido fuertemente utilizada en la supercomputadora Tianhe-2
de la Universidad de Sun Yat-Sen en China, listada en Top 500 como la supercomputadora m\'as r\'apida del
mundo en Junio 2013, Noviembre 2013 y Junio 2014~\cite{Top500XeonPhiJune2013},~\cite{Top500XeonPhiNov2013},~\cite{Top500XeonPhiJune2014}.
Los 16000 nodos de esta supercomputadora contienen, adem\'as de dos Ivy Bridge Xeon, 3 coprocesadores
XeonPhi, dando un poder total de computo te\'orico de 54.9 petaflops.

\subsection{Microarquitectura general}

En la concepci\'on del Xeon Phi se tuvieron en cuenta diversos factores, entre los cuales
se incluye el consumo energ\'etico. Uno de los objetivos fue aumentar la relaci\'on de
poder de c\'omputo por Watt de los procesadores de Xeon de la \'epoca, manteniendo un entorno
de desarrollo de prop\'osito general como el de x86.
Si bien el consumo no se ve\'ia impactado por el set de instrucciones,
si se busc\'o eliminar diversos componentes del procesador, mantendiendo las caracter\'isticas
que sirvieran al tipo de aplicaci\'on a la que se estaba
apuntando (programas altamente paralelos a nivel de datos y tareas)~\cite{BookXeonPhi}.

La microarquitectura de este coprocesador se basa en muchos (m\'as de 50) procesadores sim\'etricos que comparten la memoria, lo
cual justifica su nombre MIC (\textit{Many Integrated Core}). Cada procesador esta basado
en el dise\~no del Intel Pentium, con una ISA (\textit{Instruction Set Architecture}) similar a la IA-32 con
soporte para direccionamiento a 64 bits y nuevas instrucciones de vectorizaci\'on.

Los procesadores tienen un \textit{clock rate} de 1.0 GHz aproximadamente, haciendolos relativamente lentos frente
a otros procesadores de Intel. Por ejemplo, los cores de un Intel Xeon CPU E5-2620 tienen un \textit{clock rate}
de 2.10 GHz, m\'as del doble.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.6\textwidth]{images/xeon-phi-core.jpg}
   \caption{Pipeline del procesador Intel Xeon Phi, tomado de~\cite{XeonPhiArchArticle} }
   \label{fig::xeon_phi_core}
\end{figure}

Un esquema de la arquitectura de cada procesador puede verse en la figura~\ref{fig::xeon_phi_core}. Cada uno de los cores
permite hasta 4 \textit{threads} simult\'aneos, con el prop\'osito de esconder la latencia de memoria y del tiempo de ejecuci\'on de
las instrucciones vectoriales. Adicionalmente, el uso de dos \textit{pipelines} permite que se ejecuten hasta 2
instrucciones por ciclo de clock.  Algunas instrucciones, sin embargo, solo pueden ser despachadas en uno de ambos:
por ejemplo las instrucciones de vectorizaci\'on solo pueden ser ejecutadas en la \textit{U-pipe}. Para esto se dispone
de una unidad de vectorizaci\'on (VPU, \textit{Vector Processing Unit}) con 32 registros SIMD (\textit{Single Instruction
Multiple Data}) de 512 bits por thread, con lo cual cada uno puede te\'oricamente realizar 16 operaciones de punto flotante de
32 bits al mismo tiempo. La latencia de estas instrucciones es de 4 ciclos de clock. Sin embargo gracias a su micro
arquitectura de \textit{full pipeline} se puede obtener un \textit{throughput} te\'orico de 1 instrucci\'on vectorial por ciclo.

\subsection{Pipeline}

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.6\textwidth]{images/xeon-phi-pipeline.png}
   \caption{Pipeline del procesador Intel Xeon Phi, tomado de~\cite{XeonPhiBook} }
   \label{fig::xeon_phi_pipeline}
\end{figure}


El \textit{pipeline} de instrucciones tiene siete etapas  para las instrucciones escalares, y las
instrucciones vectoriales ocupan otras siete. Un esquema de estas puede verse en la figura~\ref{fig::xeon_phi_pipeline}.
El pipeline puede detenerse parcialmente si alguna de las partes sufre un stall, a diferencia de lo que ocurre en
arquitecturas como Intel Xeon.

El pipeline tiene las fases usuales del ciclo \textit{fetch-decode-execute}) con algunas modificaciones.

El \textit{instruction fetch} esta dividido en dos fases para elegir el \textit{thread} por hardware a ejecutar:
\textit{Prethread picker function} (PPF) y \textit{Picker function} (PF). En la fase PPF se mueve la instrucci\'on
a uno de los 4 buffers de \textit{prefetch} que tiene cada procesador. Por cada thread hay adem\'as dos streams de
instrucciones. Si un stream es \textit{stalleado} por el pipeline, se puede proseguir con el otro stream.

La etapa PF selecciona el thread a ejecutar, usando el buffer de prefetch. Cada buffer tiene espacio para dos
instrucciones (porque puede despacharse una instrucci\'on por la \textit{U-pipe} y otra por la \textit{V-pipe}). PF
funciona de manera \textit{round robin} en los buffers de prefetch. Recargar este buffer (por ejemplo cuando hay un
\textit{miss} de cache de instrucciones) toma aproximadamente 4 o 5 ciclos.

Los 4 threads de un core son multiplexados por el pipeline del mismo, de manera de que si uno de los threads esta
\textit{stalleado} se elija a otro que tenga trabajo para hacer. Se sugiere entonces el uso de al menos dos
\textit{threads} por core para esconder latencias de memoria y de instrucciones vectoriales, y asi aumentar la performance.

Una vez que una instrucci\'on ha sido elegida para decodificarse, pasa a los estad\'ios D0 y D1, a una velocidad de
decodificaci\'on de 2 instrucciones por ciclo de \textit{clock}. De ahi son enviadas a cada pipeline para ejecutarse.
Por \'ultimo se pasa a la etapa de \textit{writeback} (WB). No necesariamente cuando una instrucci\'on llega a esta
fase ha terminado de ejecutarse, puesto que si la operaci\'on es vectorial reci\'en termina de ejecutarse en la unidad
vectorial 5 ciclos despu\'es.

Este pipeline corto (7 etapas frente a las 20 de la arquitectura Pentium 5 en la que se basa Xeon Phi) contribuye a que
los \textit{branch mispredictions} tengan menor latencia, y las instrucciones escalares tengan poca latencia~\cite{IntelXeonPhiWhitePaper}.

\subsection{Estructura de cache}

Adem\'as de la unidad de vectorizaci\'on y la unidad escalar, cada procesador cuenta con 32 Kb de cache L1 y 512 Kb de cache
L2 unificada para datos y c\'odigo. Estas caches son \textit{set associative} \textit{8-way} con una linea de cache de 64 bytes.
La cache de datos es no bloqueante, de manera que un \textit{miss} de cache de un thread en un core,
no se produce un \textit{flush} del pipeline en los dem\'as threads.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.6\textwidth]{images/xeon-phi-ring.jpg}
   \caption{Detalle de la estructura de anillo del Xeon Phi, tomado de~\cite{XeonPhiArchArticle}}
   \label{fig::xeon_phi_arch_global}
\end{figure}

La cache L2 mantiene su coherencia mediante el uso de un directorio distribuido de \textit{tags}. Un esquema del mismo puede
verse en la figura~\ref{fig::xeon_phi_arch_global}. El mismo esta dividido en 64 secciones y une tanto los segmentos del
directorio de tags con los cores y los controladores de memoria principal, utilizando una topolog\'ia de anillo bidireccional,
contando con un anillo para datos, uno para direcciones y uno para confirmaciones (\textit{acknowledgements}).

Cuando ocurre un \textit{miss} de cache L2, la direcci\'on es enviada al anillo de direcciones. Si la l\'inea de cache
es de otro core, se env\'ia un request de \textit{forwarding} y los datos son enviados por el anillo de datos. Si ning\'un
core tiene esa l\'inea de cache se env\'ia un pedido a uno de los varios controladores de memoria. Cada uno de estos controladores
maneja un subconjunto del espacio de direcciones con el prop\'osito de reducir los cuellos de botella y aumentar el ancho de banda.

El protocolo de coherencia de estas caches esta basado en MESI utilizando GOLS.

Cada cache L2 esta divida en dos bancos, con una latencia de 2 ciclos para leer 64 bytes (una l\'inea) y 1 ciclo para escribir 64 bytes.

La unidad de procesamiento vectorial lee y escribe los datos de cache con una granularidad de 512 bits, a trav\'es de un
\textit{bus} dedicado de 512 bits. Los accesos son alineados antes de ser escritos al \textit{write-commit} buffer de la
cach\'e.

Una cosa interesante de esta arquitectura, es que la pol\'tica de cach\'e puede modificarse mediante \textit{hints} al coprocesador. Por defecto la pol\'itica del subsistema
de cach\'e es pseudo-LRU (\textit{Least Recently Used}). Antes de escribir una l\'inea de cache el procesador debe leerla a su propia
cache (pol\'tica conocida como RFO, \textit{Read For Ownership}). Dado que esto puede implicar que datos \'utiles de cache
sean removidos para dejar lugar a datos que solo se van a escribir, la arquitectura da soporte a stores \textit{stremeados},
que pasan de largo a los caches. Tambi\'en se dispone de un \textit{prefetcher} por hardware.

\subsection{Arquitectura de instrucciones}

Si bien la base del conjunto de instrucciones de la arquitectura del Xeon Phi es la P54C de Pentium (IA-32), nuevas instrucciones
se han incluido con el prop\'osito de aumentar la idoneidad del procesador para computo de alta performance.
Estas operaciones incluyen implementaciones por \textit{hardware} de operaciones com\'unes en HPC: rec\'iproco de un valor,
ra\'iz cuadrada, potencia y exponenciaci\'on, y operaciones m\'as relacionadas con la memoria como por ejemplo \textit{scatter and gatter}
y stores \textit{streameados} de manera de aprovechar mejor el ancho de banda de memoria que permite la arquitectura.

El dise\~no de las instrucciones vectoriales es ternario, con dos operandos fuentes y uno destino codificados en la instrucci\'on.
Esta configuraci\'on permite una ganancia de hasta 20\% sobre la configuraci\'on usual binaria de otras arquictecturas SIMD (como
por ejemplo SSE o AVX)~\cite{BookXeonPhi}. Tambi\'en el \textit{set} de instrucciones cuenta con operaciones FMA (\textit{Fused
Multiply Add}) que utilizan los tres operandos como fuente y uno de ellos como el destino.

Una expansi\'on adicional para aprovechar los registros amplios son los registros de m\'ascara (\textit{masked registers}).
Estos registros de 16 bits se utilizan en varias de las instrucciones vectoriales para habilitar o deshabilitar elementos
de los 16 (como m\'aximo) que tiene un registro SIMD del Xeon Phi como parte de un c\'omputo, contribuyendo a la generalidad de
las instrucciones adicionales de la arquitectura. Si un elemento no estuviera en el registro de m\'ascara, el valor resultado de la operaci\'on
sobre los dos elementos correspondientes de los operandos fuentes es escrito en el lugar correspondiente del operando destino.

Asi como ocurre con los 32 registros SIMD, los 8 registros de m\'ascara son
por \textit{thread}. El uso de estas m\'ascaras esta contemplado en las instrucciones, requiriendo un ciclo de clock adicional
para pasar por la \textit{Mask pipeline}.

Las instrucciones especiales para exponenciaci\'on forman parte de la EMU (\textit{Extended Math Unit}), utilizando cuadratura
num\'erica y tablas para aproximar las funciones trascendentales por \textit{hardware}. Debido a la prevalencia en computo
cient\'ifico de estas instrucciones se busc\'o la disminuci\'on en el uso de ciclos de reloj para su c\'alculo.

La adici\'on de instrucciones sobre memoria no secuencial (\textit{scatter and gather}) resultan interesantes por su prevalencia
en HPC y porque facilitan la tarea de generaci\'on de c\'odigo vectorial en los \textit{backends} de compiladores. Tambi\'en, con
el prop\'osito de permitir un control m\'as fino de los cach\'es, los operandos en memoria para operaciones vectoriales permiten
la inclusi\'on de un atributo denominado \textit{eviction hint}. Este atributo permite determinar si los datos ser\'an o no
reusados, escribiendolos directamente a memoria si no fuese as\'i. De esta manera se evita la contaminaci\'on de cache con datos
que no ser\'an vueltos a utilizar en calculos posteriores. Como su nombre lo indica, este atributo es un consejo al coprocesador,
y este puede ignorarlo.

Por \'ultimo, tambi\'en la unidad vectorial implementa un \textit{prefetcher} por software, tanto para los caches L1 y L2. Estos
pueden combinarse con instrucciones de \textit{gather} y \textit{scatter} para disminuir los accesos a memoria y los \textit{stalls}
en retiro de instrucciones.

\subsection{Organizaci\'on de la memoria}

La memoria principal del Xeon Phi consiste de memoria RAM GDDR5 en la placa, de 8 GB. Los cores y la memoria principal son comunicados mediante el uso de 8
controladoras, conectadas mediante el anillo bidireccional, de dos canales a 5.5 Gb/s. Cada transferencia realizada es de 4 bytes.
Esto nos da un l\'imite de ancho de banda te\'orico de 352 GB/s pero detalles de implementaci\'on de los chips limitan este valor a 200 GB/s.

Las controladores reciben los pedidos mediante el anillo de direcciones y convierten estos en comandos para GDD5, retornando finalmente los datos en
el anillo de datos. Los dispositivos GDDR5 tienen el espacio de direcciones interlineado para distribuir mejor la carga de pedidos.

\begin{figure}[htbp]
   \centering
   \includegraphics[width=0.6\textwidth]{images/xeon-phi-addressing.png}
   \caption{Esquema de paginaci\'on del Xeon Phi}
   \label{fig::xeon_phi_addressing}
\end{figure}

La memoria esta dividida en p\'aginas utilizando un esquema de 4 niveles, con direcciones de 64 bits~\ref{fig::xeon_phi_addressing}. El tama\~no de p\'agina soportado es de 4 Kb
o 2 Mb, siendo el esquema de 2 Mb preferido en aplicaciones de computo cient\'ifico. Como es de esperar, el Xeon Phi utiliza TLBs (\textit{Translation Lookaside
Buffers}) para disminuir la cantidad de accesos a memoria necesarios para mapear una p\'agina de memoria virtual en una f\'isica.

\subsection{Conexion Host - Coprocesador}

El Xeon Phi se conecta con su host mediante el uso de un bus PCI Express 2.0 de 16 lineas. Las transferencias pueden ser
mediante I/O programada o DMA (\textit{Direct Memory Access}). Este bus permite la transferencia no solo al host sino a
otros coprocesadores, permitiendo tener m\'ultiples placas en una misma computadora. La velocidad de transferencia alcanzada
es de m\'as de 6 GB/s. DMA ocurre a velocidad de \textit{clock} del procesador, y los 8 canales de DMA pueden transferir
en paralelo. Cada transacci\'on de DMA es entre 64 y 256 bytes.

Siendo que el coprocesador dispone de su propio sistema operativo basado en Linux, lo cual incluye una implementaci\'on
de un \textit{stack} completo de TCP/IP, el bus PCI se puede utilizar para comunicar el sistema operativo del host con
el del Xeon Phi a trav\'es de protocolos como SSH (Secure SHell). Esto facilita la administraci\'on de la placa y la
transferencia de c\'odigo y datos para ejecuci\'on, y el uso del coprocesador como nodo de computaci\'on adicional en un
esquema de cluster.

La placa no tiene acceso a perif\'ericos, en particular disco duro. Para esto se puede utilizar un sistema NFS (\textit{Network
File System}) con el prop\'osito de almacenar datos en el host.

\subsection{Modos de ejecuci\'on}

Existen tres m\'etodos para ejecutar c\'odigo en el coprocesador.

\begin{enumerate}
    \item Nativo: El Xeon Phi permite correr c\'odigo directamente. Esto es gracias a la presencia de BusyBox Linux como sistema operativo,
    lo cual da soporte de sistema de archivos y entorno de ejecuci\'on. El c\'odigo puede ser enviado al Xeon Phi desde el host, y por defecto se utiliza un sistema
    de archivos montado sobre la propia RAM.
    \item Offloading: El \textit{host} puede delegar la ejecuci\'on de ciertas porciones de c\'odigo al coprocesador.
    Esto requiere que los datos necesarios para el c\'omputo sean copiados del \textit{host} al Xeon Phi, lo cual puede implicar que el bus puede ser un cuello de botella
    importante (puesto que los datos de entrada, y la salida deben ser movido al Xeon Phi y traidos de vuelta al finalizar el c\'omputo).
    \item Sim√©trico: En este modo de ejecuci\'on se piensa al Xeon Phi y su host como dos nodos en un \textit{cluster} de c\'omputo, y al bus PCIe como una red de alta velocidad.
    Este modo es especialmente interesante si se dispone de m\'as de un Xeon Phi en un mismo host,
    y se utiliza una interfaz de pasado de mensajes entre ellos como por ejemplo MPI (\textit{Message Passing Interface}).
\end{enumerate}

\subsection{Consumo}

El consumo energ\'etico es uno de los principales factores considerados para el dise\~no de la arquitectura MIC. La filosof\'ia
utilizada para lograr un m\'aximo equilibrio de potencia y consumo es que la aplicaci\'on dictamina al hardware como
manejar los estados de poder que dispone.

Existen varios estados de consumo energ\'etico: Turbo, P-states, C-states y M-states. En modo Turbo, seg\'un la cantidad de
cores ejecutando, se puede incrementar la frecuencia de reloj de los mismos. La frecuencia de operaci\'on de todos los procesadores
puede modificarse usando los distintos P-states, que rigen la velocidad del clock general. Adicionalmente, cada core puede estar en
distintos C-states. El procesador puede correr a su m\'axima capacidad (C0), puede no recibir el clock general pero si interrupciones
(C1), o puede estar apagado en su totalidad (C6). Existen estados similares para los controladores de memoria (M-states).

Con el objetivo de proveer informaci\'on de temperatura y uso al sistema operativo y aplicaciones, el coprocesador dispone de
hardware dedicado de colecci\'ion de estos datos para uso posterior. Con esto se puede hacer uso inteligente de los diferentes
estados del procesador de acuerdo a la carga de trabajo del sistema y su comportamiento.

Esta interfaz con el hardware es utilizada por tanto el sistema operativo incluido en la placa como por el \textit{stack} de
aplicaci\'on (MPSS, \textit{Manycore Platform Software Stack}) para controlar el consumo energ\'etico. Por ejemplo, cuando el
coprocesador se encuentra \textit{idle}, sin trabajo que realizar.

\subsection{Herramientas de desarrollo y profiling}

El desarrollo de aplicaciones para el Xeon Phi es muy similar al desarrollo de aplicaciones para las arquitecturas
de la linea Xeon de Intel. Esto es gracias a la existencia de compiladores de C, C++ y Fortran (los lenguajes m\'as usados en
el \'ambito de HPC) que pueden producir c\'odigo para el Xeon Phi, y la presencia de un sistema operativo y \textit{stack}
de aplicaciones que facilitan la tarea de ejecuci\'on y mantenimiento del c\'odigo. A diferencia de \nvidia CUDA, no es necesario
el uso de un lenguaje de programaci\'on separado.

Los distintos modos de uso para el Xeon Phi reciben soporte de diversas librer\'ias. El uso mediante \textit{offloading}, por ejemplo,
es provisto mediante el uso de pragmas y opciones de compilaci\'on que permiten indicar que computo y como deben ser transferidos
a la placa. Estos pragmas permiten control fino de como enviar los datos a la placa, si deben ser transmitidos de vuelta de la placa,
etc.

Asimismo los compiladores de Intel pueden generar c\'odigo para la arquitectura MIC, el cual se puede ejecutar directamente sobre
el Xeon Phi a trav\'es del sistema operativo.

Por \'ultimo, Intel tambi\'en provee librer\'ias portadas como OpenMP, MPI, Thread Building Blocks, etc. Librer\'ias ya existentes
como la MKL (\textit{Math Kernel Library}), que implementa las conocidas rutinas de BLAS y LAPACK, tambi\'en tienen su versi\'on
pensada para Xeon Phi, e incluso pueden utilizar transparentemente el Xen Phi.

\subsection{Idoneidad para la tarea}

Siendo que el mercado principal para Xeon Phi son las aplicaciones de computo cient\'ifico, esto hace que la
aplicaci\'on LIO sea ya de por si una posible candidata al uso de esta tecnolog\'ia. Otros aspectos que
hacen interesante portar la misma al uso del Xeon Phi son la existencia de una gran cantidad de c\'odigo ya
implementada en C++ y Fortran, la cual ya puede ser movida a Xeon Phi sin requerir m\'as que una compilaci\'on.
Este c\'odigo existente tiene una performance fuertemente atada al computo asociado,
y la tarea es altamente paralelizable (como se demostr\'o en las implementaciones ya realizadas usando CUDA) y
vectorizable.
