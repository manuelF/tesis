\section{?`Qu\'e me conviene comprar?}
Luego de todas las mejoras realizadas a la aplicaci\'on LIO, queda la interrogante pendiente. Para
correr simulaciones de QM, ?`Qu\'e versi\'on me conviene correr, CPU o GPU?

Esta pregunta es siempre capciosa y es susceptible a cambios tecnol\'ogicos muy r\'apidamente. Intentaremos
responderla utilizando dos \textit{tiers} donde correr LIO, uno para una estaci\'on de trabajo
y el otro para un servidor de c\'omputo. Definimos, para cada configuraci\'on, una versi\'on para GPU y otra
para CPU de modo de priorizar los recursos invertidos. Se busc\'o que los costos de ambos niveles
sean parejos, de modo de hacer la comparaci\'on realista de que conviene invertir.

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Configuraci\'on& Estaci\'on de trabajo                                                                                                      & Servidor                                                                                              \\ \midrule
GPU                      & \begin{tabular} [c]{@{}l@{}} Intel Core i5-4460 @ 3.2GHz\\ 8GB RAM DDR3\\ 1 o 2 x GeForce GTX 780 3GB\end{tabular} & \begin{tabular}[c]{@{}l@{}}E3 low end\\ 16GB RAM DDR3\\ 1 o 2 x NVIDIA Tesla K40 12GB\end{tabular} \\ \hline \\
CPU                      & \begin{tabular}[c]{@{}l@{}}Intel Core i7-3770 CPU @ 3.40GHz \\ 16GB RAM DDR3\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}2 x Intel Xeon E5-2620 v2 @ 2.10GHz\\ 32GB RAM\end{tabular}                             \\ \bottomrule
\end{tabular}
\label{tbl:configs}
\caption{Distintas configuraciones de estaciones de trabajo y servidores para computo de QM/MM usando LIO}
\end{table}

%en hemo k40
%total iter = 1s 331302
%iteration = 522937
%TODO especificar bien en algo medible.

Lo que primero se debe notar en GPU es que se usan placas que tienen potencia de c\'alculo casi equivalente
para las cuentas en simple precisi\'on. Sin embargo, elegimos una Tesla para configuraci\'on de servidor
porque son las placas mejor preparadas para HPC. Estas cuentan con cuatro veces mas memoria en la placa
que tiene ECC y con mayor MTFB (\textit{Mean Time Between Failures}), factor vital en servidores
que deben correr confiablemente.

La m\'etrica que usaremos para comparar los distintos sistemas es cuantas iteraciones del c\'alculo de
SCF se pueden ejecutar por d\'ia. Esto lo hacemos para medir estrictamente la performance
de QM, importante en simulaciones de \textit{Time-Dependant Density Functional Theory} por ejemplo, y no
hablar de las implementaciones de los sistemas de QM/MM que utilizan LIO.
Esta m\'etrica es similar a las que se usan en el \'area de MM. Programas como Amber\cite{Amber} la usan para comparar
distintas configuraciones de hardware sobre la cual correr mejor.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\plotwidth]{plots/otros/its-dia.png}
    \caption{Cantidad de iteraciones de SCF por d\'ia obtenidas usando distintas configuraciones corriendo Hemoglobina usando los
              par\'ametros \'optimos correspondientes.}
    \label{fig:its-dia}
\end{figure}

Un factor a tomar en cuenta para analizar la performance de diversas configuraciones, como en la figura \ref{fig:its-dia},
es que el c\'alculo de SCF incluye contribuciones que no son solamente las relativas a intercambio correlaci\'on
(las estudiadas en esta tesis), por los cuales los tiempos de ejecuci\'on totales van
a presentar menores aceleraciones que las presentadas anteriormente.

La principal ventaja que se obtiene de correr en las estaciones de trabajo con GPU es la potencia
de calculo concentrado en los dispositivos GPU. La mayor aceleraci\'on del calculo de XC se obtiene
con dos GeForce GTX 780, en comparaci\'on con el procesador. Esto se condice con las hip\'otesis discutidas
anteriormente sobre que los n\'ucleos del procesador se encuentran completamente ocupados.
En cambio, las estaciones de trabajo con CPU cuentan con la ventaja de que se acelera tambi\'en el
resto de las cuentas que componen una iteraci\'on de SCF. Estas cuentas, son de menor costo computacional
comparada con las de XC, pero luego de las aceleraciones alcanzadas por este trabajo, el peso de estas
es comparativamente grande. Como estas otras contribuciones de SCF no est\'an
aceleradas con GPU, ese recurso se subutiliza fuera de XC.

Cuando se comparan las configuraciones de servidores, el panorama cambia sustancialmente.
Las aceleraciones obtenidas en la versi\'on GPU provienen principalmente de un solo factor,
la mayor memoria del acelerador GPU. Esta memoria permite entonces mantener las matrices
de funciones en memoria sin tener que recalcularlas. Esta optimizaci\'on se detall\'o en la secci\'on
\ref{GuardarFunctionsGPU}. Como la GeForce GTX 780 cuenta con una velocidad de clock mayor a
la K40 (probablemente porque en la linea de placas para v\'ideo juegos sea m\'as importante
la performance que la estabilidad) las cuentas se realizan incluso mas r\'apido en la
versi\'on de consumidor. Sin embargo, la Tesla K40 tiene un costo alrededor de ocho veces mayor.

La configuraci\'on de CPU en cambio, la performance escala linealmente en la cantidad de cores.
Dadas las optimizaciones realizadas en CPU priorizando la escalabilidad, es de notar la gran
diferencia de performance con respecto a la configuraci\'on de estaci\'on de trabajo. Priorizar CPU
favorece tambi\'en a todo el resto del c\'alculo de SCF. La mayor\'ia de las cuentas que no son
de intercambio-correlaci\'on se resuelven, o bien a trav\'es de librer\'ias BLAS en CPU, o bien
aprovechando las t\'ecnicas de paralelizaci\'on autom\'atica que brindan los compiladores usados.

Comparando ambas configuraciones de servidores, hoy en d\'ia es mucho mas f\'acil encontrar
nodos de computo de HPC usando m\'ultiples procesadores que nodos con varias GPU por varios motivos.
Primero, porque son mas generales: los nodos se pueden usar para m\'ultiples aplicaciones de HPC,
todav\'ia mayormente basados en CPU. Segundo, motivos energ\'eticos: el consumo energ\'etico de un cluster
HPC es muy elevado y las GPU pueden tener altos consumos que hacen muy dif\'icil la disipaci\'on t\'ermica.
Finalmente, el costo: las placas GPU de la linea de servidores cuestan mas de seis veces que los
procesadores de muy alta gama que se usan en HPC. Si las aplicaciones no van a hacer uso constante
de estas y tener aceleraciones comparables con el costo, entonces puede incluso no ser rentables
para las aplicaciones .

%\section{Importancia del problema}
%\section{Impacto de trabajar correctamente con la arquitectura}
\section{Posible trabajo a futuro}
El trabajo pendiente obvio que se ve ac\'a, que brindar\'ia inicialmente la mayor aceleraci\'on
es la realizaci\'on de una versi\'on h\'ibrida de LIO. Como pudimos observar, las cuentas son
totalmente independientes (salvo una reducci\'on final). Esto se explot\'o en forma de paralelismo
de m\'ultiples placas GPU y en m\'ultiples n\'ucleos de procesador. Nada, \textit{a priori}, previene
que se pueda usar simult\'aneamente como recursos asim\'etricos para resolver de forma h\'ibrida los sistemas.

La exploraci\'on preliminar que se realizo con el Xeon-Phi en este trabajo no trajo los resultados
prometidos por las especificaciones del fabricante. La velocidad de los cores y la de la memoria
presentaron cuellos de botella fundamentales que, al menos durante esta investigaci\'on, no se
pudieron resolver. Una versi\'on h\'ibrida con offloading es prometedora, pero las aceleraciones m\'aximas
estar\'an limitadas por los costos de transferencia de memoria entre CPU y el Xeon Phi.

Un \'area de inter\'es para estudiar en la aplicaci\'on es la construcci\'on de las particiones
con los mejores par\'ametros para cada configuraci\'on. Si la aplicaci\'on pudiera descubrirlos
independientemente de tener que cargarlos a mano, seria un gran progreso para evitar mucho
tiempo de ajuste fino manual. Esto tambi\'en se puede expandir para muchos de los par\'ametros
que se optimizar en GPU (el tama\~no de los bloques de todos los kernels) y en CPU (TODO).
Esto permitir\'ia que, simplemente con una recompilaci\'on del c\'odigo, se pueda correr en generaciones
pr\'oximas de GPU y CPU, sin tener que reescribir estas secciones de c\'odigo cada 2 o 3 a\~nos.

Adem\'as de estas anteriores, pudimos observar al menos cinco categor\'ias de trabajos futuros que se
desprenden para poder extender LIO y estudiar posibles mejoras performance considerando
las arquitecturas disponibles en el mercado.
\subsection{Versiones h\'ibridas}
\begin{enumerate}
 \item Hacer una versi\'on h\'ibrida CPU-GPU-XeonPhi
  \item Probar implementar en FPGA los c\'alculos de SCF para poder resolverlos por hardware en procesadores
    Atom.
  \item Implementar una versi\'on MPI para poder resolver una iteraci\'on distribuyendo
    a m\'ultiples CPU/GPU/XeonPhi en distintos nodos.
  \item Explotar paralelismo de etapas a nivel mas granular, como realizar las densidades en CPU y las matrices
    de Kohn-Sham en GPU.
\end{enumerate}

\subsection{Balance de cargas}
\begin{enumerate}
  \item Repensar el algoritmo de partici\'on de trabajos para hacer m\'as equitativas las cargas sin
    tener que recurrir al balanceo durante la iteraci\'on.
  \item Modificar el algoritmo de generaci\'on de grilla que genere grupos m\'as equitativos para poder
    distribuir mejor las cargas.
  \item Estudiar el problema de partici\'on y sus funciones de costos para sean acertadas tanto en CPU
    como en GPU.
\end{enumerate}

\subsection{Explotar m\'as paralelismo}
\begin{enumerate}
  \item Portear LIO a OpenCL para poder unificar el c\'odigo.
  \item Investigar el uso de librer\'ias BLAS (Magma, MKL, CUBLAS, ATLAS, etc.) para offloadear fragmentos del
    c\'alculo de SCF.
  \item Analizar la posibilidad de usar CUDA Streams para intentar lograr kernels concurrentes y maximizar
    el uso de una placa.
  \item Acelerar los pasos de SCF actualmente single-core y que no tienen implementaciones en GPU.
  \item Acelerar el c\'alculo de las contribuciones de Coulomb para las fuerzas \'inter at\'omicas, el mayor factor de SCF luego de XC.
  \item Analizar otras estrategias de paralelismo en sistemas distribuidos como MapReduce para sistemas muy grandes.
  \item Estudiar alg\'un m\'etodo para elegir autom\'aticamente en CPU cuando particionar a nivel
    de m\'ultiples grupos y cuando a nivel interno por grupo.
\end{enumerate}

\subsection{Estudio autom\'atico de par\'ametros}
\begin{enumerate}
  \item Estudiar como paralelizar y vectorizar el calculo de funciones en CPU para poder escalar a m\'ultiples cores.
  \item Analizar el consumo energ\'etico de la aplicaci\'on e intentar buscar otras arquitecturas para maximizar
    la eficiencia por Watt.
\end{enumerate}

\subsection{Nuevas aplicaciones qu\'imicas}
\begin{enumerate}
  \item Experimentar el comportamiento de sistemas qu\'imicos muy grandes que no entren en memoria; como fraccionarlos.
  \item Integrar LIO a otros sistemas de MM como GROMACS y CHARMM.
\end{enumerate}
