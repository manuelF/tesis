%En este trabajo se estudiaron multiples optimizaciones apuntadas a maximizar el uso de los
%recursos de computo. Se pudieron observar grandes mejoras al tiempo de computo repensando los algoritmos
%de calculo de DFT para minimizar la capacidad ociosa que habia en las arquitecturas paralelas estudiadas.
En CPU se pudieron obtener mejoras de hasta 22 veces sobre la performance original en el c\'alculo de intercambio-correlaci\'on,
consiguiendo una escalabilidad lineal en la cantidad de procesadores disponibles. La velocidad final alcanzada es cercana a las de las GPU de alta gama
de la generaci\'on anterior y esta a menos de 1.5x de la velocidad de la m\'as r\'apida de la generaci\'on actual.
Esto se logr\'o gracias a reestructuraciones algor\'itmicas, al uso de librer\'ias est\'andar de paralelizaci\'on,
a reorganizaci\'on de los datos en la memoria para facilitar la vectorizaci\'on y a un an\'alisis de partici\'on de trabajos.
Es ademas importante se\~nalar que la implementaci\'on realizada solo requiere herramientas incorporadas al compilador,
sin uso de bibliotecas externas, favoreciendo la portabilidad del c\'odigo a m\'ultiples plataformas y abriendo la posibilidad
de incorporar el uso de librer\'ias como ATLAS y MAGMA para acelerar c\'alculos puntuales.

En GPU se consiguieron aceleraciones de hasta 8 veces en este c\'alculo con respecto a una versi\'on ya acelerada.
Las mejoras pasaron por encontrar los limitantes de las distintas funciones del c\'omputo, reestructurando las paralelizaciones,
cacheando mas resultados temporales y poder aprovechar los cambios en la arquitectura de las GPU con respecto a las memorias
on-chip. Finalmente, se obtuvieran incluso mayores aceleraciones aplicando paralelismo entre m\'ultiples
GPU, donde a pesar de no escalar linealmente con respecto a la cantidad de dispositivos, es una gran mejora
si se cuentan ya con los recursos ociosos.

En Xeon Phi no se consiguieron resultados que mejoraran los obtenidos en CPU o
Cuda. La baja performance de c\'odigo serial resulto un limitante muy
fuerte que no se logr\'o compensar. Determinamos sin embargo caracter\'sticas
de la arquitectura clave para atacar el problema de optimizar una aplicac\'on.
Identificamos la necesidad de lograr una escalabilidad muy alta en cantidad de
procesadores y la importancia de vectorizar bien.  Los resultados obtenidos
coinciden con los obtenidos en otros estudios~\cite{Jeffers}.

\section{Posible trabajo a futuro}

El trabajo pendiente inmediato que se observa y brindar\'ia inicialmente una gran aceleraci\'on,
es la realizaci\'on de una versi\'on h\'ibrida CPU-GPU de LIO. Como pudimos observar, la resoluci\'on de grupos es
totalmente independiente entre si (salvo una reducci\'on final). Esto se explot\'o en forma de paralelismo
de m\'ultiples placas GPU y en m\'ultiples n\'ucleos de procesador. Nada, \textit{a priori}, previene
que se puedan usar simult\'aneamente como recursos asim\'etricos para resolver de forma h\'ibrida los sistemas.
Los resultados vistos en la secci\'on \ref{tamgrupos} muestran muy claramente que existe alg\'un criterio
para poder separar grupos en CPU y GPU y que, al poder realizarse en paralelo, podr\'ian explotar
todos los recursos disponibles y lograr grandes aceleraciones.

La exploraci\'on preliminar que se realizo con el Xeon Phi en este trabajo no trajo los resultados
prometidos por las especificaciones del fabricante. La velocidad de los cores y la de la memoria
presentaron cuellos de botella fundamentales que, al menos durante esta investigaci\'on, no se
pudieron resolver. Una versi\'on h\'ibrida CPU-Xeon Phi con offloading es prometedora, pero las aceleraciones m\'aximas
alcanzables estar\'an limitadas por los costos de transferencia de memoria entre CPU y el Xeon Phi.

Un \'area de inter\'es para estudiar en la aplicaci\'on es la construcci\'on de las particiones
con los mejores par\'ametros para cada configuraci\'on. Si la aplicaci\'on pudiera descubrirlos
independientemente de tener que cargarlos a mano, seria un gran progreso para evitar mucho
tiempo de ajuste fino manual. Esto tambi\'en se puede expandir para muchos de los par\'ametros
que se optimizar en GPU (el tama\~no de los bloques de todos los kernels CUDA) y en CPU (el estimador de costos
de grupos).
Esto permitir\'ia que, simplemente con una recompilaci\'on del c\'odigo, se pueda correr en generaciones
pr\'oximas de GPU y CPU, sin tener que reescribir estas secciones de c\'odigo cada 2 o 3 a\~nos.

Adem\'as de estas anteriores, pudimos observar al menos cinco categor\'ias de trabajos futuros que se
desprenden para poder extender LIO y estudiar posibles mejoras performance considerando
las arquitecturas disponibles en el mercado.
\subsection*{Versiones h\'ibridas}
\begin{enumerate}
 \item Hacer una versi\'on h\'ibrida CPU-GPU-XeonPhi
  \item Probar implementar en FPGA los c\'alculos de SCF para poder resolverlos por hardware en procesadores
    Atom.
  \item Implementar una versi\'on MPI para poder resolver una iteraci\'on distribuyendo
    a m\'ultiples CPU/GPU/XeonPhi en distintos nodos.
  \item Explotar paralelismo de etapas a nivel mas granular, como realizar las densidades en CPU y las matrices
    de Kohn-Sham en GPU.
\end{enumerate}

\subsection*{Balance de cargas}
\begin{enumerate}
  \item Repensar el algoritmo de partici\'on de trabajos para hacer m\'as equitativas las cargas sin
    tener que recurrir al balanceo durante la iteraci\'on.
  \item Modificar el algoritmo de generaci\'on de grilla que genere grupos de tama\~nos variable para poder
    distribuir mejor las cargas.
  \item Estudiar el problema de partici\'on y sus funciones de costos para sean acertadas tanto en CPU
    como en GPU.
\end{enumerate}

\subsection*{Explotar m\'as paralelismo}
\begin{enumerate}
  \item Mejorar la estrategia de paralelizaci\'on para que emplee una cantidad de
  hilos de ejecuci\'on de acuerdo al problema (e.g. cubos medianos reciben m\'as
  de un thread, pueden correrse menos al mismo tiempo pero m\'as r\'apido).
  \item Portear LIO a OpenCL para poder unificar el c\'odigo.
  \item Investigar el uso de librer\'ias BLAS (Magma, MKL, CUBLAS, ATLAS, etc.) para offloadear fragmentos del
    c\'alculo de SCF.
  \item Analizar la posibilidad de usar CUDA Streams para intentar lograr kernels concurrentes y maximizar
    el uso de una placa.
  \item Acelerar los pasos de SCF actualmente single-core y que no tienen implementaciones en GPU.
  \item Acelerar el c\'alculo de las contribuciones de Coulomb para las fuerzas \'inter at\'omicas, el mayor factor de SCF luego de XC.
  \item Analizar otras estrategias de paralelismo en sistemas distribuidos como MapReduce para sistemas muy grandes.
  \item Estudiar alg\'un m\'etodo para elegir autom\'aticamente en CPU cuando particionar a nivel
    de m\'ultiples grupos y cuando a nivel interno por grupo.
\end{enumerate}

\subsection*{Estudio autom\'atico de par\'ametros}
\begin{enumerate}
  \item Estudiar como paralelizar y vectorizar el calculo de funciones en CPU para poder escalar a m\'ultiples cores.
  \item Analizar el consumo energ\'etico de la aplicaci\'on e intentar buscar otras arquitecturas para maximizar
    la eficiencia por Watt.
\end{enumerate}

\subsection*{Nuevas aplicaciones qu\'imicas}
\begin{enumerate}
  \item Experimentar el comportamiento de sistemas qu\'imicos muy grandes que no entren en memoria; como fraccionarlos.
  \item Integrar LIO a otros sistemas de MM como GROMACS y CHARMM.
\end{enumerate}
