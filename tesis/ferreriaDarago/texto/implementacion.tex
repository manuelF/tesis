\section{Implementacion en CUDA}

\input{implementacion-cuda}

\section{Implementacion en CPU}

\input{implementacion-cpu}

\section{Balance de cargas}
{ \Huge reescribir esto} \\
El balanceo de cargas optimo consiste en repartir $P$, el conjunto de problemas en
$n$ conjuntos disjuntos $P_i$, tal que $\bigcup_{i<n} P_i = P$. Este problema, conocido como PARTITION,
es NP-Completo en su version exacta, aunque son conocidos algoritmos pseudo-polinomiales para resolverlo.
La complejidad de ese algoritmo que usa programaci\'on din\'amica es $O(Nn)$, con $N$ el valor
de la suma de $P$ y $n$ la cantidad de elementos de $P$. Como nuestra m\'etrica para las
particiones es el runtime de estas expresados con precision de microsegundo, el costo computacional
de este algoritmo puede ser elevado en sistemas con cientos de particiciones de miles de puntos.
La principal desventaja de usar unicamente este algoritmo es que no contempla, o no al menos
en sus versiones m\'as directas, el uso de recursos asim\'etricos. Es decir, resolver la partici\'on
optima del problema de particiones cuando se sabe que el costo de resolver $P_i$ en el dispositivo
$A$ es distinto a resolverlo en el dispositivo $B$. Por este motivo, no podemos particionarlo estaticamente
usando este algoritmo y decidimos usar una soluci\'on h\'ibrida estatica y dinamica, reutilizando
la informaci\'on de runtime para decidir como rebalancear las particiones entre iteraciones.

Estaticamente definimos un orden para realizar toda la resoluci\'on del sistema y
se las distribuye de manera round-robin entre todos los dispositivos, para generar la
particion inicial y cargando a cada placa con una cantidad similar de tareas. Esto no signfica
que todas tarden lo mismo, y es posible notarlo en las mediciones.

Para hacer el balanceo dinamico, debemos determinar de ante mano la performance de cada
dispositivo con el que contemos. Para esto, se usa la tradicional t\'ecnica de definir un
caso de prueba representativo del problema, ejecutarlo en cada uno de los dispositivos y anotar
cuanto tarda en cada uno de ellos. Una gran ventaja de esto es que permite balancear facilmente las
cargas cuando se realizan operaciones de doble precision en conjuntos de dispositivos que incluyan
placas Tesla y placas Geforce. En simple precisi\'on, los topes de linea de cada una pueden tener
performance similares (por ejemplo, GTX 580 y M2090, GTX 780 y K20), pero en doble precisi\'on
usualmente las Tesla suelen ser entre 2 y 6 veces mas veloces. Como las cuentas se realizan
o todas en precision simple, o todas en precision doble, es sumamente importante particionar el
sistema tomando todo esto en cuenta.

El problema ejemplar elegido en este caso es realizar $10$ iteraciones del kernel de computo de la matriz RMM.
Consideramos que realizar este computo nos da una evaluaci\'on
emp\'irica de los recursos necesarios para resolver este problema. Este kernel utiliza muchos
de los recursos del dispositvo: realiza gran cantidad de accesos a memoria a traves
de la unidad de textura, usa principalmente instrucciones FMA, tiene una m\'axima ocupacion de los SM
y tiene un uso total de la memoria shared. No contabilizamos las copias de memoria desde y hacia la
placa de los parametros puesto que, al igual que en el c\'odigo de la simulaci\'on, casi la totalidad
de los datos se construyen directamente en la placa y se recuperan las matrices reducidas solamente al final
de la operaci\'on.

Teniendo las mediciones de tiempo para los $k$ dispositivos, se construye la matriz de
correcci\'on de runtimes con los $k^2$ los coeficientes de correcci\'on para poder estimar el tiempo de
ejecuci\'on en el nuevo dispostivo. Si los dispositivos presentes son identicos, entonces
la matriz va a tener solamente valores muy cercanos a 1, logicamente.

El pseudo c\'odigo del algoritmo de balanceo se detalla a continuaci\'on.
\begin{algorithm}
  \caption{Balanceo de duracion de threads}
  \label{ThreadBalancing}
\begin{algorithmic}
  \While{$ronda\_threads < max\_rondas\_threads$}
    \State $ronda\_threads++$

    \State $tiempo\_minimo \gets duracion[T_{min}]$
    \State $tiempo\_maximo \gets duracion[T_{max}]$

    \While {$tiempo_{max} < tiempo_{min} * k \wedge ronda\_trabajos < max\_ronda\_trabajos$}
      \State $ronda\_trabajos++$
      \State $\Delta T \gets (tiempo_{max} - tiempo_{min}) /2$
      \ForAll{$G_i \in Trabajos[T_{max}]$}
        \If{$duracion[G_i] * correccion[T_{max}][T_{min}] * penalidad\_migracion - \Delta T < tiempo_{G_{mejor\_candidato}}$}
          \State $G_{mejor\_candidato} \gets G_i$
        \EndIf
      \EndFor
     \State Liberar la memoria de las marices cacheadas de $G_{mejor\_candidato}$
     \State Mover $G_{mejor\_candidato}$ de $T_{max}$ a $T_{min}$
     \State Actualizar la duracion total de $T_{max}$ a $T_{min}$, aplicando la correcci\'on de tiempos
    \EndWhile
  \EndWhile
\end{algorithmic}
\end{algorithm}

En el algoritmo \ref{ThreadBalancing} se definen dos constantes adicionales, $k$ y $penalidad\_migracion$.
$k$ representa el coeficiente de m\'axima diferencia de tiempo entre el thread de mas larga duracion y el de menor
duracion. Para nuestros usos, un valor de $k \approx 5\%$ es aceptable.
$penalidad\_migracion$ es un coeficiente que agrega un costo al migrar el grupo a otra placa, ya que va
a tener que recalcular las matrices que ya se guardaba en memoria. Cuando cambie de placa, va a tener
que desalojarlas ya que, si bien es posible que entre las placas se accedan mutuamente a memoria global,
no vale la pena la latencia introducida, que se adicionar\'a a lo largo de todas las iteraciones de la resoluci\'on.
Este coeficiente es equivalente a definir una penalidad por romper la \textit{afinidad de las caches} en CPU.

Adicionalmente, el algoritmo \ref{ThreadBalancing} agrega dos contadores para este balanceo. El contador de
$ronda\_threads$ es el que sirve para balancear threads de a pares, necesario si hay mas de dos dispositivos.
El contador de $ronda\_trabajos$ es el que balancea, para el thread m\'as r\'apido y el m\'as lento, los
trabajos que van a migrar de un lado al otro. Es necesario agregar este contador por varios motivos. Por
un lado, si se mueven demasiados trabajos de un momento a otro, tal vez la estimaci\'on final deje de sear realista,
por lo que no tiene sentido seguir moviendo sin tener m\'as informaci\'on de otra corrida. Por otro lado, esta
heuristica puede ciclar infinitamente si no se cumple la condicion de que se puedan balancear los tiempos
con menos de diferencia $k$. En ninguno de nuestros casos de prueba nos hemos topado con que suceda esto, pero
consideramos que es correcto dejarlo.

Otra ventaja de usar este algoritmo adicionalmente a la partici\'on estatica es que, si bien se considera la matriz
de correcci\'on, la finalidad es balancear la carga minimizando los tiempos, por lo que incluso si la estimaci\'on original
de performance relativa era erronea, igualmente se redistribuiran los distintos grupos.


